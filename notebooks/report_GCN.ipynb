{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92e6c69",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6fe02d",
   "metadata": {},
   "source": [
    "* Dataset introduction\n",
    "* Original Paper and relative works\n",
    "* Our motivation and plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ea6db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09a24583",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e90f4ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2996f656",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850fdf3b",
   "metadata": {},
   "source": [
    "## Data Split & Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c894e939",
   "metadata": {},
   "source": [
    "### Load data and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2303ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data all: (136265, 169), Train data labeled: (29894, 169)\n",
      "Test data all: (67504, 169), Test data labeled: (16670, 169)\n",
      "Train edges all: (156843, 3), Train edges labeled: (22898, 3)\n",
      "Test edges all: (77512, 3), Test edges labeled: (13726, 3)\n",
      "Test data labeled grouped by timestep: [(35, 1341), (36, 1708), (37, 498), (38, 756), (39, 1183), (40, 1211), (41, 1132), (42, 2154), (43, 1370), (44, 1591), (45, 1221), (46, 712), (47, 846), (48, 471), (49, 476)]\n",
      "Test edges labeled grouped by timestep: [(35, 1002), (36, 1148), (37, 423), (38, 653), (39, 1055), (40, 1180), (41, 1048), (42, 1443), (43, 935), (44, 1497), (45, 1346), (46, 388), (47, 822), (48, 371), (49, 415)]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dgl\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "txs_classes = pd.read_csv('../data/elliptic_txs_classes.csv')\n",
    "txs_edges = pd.read_csv('../data/elliptic_txs_edgelist.csv')\n",
    "txs_features = pd.read_csv('../data/elliptic_txs_features.csv', header=None)\n",
    "\n",
    "\n",
    "# join features with classes using tx id (1st column of txs_features)\n",
    "txs_data = txs_features.merge(txs_classes, left_on=0, right_on='txId', how='left')\n",
    "\n",
    "# convert class labels to integers\n",
    "# 1: licit (0), 2: illicit (1), unknown: -1\n",
    "label_mapping = {'1': 0, '2': 1, 'unknown': -1}\n",
    "txs_data['class'] = txs_data['class'].map(label_mapping).astype(int)\n",
    "\n",
    "# split data and edges into train and test according to timestep (2nd column of txs_features)\n",
    "train_data_all = txs_data[txs_data[1] <= 34]\n",
    "test_data_all = txs_data[txs_data[1] > 34]\n",
    "\n",
    "# separate datasets with labels(1 or 2) from those without labels(class=unknown)\n",
    "train_data_labeled = train_data_all[train_data_all['class'].isin([0, 1])]\n",
    "test_data_labeled = test_data_all[test_data_all['class'].isin([0, 1])]\n",
    "\n",
    "# process edges like data: add timestep info and split into train and test\n",
    "txs_edges = txs_edges.merge(txs_features[[0, 1]], left_on='txId1', right_on=0, how='left').rename(columns={1: 'timestep'}).drop(columns=[0])\n",
    "train_edges_all = txs_edges[txs_edges['timestep'] <= 34]\n",
    "test_edges_all = txs_edges[txs_edges['timestep'] > 34]\n",
    "train_edges_labeled = train_edges_all[train_edges_all['txId1'].isin(train_data_labeled['txId']) & train_edges_all['txId2'].isin(train_data_labeled['txId'])]\n",
    "test_edges_labeled = test_edges_all[test_edges_all['txId1'].isin(test_data_labeled['txId']) & test_edges_all['txId2'].isin(test_data_labeled['txId'])]\n",
    "\n",
    "# print sizes of datasets\n",
    "print(f\"Train data all: {train_data_all.shape}, Train data labeled: {train_data_labeled.shape}\")\n",
    "print(f\"Test data all: {test_data_all.shape}, Test data labeled: {test_data_labeled.shape}\")\n",
    "print(f\"Train edges all: {train_edges_all.shape}, Train edges labeled: {train_edges_labeled.shape}\")\n",
    "print(f\"Test edges all: {test_edges_all.shape}, Test edges labeled: {test_edges_labeled.shape}\")\n",
    "\n",
    "# test data group dict, group by timestep for evaluation on each timestep\n",
    "test_data_labeled_timestep = {}\n",
    "test_edges_labeled_timestep = {}\n",
    "for t in range(35, 50):\n",
    "    test_data_labeled_timestep[t] = test_data_labeled[test_data_labeled[1] == t]\n",
    "    test_edges_labeled_timestep[t] = test_edges_labeled[test_edges_labeled['timestep'] == t]\n",
    "print(f\"Test data labeled grouped by timestep: {[ (t, df.shape[0]) for t, df in test_data_labeled_timestep.items() ]}\")\n",
    "print(f\"Test edges labeled grouped by timestep: {[ (t, df.shape[0]) for t, df in test_edges_labeled_timestep.items() ]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b29a3",
   "metadata": {},
   "source": [
    "### Create DGL graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b74b4b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labeled graph: 29894 nodes, 22898 edges\n",
      "Test labeled graph: 16670 nodes, 13726 edges\n",
      "Train all graph: 136265 nodes, 156843 edges\n",
      "Test all graph: 67504 nodes, 77512 edges\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create DGL graphs for train and test data\n",
    "def create_dgl_graph(data, edges):\n",
    "\n",
    "    node_ids = data['txId'].tolist()\n",
    "    id_to_idx = {node_id: idx for idx, node_id in enumerate(node_ids)}\n",
    "    \n",
    "    src = edges['txId1'].map(id_to_idx).tolist()\n",
    "    dst = edges['txId2'].map(id_to_idx).tolist()\n",
    "    \n",
    "    g = dgl.graph((src, dst), num_nodes=len(node_ids))\n",
    "    features = torch.tensor(data.iloc[:, 2:-2].values, dtype=torch.float32)\n",
    "    labels = torch.tensor(data['class'].values, dtype=torch.long)\n",
    "    \n",
    "    g.ndata['feat'] = features\n",
    "    g.ndata['label'] = labels\n",
    "\n",
    "    timestep_col = next((col for col in ('timestep', 1, '1') if col in data.columns), None)\n",
    "    if timestep_col is not None:\n",
    "        timesteps = torch.tensor(data[timestep_col].to_numpy(dtype='int64'), dtype=torch.long)\n",
    "        g.ndata['timestep'] = timesteps\n",
    "    else:\n",
    "        warnings.warn(\n",
    "            'create_dgl_graph could not find a timestep column; downstream modules requiring time context will fail.',\n",
    "            RuntimeWarning,\n",
    "        )\n",
    "    \n",
    "    return g\n",
    "\n",
    "# graph with all features\n",
    "train_labeled_graph = create_dgl_graph(train_data_labeled, train_edges_labeled)\n",
    "test_labeled_graph = create_dgl_graph(test_data_labeled, test_edges_labeled)\n",
    "train_all_graph = create_dgl_graph(train_data_all, train_edges_all)\n",
    "test_all_graph = create_dgl_graph(test_data_all, test_edges_all)\n",
    "\n",
    "print(f\"Train labeled graph: {train_labeled_graph.num_nodes()} nodes, {train_labeled_graph.num_edges()} edges\")\n",
    "print(f\"Test labeled graph: {test_labeled_graph.num_nodes()} nodes, {test_labeled_graph.num_edges()} edges\")\n",
    "print(f\"Train all graph: {train_all_graph.num_nodes()} nodes, {train_all_graph.num_edges()} edges\")\n",
    "print(f\"Test all graph: {test_all_graph.num_nodes()} nodes, {test_all_graph.num_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e79d7fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a73b9388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Preparing graphs: labeled_only=True =================\n",
      "Training Graph: Percentage isolated: 21.46%; Isolated nodes: 6415; Total nodes: 29894\n",
      "Test Graph: Percentage isolated: 25.64%; Total nodes: 16670; Isolated nodes: 4275\n",
      "\n",
      "============= Preparing graphs: labeled_only=False =================\n",
      "Training Graph: Percentage isolated: 0.00%; Isolated nodes: 0; Total nodes: 136265\n",
      "Test Graph: Percentage isolated: 0.00%; Total nodes: 67504; Isolated nodes: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare_training\n",
    "def prepare_train_graphs(labeled_only=True):\n",
    "    print(f\"============= Preparing graphs: labeled_only={labeled_only} =================\")\n",
    "    if labeled_only:\n",
    "        train_graph = dgl.to_bidirected(train_labeled_graph, copy_ndata=True)\n",
    "        test_graph = dgl.to_bidirected(test_labeled_graph, copy_ndata=True)\n",
    "    else:\n",
    "        train_graph = dgl.to_bidirected(train_all_graph, copy_ndata=True)\n",
    "        test_graph = dgl.to_bidirected(test_all_graph, copy_ndata=True)\n",
    "\n",
    "    # --- Check Train Graph ---\n",
    "    train_degrees = train_graph.in_degrees()\n",
    "    train_isolated_nodes = (train_degrees == 0).sum().item()\n",
    "    train_total_nodes = train_graph.num_nodes()\n",
    "    train_percent_isolated = (train_isolated_nodes / train_total_nodes) * 100\n",
    "\n",
    "    print(f\"Training Graph: Percentage isolated: {train_percent_isolated:.2f}%; Isolated nodes: {train_isolated_nodes}; Total nodes: {train_total_nodes}\")\n",
    "\n",
    "    # --- Check Test Graph ---\n",
    "    test_degrees = test_graph.in_degrees()\n",
    "    test_isolated_nodes = (test_degrees == 0).sum().item()\n",
    "    test_total_nodes = test_graph.num_nodes()\n",
    "    test_percent_isolated = (test_isolated_nodes / test_total_nodes) * 100\n",
    "\n",
    "    print(f\"Test Graph: Percentage isolated: {test_percent_isolated:.2f}%; Total nodes: {test_total_nodes}; Isolated nodes: {test_isolated_nodes}\\n\")\n",
    "\n",
    "    train_graph = dgl.add_self_loop(train_graph)\n",
    "    test_graph = dgl.add_self_loop(test_graph)\n",
    "\n",
    "    train_features = train_graph.ndata['feat']\n",
    "    train_labels = train_graph.ndata['label']\n",
    "    train_mask = (train_labels >= 0)\n",
    "    test_features = test_graph.ndata['feat']\n",
    "    test_labels = test_graph.ndata['label']\n",
    "    test_mask = (test_labels >= 0)\n",
    "\n",
    "    train_graph = train_graph.to(device)\n",
    "    test_graph = test_graph.to(device)\n",
    "    train_features = train_features.to(device)\n",
    "    train_labels = train_labels.to(device)\n",
    "    test_features = test_features.to(device)\n",
    "    test_labels = test_labels.to(device)\n",
    "\n",
    "    return train_graph, train_features, train_labels, train_mask, test_graph, test_features, test_labels, test_mask\n",
    "\n",
    "train_graph_labeled, train_feature_labeled, train_labels_labeled, train_mask_labeled, \\\n",
    "    test_graph_labeled, test_features_labeled, test_labels_labeled, test_mask_labeled \\\n",
    "        = prepare_train_graphs(labeled_only=True)\n",
    "\n",
    "train_graph_all, train_feature_all, train_labels_all, train_mask_all, \\\n",
    "    test_graph_all, test_features_all, test_labels_all, test_mask_all \\\n",
    "        = prepare_train_graphs(labeled_only=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b222f7f1",
   "metadata": {},
   "source": [
    "### Training and Evaluate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "336f0d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, test_graph, test_features, test_labels, test_mask, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_logits = model(test_graph, test_features)\n",
    "        test_preds = test_logits.argmax(dim=1)\n",
    "        test_loss = criterion(test_logits, test_labels).item()\n",
    "        \n",
    "        # Get masked predictions and labels\n",
    "        masked_preds = test_preds[test_mask].cpu().numpy()\n",
    "        masked_labels = test_labels[test_mask].cpu().numpy()\n",
    "        \n",
    "        # Use sklearn's precision_recall_fscore_support with zero_division handling\n",
    "        # pos_label=0 for illicit class, average=None to get per-class metrics\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            masked_labels, masked_preds, \n",
    "            labels=[0, 1],  # illicit=0, licit=1\n",
    "            average=None,\n",
    "            zero_division=0.0\n",
    "        )\n",
    "        \n",
    "        # Extract illicit (class 0) metrics\n",
    "        test_precision = precision[0] if len(precision) > 0 else 0.0\n",
    "        test_recall = recall[0] if len(recall) > 0 else 0.0\n",
    "        test_f1 = f1[0] if len(f1) > 0 else 0.0\n",
    "        \n",
    "        test_report = classification_report(\n",
    "            masked_labels, masked_preds, \n",
    "            target_names=['illicit', 'licit'],\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        return test_loss, test_precision, test_recall, test_f1, test_report\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "\n",
    "    # --- Create X-Axes ---\n",
    "    \n",
    "    # 1. X-axis for 'train_loss' (1 point per epoch)\n",
    "    # We add a small offset (0.5) to starting_epoch for clearer plotting\n",
    "    # if we resume training, so the first point isn't hidden.\n",
    "    starting_epoch = history.get('test_epochs', [1])[0] - 1\n",
    "    total_train_epochs = len(history.get('train_loss', []))\n",
    "    train_loss_epochs = list(range(starting_epoch + 1, starting_epoch + 1 + total_train_epochs))\n",
    "\n",
    "    # 2. X-axis for all test/validation metrics (sparse)\n",
    "    # This list is saved directly in our history object\n",
    "    eval_epochs = history.get('test_epochs', [])\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # --- Plot 1: Loss (Train vs. Test) ---\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    # Plot Train Loss (dense)\n",
    "    if train_loss_epochs:\n",
    "        plt.plot(train_loss_epochs, history['train_loss'], label='Train Loss', alpha=0.7, zorder=1)\n",
    "    \n",
    "    # Plot Test Loss (sparse)\n",
    "    if eval_epochs:\n",
    "        plt.plot(eval_epochs, history['test_loss'], label='Test Loss', \n",
    "                 marker='o', linestyle='--', linewidth=2, markersize=5, zorder=2)\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{model_name} Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # --- Plot 2: F1 Score (Train vs. Test) ---\n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    # Plot Train F1 (sparse, from eval steps)\n",
    "    if eval_epochs and 'train_f1' in history:\n",
    "        plt.plot(eval_epochs, history['train_f1'], label='Train F1 Score', \n",
    "                 marker='s', linestyle=':', linewidth=2, markersize=5)\n",
    "    \n",
    "    # Plot Test F1 (sparse, from eval steps)\n",
    "    if eval_epochs and 'test_f1' in history:\n",
    "        plt.plot(eval_epochs, history['test_f1'], label='Test F1 Score', \n",
    "                 color='orange', marker='o', linestyle='--', linewidth=2, markersize=5)\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title(f'{model_name} F1 Score at Evaluation Steps')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_model(model, model_name, optimizer, criterion, train_graph, train_features, train_labels, train_mask,\n",
    "                test_graph, test_features, test_labels, test_mask,\n",
    "                num_epochs, test_every=100, previous_history=None, print_best_report=True, show_plots=False,\n",
    "                early_stopping_patience=None, checkpoint_path=None):\n",
    "    # if previous_history is provided, resume training from there\n",
    "    if previous_history is not None:\n",
    "        history = previous_history\n",
    "        starting_epoch = len(history[\"train_loss\"]) + 1\n",
    "    else:\n",
    "        history = {\"train_loss\": [], \"train_f1\": [], \"train_precision\": [], \"train_recall\": [],\n",
    "               \"test_loss\": [], \"test_f1\": [], \"test_precision\": [], \"test_recall\": [],\n",
    "               \"test_epochs\": [],\n",
    "               \"best_test_f1\": 0.0, \"best_report\": None, \"best_model_state\": None, \"best_epoch\": -1, \n",
    "               \"last_test_f1\": 0.0, \"last_report\": None, \"latest_model_state\": None}\n",
    "        starting_epoch = 1\n",
    "\n",
    "    epochs_since_improvement = 0\n",
    "    checkpoint_target = checkpoint_path\n",
    "    if checkpoint_target:\n",
    "        checkpoint_dir = os.path.dirname(checkpoint_target)\n",
    "        if checkpoint_dir:\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    final_epoch_of_training = starting_epoch + num_epochs - 1\n",
    "    # training \n",
    "    for epoch in range(starting_epoch, final_epoch_of_training + 1):\n",
    "        model.train()\n",
    "        logits = model(train_graph, train_features)\n",
    "        loss = criterion(logits, train_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history[\"train_loss\"].append(loss.item())\n",
    "\n",
    "        if epoch == 1 or epoch % test_every == 0 or epoch == final_epoch_of_training:\n",
    "            history[\"test_epochs\"].append(epoch)\n",
    "            train_pred = logits.argmax(dim=1)\n",
    "            train_precision = ((train_pred[train_mask] == 0) & (train_labels[train_mask] == 0)).sum().item() / (train_pred[train_mask] == 0).sum().item()\n",
    "            train_recall = ((train_pred[train_mask] == 0) & (train_labels[train_mask] == 0)).sum().item() / (train_labels[train_mask] == 0).sum().item()\n",
    "            train_f1 = 2 * train_precision * train_recall / (train_precision + train_recall)\n",
    "            history[\"train_f1\"].append(train_f1)\n",
    "            history[\"train_precision\"].append(train_precision)\n",
    "            history[\"train_recall\"].append(train_recall)\n",
    "            \n",
    "            test_loss, test_precision, test_recall, test_f1, test_report = evaluate_model(model, test_graph, test_features, test_labels, test_mask, criterion)\n",
    "            history[\"test_loss\"].append(test_loss)\n",
    "            history[\"test_f1\"].append(test_f1)\n",
    "            history[\"test_precision\"].append(test_precision)\n",
    "            history[\"test_recall\"].append(test_recall)\n",
    "            print(f\"Epoch {epoch:03d}: Loss {loss.item():.4f}, Train F1 {train_f1:.4f}, Test Loss {test_loss:.4f}, Test F1 {test_f1:.4f}\")\n",
    "            improved = False\n",
    "        \n",
    "            if test_f1 > history[\"best_test_f1\"]:\n",
    "                history[\"best_test_f1\"] = test_f1\n",
    "                history[\"best_report\"] = test_report\n",
    "                history[\"best_epoch\"] = epoch\n",
    "                history[\"best_model_state\"] = model.state_dict()\n",
    "                improved = True\n",
    "                if checkpoint_target:\n",
    "                    torch.save(history[\"best_model_state\"], checkpoint_target)\n",
    "            if early_stopping_patience is not None:\n",
    "                if improved:\n",
    "                    epochs_since_improvement = 0\n",
    "                else:\n",
    "                    epochs_since_improvement += 1\n",
    "                    if epochs_since_improvement >= early_stopping_patience:\n",
    "                        print(f\"Early stopping at epoch {epoch} after {early_stopping_patience} eval steps without improvement\")\n",
    "                        break\n",
    "\n",
    "    # final evaluation on test set\n",
    "    # final_test_loss, final_test_precision, final_test_recall, final_test_f1, final_test_report = evaluate_model(model, test_graph, test_features, test_labels, test_mask, criterion)\n",
    "    history[\"latest_model_state\"] = model.state_dict()\n",
    "    history[\"last_test_f1\"] = history[\"test_f1\"][-1]\n",
    "    history[\"last_report\"] = test_report\n",
    "\n",
    "    print(f\"{model_name} Last Classification Report on Labeled Test Graph:\")\n",
    "    print(history[\"last_report\"])\n",
    "\n",
    "    if print_best_report:\n",
    "        print(f\"{model_name} Best Classification Report on Labeled Test Graph at epoch {history['best_epoch']}:\")\n",
    "        print(history[\"best_report\"])\n",
    "\n",
    "    if show_plots:\n",
    "        plot_training_history(history, model_name)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05fcb94",
   "metadata": {},
   "source": [
    "### Timestep-wise Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de52a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time steped testing graphs\n",
    "test_labeled_graphs_timestep = {}\n",
    "for t in range(35, 50):\n",
    "    data_t = test_data_labeled_timestep[t]\n",
    "    edges_t = test_edges_labeled_timestep[t]\n",
    "    test_graph_t = create_dgl_graph(data_t, edges_t)\n",
    "    test_graph_t = dgl.to_bidirected(test_graph_t, copy_ndata=True)\n",
    "    test_graph_t = dgl.add_self_loop(test_graph_t)\n",
    "    test_graph_t = test_graph_t.to(device)\n",
    "    test_feature_t = test_graph_t.ndata['feat'].to(device)\n",
    "    test_label_t = test_graph_t.ndata['label'].to(device)\n",
    "    test_mask_t = (test_label_t >= 0).to(device)\n",
    "    test_labeled_graphs_timestep[t] = (test_graph_t, test_feature_t, test_label_t, test_mask_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92c2f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate a trained model on per-timestep labeled test graphs and aggregate before/after t=43.\n",
    "# Uses existing variables: test_labeled_graphs_timestep, gt (model), device, classification_report, dgl, torch, criterion\n",
    "def evaluate_timestep(model_to_test, feature_builder=None):\n",
    "    model_to_test.eval()\n",
    "\n",
    "    if feature_builder is None:\n",
    "        def _identity_builder(_, feats):\n",
    "            return feats\n",
    "        feature_builder = _identity_builder\n",
    "\n",
    "    per_t_metrics = {}\n",
    "    all_preds_before = []\n",
    "    all_trues_before = []\n",
    "    all_preds_after = []\n",
    "    all_trues_after = []\n",
    "    f1_list = []\n",
    "\n",
    "    for t in sorted(test_labeled_graphs_timestep.keys()):\n",
    "        g_eval, feats, labels, mask = test_labeled_graphs_timestep[t]\n",
    "        if g_eval is None or g_eval.num_nodes() == 0:\n",
    "            print(f\"t={t}: empty graph, skipping\")\n",
    "            continue\n",
    "\n",
    "        feats_for_model = feature_builder(g_eval, feats)\n",
    "        loss, precision, recall, f1, report = evaluate_model(model_to_test, g_eval, feats_for_model, labels, mask, criterion)\n",
    "        f1_list.append(f1)\n",
    "        per_t_metrics[t] = {\n",
    "            \"n_nodes\": int(g_eval.num_nodes()),\n",
    "            \"loss\": loss,\n",
    "            \"precision_illicit\": precision,\n",
    "            \"recall_illicit\": recall,\n",
    "            \"f1_illicit\": f1,\n",
    "            \"classification_report\": report\n",
    "        }\n",
    "\n",
    "        # record masked count\n",
    "        per_t_metrics[t][\"n_masked\"] = int(mask.sum().item())\n",
    "\n",
    "        # get predictions for masked nodes\n",
    "        with torch.no_grad():\n",
    "            logits = model_to_test(g_eval, feats_for_model)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "        masked_preds = preds[mask]\n",
    "        masked_labels = labels[mask]\n",
    "\n",
    "        # accumulate for before/after 43\n",
    "        arr_preds = masked_preds.cpu().numpy()\n",
    "        arr_trues = masked_labels.cpu().numpy()\n",
    "        if t < 43:\n",
    "            all_preds_before.append(arr_preds)\n",
    "            all_trues_before.append(arr_trues)\n",
    "        else:\n",
    "            all_preds_after.append(arr_preds)\n",
    "            all_trues_after.append(arr_trues)\n",
    "\n",
    "    # --- After the loop: aggregate and print reports ---\n",
    "    def aggregate_and_report(preds_list, trues_list, label=\"\"):\n",
    "        if not preds_list:\n",
    "            print(f\"No data for {label}\")\n",
    "            return None\n",
    "        preds_all = np.concatenate(preds_list)\n",
    "        trues_all = np.concatenate(trues_list)\n",
    "        # use sklearn to compute illicit-focused metrics (class 0 == illicit)\n",
    "\n",
    "        precision = float(precision_score(trues_all, preds_all, pos_label=0, zero_division=0))\n",
    "        recall = float(recall_score(trues_all, preds_all, pos_label=0, zero_division=0))\n",
    "        f1 = float(f1_score(trues_all, preds_all, pos_label=0, zero_division=0))\n",
    "        report = classification_report(trues_all, preds_all, target_names=['illicit', 'licit'], zero_division=0)\n",
    "        return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"report\": report}\n",
    "\n",
    "    agg_before = aggregate_and_report(all_preds_before, all_trues_before, label=\"t < 43\")\n",
    "    agg_after  = aggregate_and_report(all_preds_after,  all_trues_after,  label=\"t >= 43\")\n",
    "\n",
    "    return {\n",
    "        \"f1_list\": f1_list,\n",
    "        \"agg_before_f1\": agg_before[\"f1\"] if agg_before else None,\n",
    "        \"agg_after_f1\": agg_after[\"f1\"] if agg_after else None,\n",
    "        \"per_t_metrics\": per_t_metrics,\n",
    "        \"agg_before\": agg_before,\n",
    "        \"agg_after\": agg_after\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "baf79d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def report_timestep_performance(model_name, model, feature_builder=None):\n",
    "    metrics = evaluate_timestep(model, feature_builder=feature_builder)\n",
    "    print(f\"[{model_name}] per-timestep illicit metrics:\")\n",
    "    for t in sorted(metrics['per_t_metrics'].keys()):\n",
    "        metric = metrics['per_t_metrics'][t]\n",
    "        print(f\"  t={t:02d} | nodes={metric['n_masked']:5d} | Loss={metric['loss']:.4f} | P={metric['precision_illicit']:.4f} | R={metric['recall_illicit']:.4f} | F1={metric['f1_illicit']:.4f}\")\n",
    "    agg_before = metrics.get('agg_before')\n",
    "    if agg_before:\n",
    "        print(f\"  Aggregate t<43 | P={agg_before['precision']:.4f} | R={agg_before['recall']:.4f} | F1={agg_before['f1']:.4f}\")\n",
    "    agg_after = metrics.get('agg_after')\n",
    "    if agg_after:\n",
    "        print(f\"  Aggregate t>=43 | P={agg_after['precision']:.4f} | R={agg_after['recall']:.4f} | F1={agg_after['f1']:.4f}\")\n",
    "    print()\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817198c",
   "metadata": {},
   "source": [
    "### Save trained history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23e1d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_history(history, sub_dir, save_dir = \"checkpoints\"):\n",
    "    # Save training history (JSON-safe) and model states (.pt)\n",
    "    os.makedirs(os.path.join(save_dir, sub_dir), exist_ok=True)\n",
    "\n",
    "    # Extract and remove model-state entries from history before JSON serialization\n",
    "    best_state = history.get(\"best_model_state\", None)\n",
    "    latest_state = history.get(\"latest_model_state\", None)\n",
    "\n",
    "    history_copy = {k: v for k, v in history.items() if k not in (\"best_model_state\", \"latest_model_state\")}\n",
    "\n",
    "    history_path = os.path.join(save_dir, sub_dir, \"history.json\")\n",
    "    with open(history_path, \"w\") as f:\n",
    "        json.dump(history_copy, f, indent=2)\n",
    "\n",
    "    # Save model state dicts (if present)\n",
    "\n",
    "    if best_state is not None:\n",
    "        torch.save(best_state, os.path.join(save_dir, sub_dir, \"best_model_state.pt\"))\n",
    "    if latest_state is not None:\n",
    "        torch.save(latest_state, os.path.join(save_dir, sub_dir, \"latest_model_state.pt\"))\n",
    "\n",
    "    print(\"Saved:\")\n",
    "    print(f\" - History JSON: {history_path}\")\n",
    "    if best_state is not None:\n",
    "        print(f\" - Best state: {os.path.join(save_dir, sub_dir, 'best_model_state.pt')}\")\n",
    "    if latest_state is not None:\n",
    "        print(f\" - Latest state: {os.path.join(save_dir, sub_dir, 'latest_model_state.pt')}\")\n",
    "\n",
    "def load_history(sub_dir, save_dir=\"checkpoints\", model=None, map_location=None):\n",
    "    \"\"\"\n",
    "    Load saved training history and model state dicts from disk.\n",
    "\n",
    "    Args:\n",
    "        sub_dir (str): subdirectory under save_dir where files are stored.\n",
    "        save_dir (str): root checkpoints directory (default \"checkpoints\").\n",
    "        model (nn.Module, optional): if provided and a gt_state_dict exists, it will be loaded into this model.\n",
    "        map_location (str or torch.device, optional): passed to torch.load (default \"cpu\" if None).\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"history\": dict or None,\n",
    "            \"best_model_state\": state_dict or None,\n",
    "            \"latest_model_state\": state_dict or None,\n",
    "            \"gt_state_dict\": state_dict or None,\n",
    "            \"model_loaded\": bool\n",
    "        }\n",
    "    \"\"\"\n",
    "    base_path = os.path.join(save_dir, sub_dir)\n",
    "    if map_location is None:\n",
    "        map_location = \"cpu\"\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    if not os.path.isdir(base_path):\n",
    "        raise FileNotFoundError(f\"Directory not found: {base_path}\")\n",
    "\n",
    "    # history JSON (name used by save_history)\n",
    "    history_path = os.path.join(base_path, \"history.json\")\n",
    "    if os.path.exists(history_path):\n",
    "        with open(history_path, \"r\") as f:\n",
    "            result = json.load(f)\n",
    "\n",
    "    def _load_pt(fname):\n",
    "        p = os.path.join(base_path, fname)\n",
    "        if os.path.exists(p):\n",
    "            return torch.load(p, map_location=map_location)\n",
    "        return None\n",
    "\n",
    "    result[\"best_model_state\"] = _load_pt(\"best_model_state.pt\")\n",
    "    result[\"latest_model_state\"] = _load_pt(\"latest_model_state.pt\")\n",
    "\n",
    "    # brief prints for confirmation\n",
    "    print(f\"Loaded history from {base_path}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d4eb5",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de5bb0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba782f92",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06e576",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dea35088",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, g, feat):\n",
    "        h = self.conv1(g, feat)\n",
    "        h = torch.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01025789",
   "metadata": {},
   "source": [
    "### Training without unknown nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a35ff4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.6558, Train F1 0.1194, Test Loss 0.7959, Test F1 0.1635\n",
      "Epoch 050: Loss 0.2000, Train F1 0.7618, Test Loss 0.3317, Test F1 0.3696\n",
      "Epoch 100: Loss 0.1586, Train F1 0.8169, Test Loss 0.2861, Test F1 0.4778\n",
      "Epoch 150: Loss 0.1376, Train F1 0.8427, Test Loss 0.2768, Test F1 0.5171\n",
      "Epoch 200: Loss 0.1239, Train F1 0.8556, Test Loss 0.2756, Test F1 0.5328\n",
      "Epoch 250: Loss 0.1139, Train F1 0.8676, Test Loss 0.2771, Test F1 0.5460\n",
      "Epoch 300: Loss 0.1059, Train F1 0.8783, Test Loss 0.2819, Test F1 0.5584\n",
      "Epoch 350: Loss 0.0990, Train F1 0.8860, Test Loss 0.2887, Test F1 0.5766\n",
      "Epoch 400: Loss 0.0929, Train F1 0.8931, Test Loss 0.2973, Test F1 0.5891\n",
      "Epoch 450: Loss 0.0875, Train F1 0.8985, Test Loss 0.3058, Test F1 0.6031\n",
      "Epoch 500: Loss 0.0828, Train F1 0.9032, Test Loss 0.3145, Test F1 0.6106\n",
      "Epoch 550: Loss 0.0787, Train F1 0.9078, Test Loss 0.3247, Test F1 0.6171\n",
      "Epoch 600: Loss 0.0752, Train F1 0.9115, Test Loss 0.3327, Test F1 0.6204\n",
      "Epoch 650: Loss 0.0720, Train F1 0.9140, Test Loss 0.3390, Test F1 0.6230\n",
      "Epoch 700: Loss 0.0691, Train F1 0.9177, Test Loss 0.3466, Test F1 0.6226\n",
      "Epoch 750: Loss 0.0665, Train F1 0.9210, Test Loss 0.3538, Test F1 0.6253\n",
      "Epoch 800: Loss 0.0642, Train F1 0.9242, Test Loss 0.3591, Test F1 0.6269\n",
      "Epoch 850: Loss 0.0621, Train F1 0.9262, Test Loss 0.3662, Test F1 0.6247\n",
      "Epoch 900: Loss 0.0602, Train F1 0.9298, Test Loss 0.3734, Test F1 0.6247\n",
      "Epoch 950: Loss 0.0586, Train F1 0.9319, Test Loss 0.3796, Test F1 0.6206\n",
      "Epoch 1000: Loss 0.0571, Train F1 0.9340, Test Loss 0.3865, Test F1 0.6177\n",
      "Epoch 1050: Loss 0.0557, Train F1 0.9365, Test Loss 0.3923, Test F1 0.6194\n",
      "Epoch 1100: Loss 0.0544, Train F1 0.9379, Test Loss 0.3980, Test F1 0.6173\n",
      "Epoch 1150: Loss 0.0532, Train F1 0.9400, Test Loss 0.4029, Test F1 0.6183\n",
      "Epoch 1200: Loss 0.0521, Train F1 0.9406, Test Loss 0.4064, Test F1 0.6296\n",
      "Epoch 1250: Loss 0.0510, Train F1 0.9418, Test Loss 0.4100, Test F1 0.6308\n",
      "Epoch 1300: Loss 0.0501, Train F1 0.9446, Test Loss 0.4131, Test F1 0.6330\n",
      "Epoch 1350: Loss 0.0492, Train F1 0.9455, Test Loss 0.4161, Test F1 0.6331\n",
      "Epoch 1400: Loss 0.0484, Train F1 0.9458, Test Loss 0.4200, Test F1 0.6339\n",
      "Epoch 1450: Loss 0.0476, Train F1 0.9475, Test Loss 0.4225, Test F1 0.6342\n",
      "Epoch 1500: Loss 0.0470, Train F1 0.9474, Test Loss 0.4256, Test F1 0.6322\n",
      "GCN - Labeled Only Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.77      0.54      0.63      1083\n",
      "       licit       0.97      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.87      0.76      0.81     16670\n",
      "weighted avg       0.96      0.96      0.96     16670\n",
      "\n",
      "GCN - Labeled Only Best Classification Report on Labeled Test Graph at epoch 1450:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.78      0.54      0.63      1083\n",
      "       licit       0.97      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.87      0.76      0.81     16670\n",
      "weighted avg       0.96      0.96      0.96     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/GCN_base_labeled/history.json\n",
      " - Best state: checkpoints/GCN_base_labeled/best_model_state.pt\n",
      " - Latest state: checkpoints/GCN_base_labeled/latest_model_state.pt\n",
      "[GCN - Labeled Only] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.1771 | P=0.8415 | R=0.8462 | F1=0.8438\n",
      "  t=36 | nodes= 1708 | Loss=0.1013 | P=0.3939 | R=0.3939 | F1=0.3939\n",
      "  t=37 | nodes=  498 | Loss=0.2881 | P=0.7778 | R=0.5250 | F1=0.6269\n",
      "  t=38 | nodes=  756 | Loss=0.3348 | P=0.8462 | R=0.7928 | F1=0.8186\n",
      "  t=39 | nodes= 1183 | Loss=0.2181 | P=0.8000 | R=0.4938 | F1=0.6107\n",
      "  t=40 | nodes= 1211 | Loss=0.6119 | P=0.7778 | R=0.3750 | F1=0.5060\n",
      "  t=41 | nodes= 1132 | Loss=1.3068 | P=0.7889 | R=0.6121 | F1=0.6893\n",
      "  t=42 | nodes= 2154 | Loss=0.4577 | P=0.9379 | R=0.6318 | F1=0.7550\n",
      "  t=43 | nodes= 1370 | Loss=0.2628 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.2976 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=45 | nodes= 1221 | Loss=0.0973 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0497 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=47 | nodes=  846 | Loss=0.4446 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=1.0552 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.6715 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.8262 | R=0.6346 | F1=0.7178\n",
      "  Aggregate t>=43 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "gcn_labeled = GCN(train_feature_labeled.shape[1], embedding_dim, 2).to(device)\n",
    "gcn_labeled_optimizer = torch.optim.Adam(gcn_labeled.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_gcn_labeled = train_model(\n",
    "    gcn_labeled,\n",
    "    \"GCN - Labeled Only\",\n",
    "    gcn_labeled_optimizer,\n",
    "    criterion,\n",
    "    train_graph_labeled,\n",
    "    train_feature_labeled,\n",
    "    train_labels_labeled,\n",
    "    train_mask_labeled,\n",
    "    test_graph_labeled,\n",
    "    test_features_labeled,\n",
    "    test_labels_labeled,\n",
    "    test_mask_labeled,\n",
    "    num_epochs=1500,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path=\"checkpoints/GCN/base_labeled/best_model_state.pt\"\n",
    ")\n",
    "\n",
    "save_history(history_gcn_labeled, sub_dir=\"GCN_base_labeled\")\n",
    "\n",
    "gcn_labeled_timestep_metrics = report_timestep_performance(\"GCN - Labeled Only\", gcn_labeled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee40bbb",
   "metadata": {},
   "source": [
    "### Training with unknown nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d440b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.7115, Train F1 0.1220, Test Loss 0.7125, Test F1 0.1446\n",
      "Epoch 050: Loss 0.2318, Train F1 0.7234, Test Loss 0.3764, Test F1 0.3312\n",
      "Epoch 100: Loss 0.1892, Train F1 0.7776, Test Loss 0.2986, Test F1 0.4245\n",
      "Epoch 150: Loss 0.1663, Train F1 0.8020, Test Loss 0.2766, Test F1 0.4721\n",
      "Epoch 200: Loss 0.1515, Train F1 0.8194, Test Loss 0.2730, Test F1 0.4770\n",
      "Epoch 250: Loss 0.1404, Train F1 0.8343, Test Loss 0.2768, Test F1 0.4871\n",
      "Epoch 300: Loss 0.1315, Train F1 0.8471, Test Loss 0.2824, Test F1 0.5157\n",
      "Epoch 350: Loss 0.1241, Train F1 0.8557, Test Loss 0.2871, Test F1 0.5324\n",
      "Epoch 400: Loss 0.1177, Train F1 0.8652, Test Loss 0.2906, Test F1 0.5384\n",
      "Epoch 450: Loss 0.1118, Train F1 0.8726, Test Loss 0.2939, Test F1 0.5483\n",
      "Epoch 500: Loss 0.1065, Train F1 0.8799, Test Loss 0.2987, Test F1 0.5537\n",
      "Epoch 550: Loss 0.1018, Train F1 0.8866, Test Loss 0.3038, Test F1 0.5577\n",
      "Epoch 600: Loss 0.0976, Train F1 0.8905, Test Loss 0.3086, Test F1 0.5584\n",
      "Epoch 650: Loss 0.0938, Train F1 0.8946, Test Loss 0.3142, Test F1 0.5544\n",
      "Epoch 700: Loss 0.0903, Train F1 0.8987, Test Loss 0.3196, Test F1 0.5518\n",
      "Epoch 750: Loss 0.0871, Train F1 0.9031, Test Loss 0.3245, Test F1 0.5540\n",
      "Epoch 800: Loss 0.0841, Train F1 0.9070, Test Loss 0.3300, Test F1 0.5530\n",
      "Epoch 850: Loss 0.0815, Train F1 0.9102, Test Loss 0.3350, Test F1 0.5524\n",
      "Epoch 900: Loss 0.0791, Train F1 0.9134, Test Loss 0.3402, Test F1 0.5479\n",
      "Epoch 950: Loss 0.0769, Train F1 0.9158, Test Loss 0.3455, Test F1 0.5472\n",
      "Epoch 1000: Loss 0.0748, Train F1 0.9205, Test Loss 0.3482, Test F1 0.5450\n",
      "Epoch 1050: Loss 0.0730, Train F1 0.9218, Test Loss 0.3517, Test F1 0.5507\n",
      "Epoch 1100: Loss 0.0712, Train F1 0.9243, Test Loss 0.3559, Test F1 0.5493\n",
      "Epoch 1150: Loss 0.0696, Train F1 0.9250, Test Loss 0.3605, Test F1 0.5480\n",
      "Epoch 1200: Loss 0.0682, Train F1 0.9277, Test Loss 0.3642, Test F1 0.5452\n",
      "Epoch 1250: Loss 0.0668, Train F1 0.9290, Test Loss 0.3673, Test F1 0.5440\n",
      "Epoch 1300: Loss 0.0655, Train F1 0.9304, Test Loss 0.3711, Test F1 0.5451\n",
      "Epoch 1350: Loss 0.0643, Train F1 0.9315, Test Loss 0.3751, Test F1 0.5417\n",
      "Epoch 1400: Loss 0.0631, Train F1 0.9330, Test Loss 0.3800, Test F1 0.5407\n",
      "Epoch 1450: Loss 0.0620, Train F1 0.9343, Test Loss 0.3850, Test F1 0.5390\n",
      "Epoch 1500: Loss 0.0609, Train F1 0.9360, Test Loss 0.3883, Test F1 0.5369\n",
      "GCN - All Nodes Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.59      0.49      0.54      1083\n",
      "       licit       0.97      0.98      0.97     15587\n",
      "\n",
      "    accuracy                           0.94     16670\n",
      "   macro avg       0.78      0.73      0.75     16670\n",
      "weighted avg       0.94      0.94      0.94     16670\n",
      "\n",
      "GCN - All Nodes Best Classification Report on Labeled Test Graph at epoch 600:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.71      0.46      0.56      1083\n",
      "       licit       0.96      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.84      0.72      0.77     16670\n",
      "weighted avg       0.95      0.95      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/GCN_base_all/history.json\n",
      " - Best state: checkpoints/GCN_base_all/best_model_state.pt\n",
      " - Latest state: checkpoints/GCN_base_all/latest_model_state.pt\n",
      "[GCN - All Nodes] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.1773 | P=0.8935 | R=0.8297 | F1=0.8604\n",
      "  t=36 | nodes= 1708 | Loss=0.1440 | P=0.2571 | R=0.5455 | F1=0.3495\n",
      "  t=37 | nodes=  498 | Loss=0.3000 | P=0.7619 | R=0.4000 | F1=0.5246\n",
      "  t=38 | nodes=  756 | Loss=0.3114 | P=0.8404 | R=0.7117 | F1=0.7707\n",
      "  t=39 | nodes= 1183 | Loss=0.2477 | P=0.5133 | R=0.7160 | F1=0.5979\n",
      "  t=40 | nodes= 1211 | Loss=0.6054 | P=0.9138 | R=0.4732 | F1=0.6235\n",
      "  t=41 | nodes= 1132 | Loss=0.8518 | P=0.8764 | R=0.6724 | F1=0.7610\n",
      "  t=42 | nodes= 2154 | Loss=0.4675 | P=0.9209 | R=0.6820 | F1=0.7837\n",
      "  t=43 | nodes= 1370 | Loss=0.4788 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.3673 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=45 | nodes= 1221 | Loss=0.0923 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0475 | P=0.1429 | R=0.5000 | F1=0.2222\n",
      "  t=47 | nodes=  846 | Loss=0.4157 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=1.1898 | P=0.0556 | R=0.0278 | F1=0.0370\n",
      "  t=49 | nodes=  476 | Loss=2.1084 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.7788 | R=0.6740 | F1=0.7226\n",
      "  Aggregate t>=43 | P=0.0081 | R=0.0118 | F1=0.0096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "gcn_all = GCN(train_feature_all.shape[1], embedding_dim, 2).to(device)\n",
    "gcn_all_optimizer = torch.optim.Adam(gcn_all.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_gcn_all = train_model(\n",
    "    gcn_all,\n",
    "    \"GCN - All Nodes\",\n",
    "    gcn_all_optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    train_feature_all,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    test_features_all,\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=1500,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path=\"checkpoints/GCN/base_all/best_model_state.pt\"\n",
    ")\n",
    "\n",
    "save_history(history_gcn_all, sub_dir=\"GCN_base_all\")\n",
    "\n",
    "gcn_all_timestep_metrics = report_timestep_performance(\"GCN - All Nodes\", gcn_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db967c5",
   "metadata": {},
   "source": [
    "## GCN with self supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b49395",
   "metadata": {},
   "source": [
    "### BGRL + GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced7e470",
   "metadata": {},
   "source": [
    "* Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e769f6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train graph for SSL: Graph(num_nodes=136265, num_edges=156843,\n",
      "      ndata_schemes={'feat': Scheme(shape=(165,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'timestep': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "# BGRL self-supervised pretraining on the full training set\n",
    "ssl_embedding_dim = 100\n",
    "pretrain_epochs = 30\n",
    "edge_drop_prob = 0.2\n",
    "feat_drop_prob = 0.1\n",
    "momentum = 0.99\n",
    "ssl_lr = 1e-3\n",
    "raw_feature_count = 94\n",
    "\n",
    "base_pretrain_graph = train_all_graph.to(device)\n",
    "pretrain_features = base_pretrain_graph.ndata['feat']\n",
    "num_total_features = pretrain_features.shape[1]\n",
    "agg_feature_count = num_total_features - raw_feature_count\n",
    "if agg_feature_count < 0:\n",
    "    raise ValueError(\"raw_feature_count is larger than available feature dimensions\")\n",
    "\n",
    "print(f\"Train graph for SSL: {base_pretrain_graph}\")\n",
    "\n",
    "# Pre-compute adjacency helper so edge dropout never isolates a node\n",
    "src_full, dst_full = base_pretrain_graph.edges()\n",
    "edge_indices = torch.arange(base_pretrain_graph.num_edges(), device=device)\n",
    "node_edge_nodes = torch.cat([src_full, dst_full])\n",
    "node_edge_edges = torch.cat([edge_indices, edge_indices])\n",
    "order = torch.argsort(node_edge_nodes)\n",
    "node_edge_nodes = node_edge_nodes[order]\n",
    "node_edge_edges = node_edge_edges[order]\n",
    "node_ptr = torch.searchsorted(node_edge_nodes, torch.arange(base_pretrain_graph.num_nodes() + 1, device=device))\n",
    "edge_helper = {\n",
    "    'src': src_full,\n",
    "    'dst': dst_full,\n",
    "    'node_edge_nodes': node_edge_nodes,\n",
    "    'node_edge_edges': node_edge_edges,\n",
    "    'node_ptr': node_ptr,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c646af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_masks(features):\n",
    "    raw_part = features[:, :raw_feature_count]\n",
    "    raw_mask = torch.empty_like(raw_part).uniform_(0.2, 0.4)\n",
    "    masked_raw = raw_part * raw_mask\n",
    "    if agg_feature_count == 0:\n",
    "        return masked_raw\n",
    "    agg_part = features[:, raw_feature_count:]\n",
    "    agg_mask = torch.empty_like(agg_part).uniform_(0.05, 0.2)\n",
    "    masked_agg = agg_part * agg_mask\n",
    "    return torch.cat([masked_raw, masked_agg], dim=1)\n",
    "\n",
    "\n",
    "def ensure_connectivity(keep_mask):\n",
    "    kept_idx = torch.nonzero(keep_mask, as_tuple=False).squeeze(1)\n",
    "    if kept_idx.numel() == 0:\n",
    "        random_edge = torch.randint(0, keep_mask.shape[0], (1,), device=keep_mask.device)\n",
    "        keep_mask[random_edge] = True\n",
    "        kept_idx = random_edge\n",
    "    num_nodes = base_pretrain_graph.num_nodes()\n",
    "    deg = torch.zeros(num_nodes, device=keep_mask.device, dtype=torch.int64)\n",
    "    deg.scatter_add_(0, src_full[kept_idx], torch.ones_like(kept_idx, dtype=torch.int64))\n",
    "    deg.scatter_add_(0, dst_full[kept_idx], torch.ones_like(kept_idx, dtype=torch.int64))\n",
    "    zero_nodes = (deg == 0).nonzero(as_tuple=False).squeeze(1)\n",
    "    if zero_nodes.numel() == 0:\n",
    "        return keep_mask\n",
    "    node_edge_nodes = edge_helper['node_edge_nodes']\n",
    "    node_edge_edges = edge_helper['node_edge_edges']\n",
    "    node_ptr = edge_helper['node_ptr']\n",
    "    for node in zero_nodes.tolist():\n",
    "        start = int(node_ptr[node].item())\n",
    "        end = int(node_ptr[node + 1].item())\n",
    "        if start == end:\n",
    "            continue\n",
    "        candidates = node_edge_edges[start:end]\n",
    "        chosen = candidates[torch.randint(0, candidates.shape[0], (1,), device=candidates.device)]\n",
    "        keep_mask[chosen] = True\n",
    "    return keep_mask\n",
    "\n",
    "\n",
    "def random_edge_dropout_preserve(drop_prob):\n",
    "    if drop_prob <= 0 or base_pretrain_graph.num_edges() == 0:\n",
    "        return base_pretrain_graph\n",
    "    mask = torch.rand(src_full.shape[0], device=device) > drop_prob\n",
    "    mask = ensure_connectivity(mask)\n",
    "    kept_idx = torch.nonzero(mask, as_tuple=False).squeeze(1)\n",
    "    aug = dgl.graph((src_full[kept_idx], dst_full[kept_idx]), num_nodes=base_pretrain_graph.num_nodes(), device=device)\n",
    "    return aug\n",
    "\n",
    "\n",
    "def graph_augment():\n",
    "    aug_graph = random_edge_dropout_preserve(edge_drop_prob)\n",
    "    aug_graph = dgl.add_self_loop(aug_graph)\n",
    "    masked_feats = apply_feature_masks(pretrain_features)\n",
    "    if feat_drop_prob > 0:\n",
    "        masked_feats = F.dropout(masked_feats, p=feat_drop_prob, training=True)\n",
    "    return aug_graph, masked_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "608bfee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projector(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class BGRL(nn.Module):\n",
    "    def __init__(self, encoder, hidden_dim, momentum=0.99):\n",
    "        super().__init__()\n",
    "        self.online_encoder = encoder\n",
    "        self.online_projector = Projector(hidden_dim)\n",
    "        self.target_encoder = copy.deepcopy(encoder)\n",
    "        self.target_projector = copy.deepcopy(self.online_projector)\n",
    "        for p in self.target_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.target_projector.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.predictor = Predictor(hidden_dim)\n",
    "        self.momentum = momentum\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_target(self):\n",
    "        for target_param, online_param in zip(self.target_encoder.parameters(), self.online_encoder.parameters()):\n",
    "            target_param.data = self.momentum * target_param.data + (1 - self.momentum) * online_param.data\n",
    "        for target_param, online_param in zip(self.target_projector.parameters(), self.online_projector.parameters()):\n",
    "            target_param.data = self.momentum * target_param.data + (1 - self.momentum) * online_param.data\n",
    "\n",
    "    def loss_fn(self, p, z):\n",
    "        p = F.normalize(p, dim=1)\n",
    "        z = F.normalize(z.detach(), dim=1)\n",
    "        return 2 - 2 * (p * z).sum(dim=1).mean()\n",
    "\n",
    "    def forward(self, g1, x1, g2, x2):\n",
    "        h1 = self.online_encoder(g1, x1)\n",
    "        h2 = self.online_encoder(g2, x2)\n",
    "        z1 = self.online_projector(h1)\n",
    "        z2 = self.online_projector(h2)\n",
    "        p1 = self.predictor(z1)\n",
    "        p2 = self.predictor(z2)\n",
    "        with torch.no_grad():\n",
    "            t1 = self.target_projector(self.target_encoder(g1, x1))\n",
    "            t2 = self.target_projector(self.target_encoder(g2, x2))\n",
    "        return self.loss_fn(p1, t2) + self.loss_fn(p2, t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8413fb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BGRL pretraining...\n",
      "SSL Epoch 0001 | Loss: 3.7973\n",
      "SSL Epoch 0020 | Loss: 0.3048\n",
      "Finished BGRL pretraining. The frozen encoder is available as `pretrained_encoder`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ssl_model = BGRL(GCN(num_total_features, ssl_embedding_dim, ssl_embedding_dim).to(device), ssl_embedding_dim, momentum=momentum).to(device)\n",
    "optimizer = torch.optim.Adam(ssl_model.parameters(), lr=ssl_lr, weight_decay=1e-4)\n",
    "\n",
    "print(\"Starting BGRL pretraining...\")\n",
    "for epoch in range(1, pretrain_epochs + 1):\n",
    "    g1, x1 = graph_augment()\n",
    "    g2, x2 = graph_augment()\n",
    "    loss = ssl_model(g1, x1, g2, x2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    ssl_model.update_target()\n",
    "\n",
    "    if epoch == 1 or epoch % 20 == 0:\n",
    "        print(f\"SSL Epoch {epoch:04d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "pretrained_encoder = ssl_model.online_encoder\n",
    "pretrained_encoder.eval()\n",
    "for param in pretrained_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Finished BGRL pretraining. The frozen encoder is available as `pretrained_encoder`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d3bc1",
   "metadata": {},
   "source": [
    "* BGRL Embeddings + Raw Features + GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d64168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 1.4804, Train F1 0.2082, Test Loss 1.2745, Test F1 0.1238\n",
      "Epoch 050: Loss 0.2380, Train F1 0.7108, Test Loss 0.3667, Test F1 0.3206\n",
      "Epoch 100: Loss 0.1990, Train F1 0.7599, Test Loss 0.3051, Test F1 0.3871\n",
      "Epoch 150: Loss 0.1763, Train F1 0.7874, Test Loss 0.2843, Test F1 0.4248\n",
      "Epoch 200: Loss 0.1605, Train F1 0.8099, Test Loss 0.2769, Test F1 0.4555\n",
      "Epoch 250: Loss 0.1487, Train F1 0.8247, Test Loss 0.2718, Test F1 0.4858\n",
      "Epoch 300: Loss 0.1389, Train F1 0.8346, Test Loss 0.2695, Test F1 0.5218\n",
      "Epoch 350: Loss 0.1305, Train F1 0.8494, Test Loss 0.2697, Test F1 0.5518\n",
      "Epoch 400: Loss 0.1230, Train F1 0.8565, Test Loss 0.2719, Test F1 0.5617\n",
      "Epoch 450: Loss 0.1165, Train F1 0.8637, Test Loss 0.2781, Test F1 0.5583\n",
      "Epoch 500: Loss 0.1106, Train F1 0.8700, Test Loss 0.2828, Test F1 0.5641\n",
      "Epoch 550: Loss 0.1051, Train F1 0.8775, Test Loss 0.2871, Test F1 0.5718\n",
      "Epoch 600: Loss 0.1002, Train F1 0.8858, Test Loss 0.2909, Test F1 0.5767\n",
      "BGRL Embeddings + Raw Features Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.78      0.46      0.58      1083\n",
      "       licit       0.96      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.87      0.72      0.78     16670\n",
      "weighted avg       0.95      0.96      0.95     16670\n",
      "\n",
      "BGRL Embeddings + Raw Features Best Classification Report on Labeled Test Graph at epoch 600:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.78      0.46      0.58      1083\n",
      "       licit       0.96      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.87      0.72      0.78     16670\n",
      "weighted avg       0.95      0.96      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/BGRL_gcn_concat/history.json\n",
      " - Best state: checkpoints/BGRL_gcn_concat/best_model_state.pt\n",
      " - Latest state: checkpoints/BGRL_gcn_concat/latest_model_state.pt\n",
      "[BGRL Embeddings + Raw Features] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.1593 | P=0.8922 | R=0.8187 | F1=0.8539\n",
      "  t=36 | nodes= 1708 | Loss=0.1017 | P=0.5333 | R=0.4848 | F1=0.5079\n",
      "  t=37 | nodes=  498 | Loss=0.2616 | P=0.7586 | R=0.5500 | F1=0.6377\n",
      "  t=38 | nodes=  756 | Loss=0.2884 | P=0.8426 | R=0.8198 | F1=0.8311\n",
      "  t=39 | nodes= 1183 | Loss=0.1926 | P=0.8312 | R=0.7901 | F1=0.8101\n",
      "  t=40 | nodes= 1211 | Loss=0.5256 | P=0.8980 | R=0.3929 | F1=0.5466\n",
      "  t=41 | nodes= 1132 | Loss=0.5566 | P=0.8222 | R=0.6379 | F1=0.7184\n",
      "  t=42 | nodes= 2154 | Loss=0.3960 | P=0.8935 | R=0.6318 | F1=0.7402\n",
      "  t=43 | nodes= 1370 | Loss=0.2451 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.2777 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=45 | nodes= 1221 | Loss=0.0832 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0480 | P=0.2500 | R=0.5000 | F1=0.3333\n",
      "  t=47 | nodes=  846 | Loss=0.3078 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.7860 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.1939 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.8498 | R=0.6685 | F1=0.7483\n",
      "  Aggregate t>=43 | P=0.0256 | R=0.0059 | F1=0.0096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'pretrained_encoder' not in globals():\n",
    "    raise RuntimeError('Run the BGRL pretraining cells first to populate `pretrained_encoder`.')\n",
    "\n",
    "pretrained_encoder = pretrained_encoder.to(device).eval()\n",
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ssl_train_embeddings = pretrained_encoder(train_graph_all, train_feature_all).detach()\n",
    "    ssl_test_embeddings = pretrained_encoder(test_graph_all, test_features_all).detach()\n",
    "\n",
    "ssl_aug_train = torch.cat([train_feature_all, ssl_train_embeddings], dim=1)\n",
    "ssl_aug_test = torch.cat([test_features_all, ssl_test_embeddings], dim=1)\n",
    "\n",
    "bgrl_concat_gcn = GCN(ssl_aug_train.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(bgrl_concat_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_bgrl_concat = train_model(\n",
    "    bgrl_concat_gcn,\n",
    "    'BGRL Embeddings + Raw Features',\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    ssl_aug_train,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    ssl_aug_test,\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=600,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path='checkpoints/BGRL/gcn_concat/best_model_state.pt'\n",
    ")\n",
    "\n",
    "save_history(history_bgrl_concat, sub_dir='BGRL_gcn_concat')\n",
    "\n",
    "\n",
    "def build_bgrl_concat_features(g_eval, feats):\n",
    "    g_eval = g_eval.to(device)\n",
    "    feats = feats.to(device)\n",
    "    with torch.no_grad():\n",
    "        ssl_embeds = pretrained_encoder(g_eval, feats)\n",
    "    return torch.cat([feats, ssl_embeds], dim=1)\n",
    "\n",
    "\n",
    "bgrl_concat_timestep_metrics = report_timestep_performance(\n",
    "    \"BGRL Embeddings + Raw Features\",\n",
    "    bgrl_concat_gcn,\n",
    "    feature_builder=build_bgrl_concat_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c806b3",
   "metadata": {},
   "source": [
    "* BGRL Node Score + Raw Features + GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d5957a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.5096, Train F1 0.1429, Test Loss 0.4972, Test F1 0.0867\n",
      "Epoch 050: Loss 0.1995, Train F1 0.7629, Test Loss 0.3199, Test F1 0.3685\n",
      "Epoch 100: Loss 0.1632, Train F1 0.8027, Test Loss 0.2931, Test F1 0.4075\n",
      "Epoch 150: Loss 0.1425, Train F1 0.8320, Test Loss 0.2883, Test F1 0.4439\n",
      "Epoch 200: Loss 0.1282, Train F1 0.8497, Test Loss 0.2895, Test F1 0.4947\n",
      "Epoch 250: Loss 0.1172, Train F1 0.8645, Test Loss 0.2933, Test F1 0.5345\n",
      "Epoch 300: Loss 0.1084, Train F1 0.8766, Test Loss 0.2979, Test F1 0.5547\n",
      "Epoch 350: Loss 0.1011, Train F1 0.8855, Test Loss 0.3019, Test F1 0.5670\n",
      "Epoch 400: Loss 0.0946, Train F1 0.8923, Test Loss 0.3085, Test F1 0.5711\n",
      "Epoch 450: Loss 0.0890, Train F1 0.8980, Test Loss 0.3145, Test F1 0.5699\n",
      "Epoch 500: Loss 0.0841, Train F1 0.9048, Test Loss 0.3198, Test F1 0.5730\n",
      "Epoch 550: Loss 0.0797, Train F1 0.9106, Test Loss 0.3236, Test F1 0.5752\n",
      "Epoch 600: Loss 0.0758, Train F1 0.9171, Test Loss 0.3283, Test F1 0.5781\n",
      "BGRL Node Score + Raw Features Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.68      0.50      0.58      1083\n",
      "       licit       0.97      0.98      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.82      0.74      0.78     16670\n",
      "weighted avg       0.95      0.95      0.95     16670\n",
      "\n",
      "BGRL Node Score + Raw Features Best Classification Report on Labeled Test Graph at epoch 600:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.68      0.50      0.58      1083\n",
      "       licit       0.97      0.98      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.82      0.74      0.78     16670\n",
      "weighted avg       0.95      0.95      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/BGRL_gcn_scores/history.json\n",
      " - Best state: checkpoints/BGRL_gcn_scores/best_model_state.pt\n",
      " - Latest state: checkpoints/BGRL_gcn_scores/latest_model_state.pt\n",
      "[BGRL Node Score + Raw Features] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.1635 | P=0.8947 | R=0.8407 | F1=0.8669\n",
      "  t=36 | nodes= 1708 | Loss=0.1214 | P=0.5000 | R=0.6061 | F1=0.5479\n",
      "  t=37 | nodes=  498 | Loss=0.2600 | P=0.8500 | R=0.4250 | F1=0.5667\n",
      "  t=38 | nodes=  756 | Loss=0.2502 | P=0.8349 | R=0.8198 | F1=0.8273\n",
      "  t=39 | nodes= 1183 | Loss=0.2154 | P=0.5889 | R=0.6543 | F1=0.6199\n",
      "  t=40 | nodes= 1211 | Loss=0.5496 | P=0.9245 | R=0.4375 | F1=0.5939\n",
      "  t=41 | nodes= 1132 | Loss=0.8623 | P=0.8804 | R=0.6983 | F1=0.7788\n",
      "  t=42 | nodes= 2154 | Loss=0.3918 | P=0.9257 | R=0.6778 | F1=0.7826\n",
      "  t=43 | nodes= 1370 | Loss=0.2942 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.2904 | P=0.0833 | R=0.0417 | F1=0.0556\n",
      "  t=45 | nodes= 1221 | Loss=0.0815 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0498 | P=0.2000 | R=0.5000 | F1=0.2857\n",
      "  t=47 | nodes=  846 | Loss=0.3573 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.9299 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.7459 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.8347 | R=0.6849 | F1=0.7524\n",
      "  Aggregate t>=43 | P=0.0116 | R=0.0118 | F1=0.0117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "\n",
    "if 'pretrained_encoder' not in globals():\n",
    "    raise RuntimeError('Run the BGRL pretraining cells first to populate `pretrained_encoder`.')\n",
    "\n",
    "if 'ssl_train_embeddings' not in globals() or 'ssl_test_embeddings' not in globals():\n",
    "    raise RuntimeError('Generate SSL embeddings before training the score-based head.')\n",
    "\n",
    "with torch.no_grad():\n",
    "    bgrl_train_scores = torch.norm(ssl_train_embeddings, dim=1, keepdim=True)\n",
    "    bgrl_test_scores = torch.norm(ssl_test_embeddings, dim=1, keepdim=True)\n",
    "\n",
    "bgrl_score_mean = bgrl_train_scores.mean()\n",
    "bgrl_score_std = bgrl_train_scores.std().clamp_min(1e-6)\n",
    "bgrl_train_scores = (bgrl_train_scores - bgrl_score_mean) / bgrl_score_std\n",
    "bgrl_test_scores = (bgrl_test_scores - bgrl_score_mean) / bgrl_score_std\n",
    "\n",
    "bgrl_score_train = torch.cat([train_feature_all, bgrl_train_scores.to(device)], dim=1)\n",
    "bgrl_score_test = torch.cat([test_features_all, bgrl_test_scores.to(device)], dim=1)\n",
    "\n",
    "bgrl_score_gcn = GCN(bgrl_score_train.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(bgrl_score_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_bgrl_score = train_model(\n",
    "    bgrl_score_gcn,\n",
    "    \"BGRL Node Score + Raw Features\",\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    bgrl_score_train,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    bgrl_score_test,\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=600,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path='checkpoints/BGRL/gcn_scores/best_model_state.pt'\n",
    ")\n",
    "\n",
    "save_history(history_bgrl_score, sub_dir='BGRL_gcn_scores')\n",
    "\n",
    "\n",
    "def build_bgrl_score_features(g_eval, feats):\n",
    "    g_eval = g_eval.to(device)\n",
    "    feats = feats.to(device)\n",
    "    with torch.no_grad():\n",
    "        ssl_embeds = pretrained_encoder(g_eval, feats)\n",
    "        scores = torch.norm(ssl_embeds, dim=1, keepdim=True)\n",
    "    scores = (scores - bgrl_score_mean) / bgrl_score_std\n",
    "    return torch.cat([feats, scores], dim=1)\n",
    "\n",
    "\n",
    "bgrl_score_timestep_metrics = report_timestep_performance(\n",
    "    \"BGRL Node Score + Raw Features\",\n",
    "    bgrl_score_gcn,\n",
    "    feature_builder=build_bgrl_score_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02bb1d",
   "metadata": {},
   "source": [
    "### GAE + GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43cdcd7",
   "metadata": {},
   "source": [
    "* Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d9df84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if 'train_all_graph' not in globals():\n",
    "    raise RuntimeError('Load the graphs first so `train_all_graph` is defined.')\n",
    "if 'GCN' not in globals():\n",
    "    raise RuntimeError('Run the GCN baseline cell to define the `GCN` encoder class.')\n",
    "\n",
    "gae_hidden_dim = 256\n",
    "gae_latent_dim = 128\n",
    "gae_epochs = 30\n",
    "gae_lr = 1e-3\n",
    "alpha_attr = 0.5\n",
    "neg_sample_ratio = 1.0\n",
    "max_struct_samples = 200_000\n",
    "\n",
    "\n",
    "def prepare_graph_for_gae(graph):\n",
    "    graph = dgl.to_bidirected(graph, copy_ndata=True)\n",
    "    return dgl.add_self_loop(graph)\n",
    "\n",
    "\n",
    "gae_graph = prepare_graph_for_gae(train_all_graph).to(device)\n",
    "gae_features = gae_graph.ndata['feat'].float().to(device)\n",
    "num_nodes = gae_graph.num_nodes()\n",
    "in_feats = gae_features.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "833b57a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InnerProductDecoder(nn.Module):\n",
    "    def forward(self, z, src, dst):\n",
    "        return (z[src] * z[dst]).sum(dim=1)\n",
    "\n",
    "\n",
    "gae_encoder = GCN(in_feats, gae_hidden_dim, gae_latent_dim).to(device)\n",
    "gae_structure_decoder = InnerProductDecoder().to(device)\n",
    "gae_attribute_decoder = nn.Sequential(\n",
    "    nn.Linear(gae_latent_dim, gae_hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(gae_hidden_dim, in_feats)\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(gae_encoder.parameters()) + list(gae_attribute_decoder.parameters()),\n",
    "    lr=gae_lr,\n",
    "    weight_decay=5e-4,\n",
    ")\n",
    "\n",
    "src_all, dst_all = gae_graph.edges()\n",
    "src_all = src_all.to(device)\n",
    "dst_all = dst_all.to(device)\n",
    "\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "mse_loss = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f6a85fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_positive_edges(num_samples):\n",
    "    if num_samples >= src_all.shape[0]:\n",
    "        return src_all, dst_all\n",
    "    idx = torch.randint(0, src_all.shape[0], (num_samples,), device=device)\n",
    "    return src_all[idx], dst_all[idx]\n",
    "\n",
    "\n",
    "def sample_negative_edges(num_samples):\n",
    "    neg_src = torch.randint(0, num_nodes, (num_samples,), device=device)\n",
    "    neg_dst = torch.randint(0, num_nodes, (num_samples,), device=device)\n",
    "    return neg_src, neg_dst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b50a1bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Graph Autoencoder (GAE) pretraining on train_all_graph...\n",
      "GAE Epoch 0001 | Total: 4.1441 | Struct: 3.6520 | Attr: 0.9842\n",
      "GAE Epoch 0020 | Total: 0.9208 | Struct: 0.4913 | Attr: 0.8590\n",
      "GAE Epoch 0030 | Total: 0.8397 | Struct: 0.4418 | Attr: 0.7958\n",
      "Finished GAE pretraining. Saved embeddings for downstream tasks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Starting Graph Autoencoder (GAE) pretraining on train_all_graph...')\n",
    "for epoch in range(1, gae_epochs + 1):\n",
    "    gae_encoder.train()\n",
    "    gae_attribute_decoder.train()\n",
    "\n",
    "    latent_z = gae_encoder(gae_graph, gae_features)\n",
    "\n",
    "    num_pos_samples = min(src_all.shape[0], max_struct_samples)\n",
    "    pos_src, pos_dst = sample_positive_edges(num_pos_samples)\n",
    "    pos_logits = gae_structure_decoder(latent_z, pos_src, pos_dst)\n",
    "    pos_labels = torch.ones_like(pos_logits)\n",
    "\n",
    "    num_neg_samples = max(1, int(neg_sample_ratio * num_pos_samples))\n",
    "    neg_src, neg_dst = sample_negative_edges(num_neg_samples)\n",
    "    neg_logits = gae_structure_decoder(latent_z, neg_src, neg_dst)\n",
    "    neg_labels = torch.zeros_like(neg_logits)\n",
    "\n",
    "    struct_loss = bce_loss(\n",
    "        torch.cat([pos_logits, neg_logits], dim=0),\n",
    "        torch.cat([pos_labels, neg_labels], dim=0)\n",
    "    )\n",
    "\n",
    "    recon_features = gae_attribute_decoder(latent_z)\n",
    "    attr_loss = mse_loss(recon_features, gae_features)\n",
    "\n",
    "    loss = struct_loss + alpha_attr * attr_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch == 1 or epoch % 20 == 0 or epoch == gae_epochs:\n",
    "        print(\n",
    "            f'GAE Epoch {epoch:04d} | Total: {loss.item():.4f} | '\n",
    "            f'Struct: {struct_loss.item():.4f} | Attr: {attr_loss.item():.4f}'\n",
    "        )\n",
    "\n",
    "gae_encoder.eval()\n",
    "gae_structure_decoder.eval()\n",
    "gae_attribute_decoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    gae_train_embeddings = gae_encoder(gae_graph, gae_features).detach()\n",
    "    gae_train_attr_recon = gae_attribute_decoder(gae_train_embeddings).detach()\n",
    "\n",
    "print('Finished GAE pretraining. Saved embeddings for downstream tasks.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "176e2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score_graph_with_gae(graph, feature_key='feat'):\n",
    "    graph = prepare_graph_for_gae(graph).to(device)\n",
    "    features = graph.ndata[feature_key].float().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = gae_encoder(graph, features)\n",
    "        feat_recon = gae_attribute_decoder(z)\n",
    "\n",
    "        attr_err = ((feat_recon - features) ** 2).sum(dim=1)\n",
    "\n",
    "        src, dst = graph.edges()\n",
    "        src = src.to(device)\n",
    "        dst = dst.to(device)\n",
    "        logits = gae_structure_decoder(z, src, dst)\n",
    "        labels = torch.ones_like(logits)\n",
    "        edge_errors = F.binary_cross_entropy_with_logits(logits, labels, reduction='none')\n",
    "\n",
    "        node_struct_error = torch.zeros(graph.num_nodes(), device=device)\n",
    "        node_struct_error.scatter_add_(0, src, edge_errors)\n",
    "        node_struct_error.scatter_add_(0, dst, edge_errors)\n",
    "        degrees = (graph.out_degrees() + graph.in_degrees()).float().to(device)\n",
    "        degrees = torch.clamp(degrees, min=1.0)\n",
    "        node_struct_error = node_struct_error / degrees\n",
    "\n",
    "        combined_error = node_struct_error + alpha_attr * attr_err\n",
    "\n",
    "    return {\n",
    "        'struct_error': node_struct_error.detach().cpu(),\n",
    "        'attr_error': attr_err.detach().cpu(),\n",
    "        'combined_error': combined_error.detach().cpu(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c5a05",
   "metadata": {},
   "source": [
    "* Using GAE embeddings only for GCN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ae9b4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.7299, Train F1 0.1858, Test Loss 0.8185, Test F1 0.1225\n",
      "Epoch 050: Loss 0.3291, Train F1 0.5753, Test Loss 0.4479, Test F1 0.2770\n",
      "Epoch 100: Loss 0.2575, Train F1 0.7002, Test Loss 0.3498, Test F1 0.2982\n",
      "Epoch 150: Loss 0.2275, Train F1 0.7317, Test Loss 0.3332, Test F1 0.3055\n",
      "Epoch 200: Loss 0.2100, Train F1 0.7493, Test Loss 0.3273, Test F1 0.3276\n",
      "Epoch 250: Loss 0.1973, Train F1 0.7666, Test Loss 0.3216, Test F1 0.3567\n",
      "Epoch 300: Loss 0.1872, Train F1 0.7765, Test Loss 0.3176, Test F1 0.3575\n",
      "Epoch 350: Loss 0.1791, Train F1 0.7865, Test Loss 0.3153, Test F1 0.3614\n",
      "Epoch 400: Loss 0.1723, Train F1 0.7949, Test Loss 0.3142, Test F1 0.3632\n",
      "Epoch 450: Loss 0.1667, Train F1 0.7997, Test Loss 0.3132, Test F1 0.3770\n",
      "Epoch 500: Loss 0.1618, Train F1 0.8066, Test Loss 0.3138, Test F1 0.3818\n",
      "Epoch 550: Loss 0.1575, Train F1 0.8115, Test Loss 0.3147, Test F1 0.3838\n",
      "Epoch 600: Loss 0.1536, Train F1 0.8175, Test Loss 0.3151, Test F1 0.3835\n",
      "Epoch 650: Loss 0.1501, Train F1 0.8232, Test Loss 0.3148, Test F1 0.3892\n",
      "Epoch 700: Loss 0.1468, Train F1 0.8278, Test Loss 0.3150, Test F1 0.3949\n",
      "Epoch 750: Loss 0.1439, Train F1 0.8327, Test Loss 0.3156, Test F1 0.3984\n",
      "Epoch 800: Loss 0.1410, Train F1 0.8361, Test Loss 0.3156, Test F1 0.4021\n",
      "GAE Embeddings -> GCN Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.46      0.36      0.40      1083\n",
      "       licit       0.96      0.97      0.96     15587\n",
      "\n",
      "    accuracy                           0.93     16670\n",
      "   macro avg       0.71      0.67      0.68     16670\n",
      "weighted avg       0.92      0.93      0.93     16670\n",
      "\n",
      "GAE Embeddings -> GCN Best Classification Report on Labeled Test Graph at epoch 800:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.46      0.36      0.40      1083\n",
      "       licit       0.96      0.97      0.96     15587\n",
      "\n",
      "    accuracy                           0.93     16670\n",
      "   macro avg       0.71      0.67      0.68     16670\n",
      "weighted avg       0.92      0.93      0.93     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/GAE_gcn_embeddings/history.json\n",
      " - Best state: checkpoints/GAE_gcn_embeddings/best_model_state.pt\n",
      " - Latest state: checkpoints/GAE_gcn_embeddings/latest_model_state.pt\n",
      "[GAE Embeddings -> GCN] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.2408 | P=0.8280 | R=0.7143 | F1=0.7670\n",
      "  t=36 | nodes= 1708 | Loss=0.1673 | P=0.2963 | R=0.4848 | F1=0.3678\n",
      "  t=37 | nodes=  498 | Loss=0.2606 | P=0.5455 | R=0.4500 | F1=0.4932\n",
      "  t=38 | nodes=  756 | Loss=0.3197 | P=0.7963 | R=0.7748 | F1=0.7854\n",
      "  t=39 | nodes= 1183 | Loss=0.2681 | P=0.4000 | R=0.4198 | F1=0.4096\n",
      "  t=40 | nodes= 1211 | Loss=0.5203 | P=0.6761 | R=0.4286 | F1=0.5246\n",
      "  t=41 | nodes= 1132 | Loss=0.7150 | P=0.6466 | R=0.6466 | F1=0.6466\n",
      "  t=42 | nodes= 2154 | Loss=0.3982 | P=0.7304 | R=0.6234 | F1=0.6727\n",
      "  t=43 | nodes= 1370 | Loss=0.3246 | P=0.0267 | R=0.0833 | F1=0.0404\n",
      "  t=44 | nodes= 1591 | Loss=0.3206 | P=0.0543 | R=0.2083 | F1=0.0862\n",
      "  t=45 | nodes= 1221 | Loss=0.0776 | P=0.0667 | R=0.2000 | F1=0.1000\n",
      "  t=46 | nodes=  712 | Loss=0.1512 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=47 | nodes=  846 | Loss=0.3143 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.7783 | P=0.0625 | R=0.0278 | F1=0.0385\n",
      "  t=49 | nodes=  476 | Loss=1.1366 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.6715 | R=0.6083 | F1=0.6383\n",
      "  Aggregate t>=43 | P=0.0271 | R=0.0533 | F1=0.0359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'gae_train_embeddings' not in globals():\n",
    "    raise RuntimeError('Run the GAE pretraining cell before launching this training step.')\n",
    "\n",
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "gae_train_features = gae_train_embeddings.to(device)\n",
    "\n",
    "_gae_test_graph = prepare_graph_for_gae(test_all_graph).to(device)\n",
    "with torch.no_grad():\n",
    "    gae_test_embeddings = gae_encoder(_gae_test_graph, _gae_test_graph.ndata['feat'].float()).detach()\n",
    "\n",
    "gae_gcn = GCN(gae_train_features.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(gae_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_gae_gcn = train_model(\n",
    "    gae_gcn,\n",
    "    \"GAE Embeddings -> GCN\",\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    gae_train_features,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    gae_test_embeddings.to(device),\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=800,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path=\"checkpoints/GAE/gcn_embeddings/best_model_state.pt\"\n",
    ")\n",
    "\n",
    "save_history(history_gae_gcn, sub_dir=\"GAE_gcn_embeddings\")\n",
    "\n",
    "\n",
    "def build_gae_embedding_features(g_eval, feats):\n",
    "    g_eval = g_eval.to(device)\n",
    "    feats = feats.float().to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = gae_encoder(g_eval, feats)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "gae_gcn_timestep_metrics = report_timestep_performance(\n",
    "    \"GAE Embeddings -> GCN\",\n",
    "    gae_gcn,\n",
    "    feature_builder=build_gae_embedding_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99dc714",
   "metadata": {},
   "source": [
    "* GAE Embeddings + Original Features + GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0a42fe70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.7476, Train F1 0.1919, Test Loss 0.8290, Test F1 0.1300\n",
      "Epoch 050: Loss 0.2117, Train F1 0.7525, Test Loss 0.3137, Test F1 0.3888\n",
      "Epoch 100: Loss 0.1740, Train F1 0.7986, Test Loss 0.2850, Test F1 0.4366\n",
      "Epoch 150: Loss 0.1534, Train F1 0.8221, Test Loss 0.2832, Test F1 0.4429\n",
      "Epoch 200: Loss 0.1392, Train F1 0.8395, Test Loss 0.2844, Test F1 0.4659\n",
      "Epoch 250: Loss 0.1281, Train F1 0.8554, Test Loss 0.2843, Test F1 0.4914\n",
      "Epoch 300: Loss 0.1189, Train F1 0.8663, Test Loss 0.2854, Test F1 0.5105\n",
      "Epoch 350: Loss 0.1111, Train F1 0.8755, Test Loss 0.2872, Test F1 0.5319\n",
      "Epoch 400: Loss 0.1044, Train F1 0.8832, Test Loss 0.2892, Test F1 0.5484\n",
      "Epoch 450: Loss 0.0986, Train F1 0.8882, Test Loss 0.2920, Test F1 0.5519\n",
      "Epoch 500: Loss 0.0935, Train F1 0.8959, Test Loss 0.2950, Test F1 0.5558\n",
      "Epoch 550: Loss 0.0889, Train F1 0.9011, Test Loss 0.2981, Test F1 0.5600\n",
      "Epoch 600: Loss 0.0848, Train F1 0.9061, Test Loss 0.3019, Test F1 0.5607\n",
      "Epoch 650: Loss 0.0810, Train F1 0.9108, Test Loss 0.3053, Test F1 0.5582\n",
      "Epoch 700: Loss 0.0776, Train F1 0.9153, Test Loss 0.3099, Test F1 0.5558\n",
      "Epoch 750: Loss 0.0745, Train F1 0.9180, Test Loss 0.3155, Test F1 0.5526\n",
      "Epoch 800: Loss 0.0717, Train F1 0.9233, Test Loss 0.3201, Test F1 0.5500\n",
      "Raw + GAE Embeddings Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.60      0.51      0.55      1083\n",
      "       licit       0.97      0.98      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.78      0.74      0.76     16670\n",
      "weighted avg       0.94      0.95      0.94     16670\n",
      "\n",
      "Raw + GAE Embeddings Best Classification Report on Labeled Test Graph at epoch 600:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.64      0.50      0.56      1083\n",
      "       licit       0.97      0.98      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.80      0.74      0.77     16670\n",
      "weighted avg       0.94      0.95      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/GAE_gcn_concat/history.json\n",
      " - Best state: checkpoints/GAE_gcn_concat/best_model_state.pt\n",
      " - Latest state: checkpoints/GAE_gcn_concat/latest_model_state.pt\n",
      "[Raw + GAE Embeddings] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.1646 | P=0.8830 | R=0.8297 | F1=0.8555\n",
      "  t=36 | nodes= 1708 | Loss=0.1041 | P=0.3830 | R=0.5455 | F1=0.4500\n",
      "  t=37 | nodes=  498 | Loss=0.2196 | P=0.7600 | R=0.4750 | F1=0.5846\n",
      "  t=38 | nodes=  756 | Loss=0.2492 | P=0.8426 | R=0.8198 | F1=0.8311\n",
      "  t=39 | nodes= 1183 | Loss=0.2082 | P=0.5321 | R=0.7160 | F1=0.6105\n",
      "  t=40 | nodes= 1211 | Loss=0.4867 | P=0.7971 | R=0.4911 | F1=0.6077\n",
      "  t=41 | nodes= 1132 | Loss=0.7061 | P=0.7642 | R=0.6983 | F1=0.7297\n",
      "  t=42 | nodes= 2154 | Loss=0.3935 | P=0.8446 | R=0.6820 | F1=0.7546\n",
      "  t=43 | nodes= 1370 | Loss=0.3623 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.3282 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=45 | nodes= 1221 | Loss=0.0658 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0628 | P=0.0769 | R=0.5000 | F1=0.1333\n",
      "  t=47 | nodes=  846 | Loss=0.3584 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.8713 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.5695 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.7681 | R=0.6958 | F1=0.7302\n",
      "  Aggregate t>=43 | P=0.0036 | R=0.0059 | F1=0.0045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "train_concat_features = torch.cat([train_feature_all, gae_train_embeddings.to(device)], dim=1)\n",
    "\n",
    "_gae_test_graph = prepare_graph_for_gae(test_all_graph).to(device)\n",
    "with torch.no_grad():\n",
    "    gae_test_embeddings = gae_encoder(_gae_test_graph, _gae_test_graph.ndata['feat'].float()).detach()\n",
    "\n",
    "test_concat_features = torch.cat([test_features_all, gae_test_embeddings.to(device)], dim=1)\n",
    "\n",
    "concat_gcn = GCN(train_concat_features.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(concat_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_concat_gcn = train_model(\n",
    "    concat_gcn,\n",
    "    \"Raw + GAE Embeddings\",\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    train_concat_features,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    test_concat_features,\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=800,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path=\"checkpoints/GAE/gcn_concat/best_model_state.pt\"\n",
    ")\n",
    "\n",
    "save_history(history_concat_gcn, sub_dir=\"GAE_gcn_concat\")\n",
    "\n",
    "\n",
    "def build_gae_concat_features(g_eval, feats):\n",
    "    g_eval = g_eval.to(device)\n",
    "    feats = feats.float().to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = gae_encoder(g_eval, feats)\n",
    "    return torch.cat([feats, embeddings], dim=1)\n",
    "\n",
    "\n",
    "gae_concat_timestep_metrics = report_timestep_performance(\n",
    "    \"Raw + GAE Embeddings\",\n",
    "    concat_gcn,\n",
    "    feature_builder=build_gae_concat_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f9587",
   "metadata": {},
   "source": [
    "* GAE anomaly score + Original Features + GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "95308627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.9480, Train F1 0.2127, Test Loss 1.1737, Test F1 0.1366\n",
      "Epoch 050: Loss 0.2441, Train F1 0.7056, Test Loss 0.4097, Test F1 0.2990\n",
      "Epoch 100: Loss 0.1999, Train F1 0.7682, Test Loss 0.3158, Test F1 0.3898\n",
      "Epoch 150: Loss 0.1766, Train F1 0.7905, Test Loss 0.2848, Test F1 0.4499\n",
      "Epoch 200: Loss 0.1617, Train F1 0.8075, Test Loss 0.2766, Test F1 0.4697\n",
      "Epoch 250: Loss 0.1508, Train F1 0.8212, Test Loss 0.2757, Test F1 0.4810\n",
      "Epoch 300: Loss 0.1422, Train F1 0.8323, Test Loss 0.2781, Test F1 0.4842\n",
      "Epoch 350: Loss 0.1351, Train F1 0.8431, Test Loss 0.2805, Test F1 0.4839\n",
      "Epoch 400: Loss 0.1290, Train F1 0.8513, Test Loss 0.2821, Test F1 0.5037\n",
      "Epoch 450: Loss 0.1234, Train F1 0.8585, Test Loss 0.2826, Test F1 0.5325\n",
      "Epoch 500: Loss 0.1182, Train F1 0.8642, Test Loss 0.2847, Test F1 0.5530\n",
      "Epoch 550: Loss 0.1136, Train F1 0.8694, Test Loss 0.2864, Test F1 0.5692\n",
      "Epoch 600: Loss 0.1094, Train F1 0.8748, Test Loss 0.2894, Test F1 0.5767\n",
      "Epoch 650: Loss 0.1055, Train F1 0.8803, Test Loss 0.2931, Test F1 0.5843\n",
      "Epoch 700: Loss 0.1019, Train F1 0.8848, Test Loss 0.2965, Test F1 0.5871\n",
      "Epoch 750: Loss 0.0986, Train F1 0.8869, Test Loss 0.3008, Test F1 0.5886\n",
      "Epoch 800: Loss 0.0956, Train F1 0.8907, Test Loss 0.3053, Test F1 0.5875\n",
      "GAE Node Score + Raw Features Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.75      0.48      0.59      1083\n",
      "       licit       0.96      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.86      0.74      0.78     16670\n",
      "weighted avg       0.95      0.96      0.95     16670\n",
      "\n",
      "GAE Node Score + Raw Features Best Classification Report on Labeled Test Graph at epoch 750:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.75      0.48      0.59      1083\n",
      "       licit       0.97      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.86      0.74      0.78     16670\n",
      "weighted avg       0.95      0.96      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/GAE_gcn_scores/history.json\n",
      " - Best state: checkpoints/GAE_gcn_scores/best_model_state.pt\n",
      " - Latest state: checkpoints/GAE_gcn_scores/latest_model_state.pt\n",
      "[GAE Node Score + Raw Features] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.2262 | P=0.8916 | R=0.8132 | F1=0.8506\n",
      "  t=36 | nodes= 1708 | Loss=0.0970 | P=0.6207 | R=0.5455 | F1=0.5806\n",
      "  t=37 | nodes=  498 | Loss=0.2377 | P=0.8000 | R=0.5000 | F1=0.6154\n",
      "  t=38 | nodes=  756 | Loss=0.2456 | P=0.8349 | R=0.8198 | F1=0.8273\n",
      "  t=39 | nodes= 1183 | Loss=0.1703 | P=0.8108 | R=0.7407 | F1=0.7742\n",
      "  t=40 | nodes= 1211 | Loss=0.4743 | P=0.9375 | R=0.4018 | F1=0.5625\n",
      "  t=41 | nodes= 1132 | Loss=0.6295 | P=0.9101 | R=0.6983 | F1=0.7902\n",
      "  t=42 | nodes= 2154 | Loss=0.3754 | P=0.9050 | R=0.6778 | F1=0.7751\n",
      "  t=43 | nodes= 1370 | Loss=0.2937 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.2824 | P=0.0909 | R=0.0417 | F1=0.0571\n",
      "  t=45 | nodes= 1221 | Loss=0.0720 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0342 | P=0.3333 | R=0.5000 | F1=0.4000\n",
      "  t=47 | nodes=  846 | Loss=0.3579 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.8649 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.2438 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.8693 | R=0.6838 | F1=0.7655\n",
      "  Aggregate t>=43 | P=0.0215 | R=0.0118 | F1=0.0153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'gae_encoder' not in globals():\n",
    "    raise RuntimeError('Run the GAE pretraining cell before launching this training step.')\n",
    "\n",
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "train_scores = score_graph_with_gae(train_all_graph)\n",
    "test_scores = score_graph_with_gae(test_all_graph)\n",
    "\n",
    "gae_score_scaler = StandardScaler()\n",
    "train_anomaly_np = gae_score_scaler.fit_transform(train_scores['combined_error'].numpy().reshape(-1, 1))\n",
    "test_anomaly_np = gae_score_scaler.transform(test_scores['combined_error'].numpy().reshape(-1, 1))\n",
    "\n",
    "train_score_features = torch.tensor(train_anomaly_np, dtype=torch.float32)\n",
    "test_score_features = torch.tensor(test_anomaly_np, dtype=torch.float32)\n",
    "\n",
    "train_aug_features = torch.cat([train_feature_all.cpu(), train_score_features], dim=1).to(device)\n",
    "test_aug_features = torch.cat([test_features_all.cpu(), test_score_features], dim=1).to(device)\n",
    "\n",
    "gae_score_gcn = GCN(train_aug_features.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(gae_score_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_gae_score = train_model(\n",
    "    gae_score_gcn,\n",
    "    \"GAE Node Score + Raw Features\",\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    train_aug_features,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    test_aug_features,\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=800,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path='checkpoints/GAE/gcn_scores/best_model_state.pt'\n",
    ")\n",
    "\n",
    "save_history(history_gae_score, sub_dir='GAE_gcn_scores')\n",
    "\n",
    "\n",
    "def build_gae_score_features(g_eval, feats):\n",
    "    graph_cpu = g_eval.to('cpu')\n",
    "    score_outputs = score_graph_with_gae(graph_cpu)\n",
    "    anomaly_np = gae_score_scaler.transform(score_outputs['combined_error'].numpy().reshape(-1, 1))\n",
    "    anomaly_tensor = torch.tensor(anomaly_np, dtype=torch.float32, device=feats.device)\n",
    "    feats = feats.to(device)\n",
    "    return torch.cat([feats, anomaly_tensor], dim=1)\n",
    "\n",
    "\n",
    "gae_score_timestep_metrics = report_timestep_performance(\n",
    "    \"GAE Node Score + Raw Features\",\n",
    "    gae_score_gcn,\n",
    "    feature_builder=build_gae_score_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60596ac",
   "metadata": {},
   "source": [
    "### Local-GAE + GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478cf37",
   "metadata": {},
   "source": [
    "* Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "efc2dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'train_all_graph' not in globals():\n",
    "    raise RuntimeError('Run the data loading and graph construction cells first so `train_all_graph` exists.')\n",
    "\n",
    "local_gae_config = {\n",
    "    'rw_length': 24,\n",
    "    'rw_trials': 4,\n",
    "    'rw_restart_prob': 0.3,\n",
    "    'hidden_dim': 128,\n",
    "    'epochs': 20,\n",
    "    'samples_per_epoch': 4000,\n",
    "    'lr': 5e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'val_sample_size': 1024,\n",
    "    'val_interval': 5,\n",
    "}\n",
    "\n",
    "base_graph_for_local_gae = dgl.add_self_loop(dgl.to_bidirected(train_all_graph, copy_ndata=True))\n",
    "base_graph_for_local_gae.ndata['feat'] = base_graph_for_local_gae.ndata['feat'].float()\n",
    "local_train_feats = base_graph_for_local_gae.ndata['feat']\n",
    "local_num_nodes = base_graph_for_local_gae.num_nodes()\n",
    "\n",
    "samples_per_epoch = local_gae_config['samples_per_epoch']\n",
    "if samples_per_epoch is None:\n",
    "    samples_per_epoch = local_num_nodes\n",
    "samples_per_epoch = max(1, min(local_num_nodes, samples_per_epoch))\n",
    "\n",
    "val_sample_size = max(0, int(local_gae_config.get('val_sample_size', 0)))\n",
    "if val_sample_size > 0:\n",
    "    validation_nodes = torch.randperm(local_num_nodes)[:min(local_num_nodes, val_sample_size)]\n",
    "else:\n",
    "    validation_nodes = torch.tensor([], dtype=torch.long)\n",
    "validation_interval = max(1, int(local_gae_config.get('val_interval', 5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c99f05a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def anonymized_random_walk_subgraph(\n",
    "    graph,\n",
    "    feature_tensor,\n",
    "    target_nid,\n",
    "    *,\n",
    "    walk_length,\n",
    "    num_traces,\n",
    "    restart_prob,\n",
    "    fallback_khop=2,\n",
    "):\n",
    "    \"\"\"Sample a random-walk-with-restart ego graph and anonymize the target node.\"\"\"\n",
    "    seeds = torch.full((num_traces,), int(target_nid), dtype=torch.long)\n",
    "    try:\n",
    "        traces, _ = dgl.sampling.random_walk(\n",
    "            graph,\n",
    "            seeds,\n",
    "            length=walk_length,\n",
    "            restart_prob=restart_prob,\n",
    "        )\n",
    "        visited = traces.reshape(-1)\n",
    "        visited = visited[visited >= 0]\n",
    "        if visited.numel() == 0:\n",
    "            visited = torch.tensor([int(target_nid)], dtype=torch.long)\n",
    "        else:\n",
    "            visited = visited.unique()\n",
    "    except dgl.DGLError:\n",
    "        khop_result = dgl.khop_in_subgraph(graph, torch.tensor([int(target_nid)]), fallback_khop)\n",
    "        if isinstance(khop_result, tuple):\n",
    "            subg_tmp, induced = khop_result\n",
    "            if isinstance(induced, (list, tuple)):\n",
    "                visited = torch.as_tensor(induced[0], dtype=torch.long)\n",
    "            else:\n",
    "                visited = torch.as_tensor(induced, dtype=torch.long)\n",
    "            graph = subg_tmp\n",
    "        else:\n",
    "            graph = khop_result\n",
    "            visited = graph.ndata[dgl.NID].long()\n",
    "    target_tensor = torch.tensor([int(target_nid)], dtype=torch.long)\n",
    "    unique_nodes = torch.unique(torch.cat([visited, target_tensor]))\n",
    "\n",
    "    subg = dgl.node_subgraph(graph, unique_nodes)\n",
    "    parent_nids = subg.ndata[dgl.NID].long()\n",
    "    sub_feats = feature_tensor[parent_nids].clone()\n",
    "    target_mask = (parent_nids == int(target_nid))\n",
    "    target_indices = target_mask.nonzero(as_tuple=False).view(-1)\n",
    "    if target_indices.numel() == 0:\n",
    "        raise RuntimeError(f'Unable to locate target node {target_nid} inside the sampled subgraph.')\n",
    "    target_idx = target_indices[0].item()\n",
    "    target_feat = sub_feats[target_idx].clone()\n",
    "    sub_feats[target_idx] = 0.0\n",
    "    subg = dgl.add_self_loop(subg)\n",
    "    return subg, sub_feats, target_idx, target_feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "51f07130",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LocalSubgraphGAE(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = GraphConv(in_feats, hidden_dim)\n",
    "        self.decoder = GraphConv(hidden_dim, in_feats)\n",
    "\n",
    "    def forward(self, g, feat):\n",
    "        h = torch.relu(self.encoder(g, feat))\n",
    "        recon = self.decoder(g, h)\n",
    "        return recon, h\n",
    "\n",
    "\n",
    "local_subgraph_gae = LocalSubgraphGAE(local_train_feats.shape[1], local_gae_config['hidden_dim']).to(local_device)\n",
    "local_subgraph_optimizer = torch.optim.Adam(\n",
    "    local_subgraph_gae.parameters(),\n",
    "    lr=local_gae_config['lr'],\n",
    "    weight_decay=local_gae_config['weight_decay'],\n",
    ")\n",
    "target_recon_loss = nn.MSELoss()\n",
    "local_subgraph_history = []\n",
    "local_subgraph_val_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9a113c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score_nodes_with_local_subgraph_gae(\n",
    "    graph,\n",
    "    target_nodes=None,\n",
    "    *,\n",
    "    walk_length=None,\n",
    "    num_traces=None,\n",
    "    restart_prob=None,\n",
    "    model=None,\n",
    "):\n",
    "    if model is None:\n",
    "        model = local_subgraph_gae\n",
    "    if walk_length is None:\n",
    "        walk_length = local_gae_config['rw_length']\n",
    "    if num_traces is None:\n",
    "        num_traces = local_gae_config['rw_trials']\n",
    "    if restart_prob is None:\n",
    "        restart_prob = local_gae_config['rw_restart_prob']\n",
    "\n",
    "    graph_cpu = dgl.add_self_loop(dgl.to_bidirected(graph, copy_ndata=True))\n",
    "    feature_tensor = graph_cpu.ndata['feat'].float()\n",
    "\n",
    "    if target_nodes is None:\n",
    "        target_nodes = torch.arange(graph_cpu.num_nodes())\n",
    "    else:\n",
    "        target_nodes = torch.as_tensor(target_nodes, dtype=torch.long)\n",
    "\n",
    "    model.eval()\n",
    "    l2_scores = torch.zeros(target_nodes.shape[0])\n",
    "    mse_scores = torch.zeros_like(l2_scores)\n",
    "    with torch.no_grad():\n",
    "        for idx, node_id in enumerate(target_nodes.tolist()):\n",
    "            subg, anonymized_feat, target_idx, target_feat = anonymized_random_walk_subgraph(\n",
    "                graph_cpu,\n",
    "                feature_tensor,\n",
    "                int(node_id),\n",
    "                walk_length=walk_length,\n",
    "                num_traces=num_traces,\n",
    "                restart_prob=restart_prob,\n",
    "            )\n",
    "            recon, _ = model(subg.to(local_device), anonymized_feat.to(local_device))\n",
    "            diff = recon[target_idx] - target_feat.to(local_device)\n",
    "            l2_scores[idx] = torch.norm(diff, p=2).item()\n",
    "            mse_scores[idx] = torch.mean(diff.pow(2)).item()\n",
    "    return {\n",
    "        'node_ids': target_nodes,\n",
    "        'l2_recon_error': l2_scores,\n",
    "        'per_node_mse': mse_scores,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "468280f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training local-subgraph anonymized GAE (random-walk views)...\n",
      "[Local-GAE] Epoch 001 | Targets 4000 | MSE 0.7135\n",
      "[Local-GAE] Epoch 002 | Targets 4000 | MSE 0.7495\n",
      "[Local-GAE] Epoch 003 | Targets 4000 | MSE 0.8813\n",
      "[Local-GAE] Epoch 004 | Targets 4000 | MSE 0.8488\n",
      "[Local-GAE] Epoch 005 | Targets 4000 | MSE 0.7108\n",
      "    Validation (1024 nodes) MSE: 0.7562\n",
      "[Local-GAE] Epoch 006 | Targets 4000 | MSE 0.7224\n",
      "[Local-GAE] Epoch 007 | Targets 4000 | MSE 0.7969\n",
      "[Local-GAE] Epoch 008 | Targets 4000 | MSE 0.8466\n",
      "[Local-GAE] Epoch 009 | Targets 4000 | MSE 0.7123\n",
      "[Local-GAE] Epoch 010 | Targets 4000 | MSE 0.7516\n",
      "    Validation (1024 nodes) MSE: 0.7173\n",
      "[Local-GAE] Epoch 011 | Targets 4000 | MSE 0.7326\n",
      "[Local-GAE] Epoch 012 | Targets 4000 | MSE 0.6918\n",
      "[Local-GAE] Epoch 013 | Targets 4000 | MSE 0.6791\n",
      "[Local-GAE] Epoch 014 | Targets 4000 | MSE 0.7032\n",
      "[Local-GAE] Epoch 015 | Targets 4000 | MSE 0.7263\n",
      "    Validation (1024 nodes) MSE: 0.7339\n",
      "[Local-GAE] Epoch 016 | Targets 4000 | MSE 0.7191\n",
      "[Local-GAE] Epoch 017 | Targets 4000 | MSE 0.7024\n",
      "[Local-GAE] Epoch 018 | Targets 4000 | MSE 0.7715\n",
      "[Local-GAE] Epoch 019 | Targets 4000 | MSE 0.6566\n",
      "[Local-GAE] Epoch 020 | Targets 4000 | MSE 0.6692\n",
      "    Validation (1024 nodes) MSE: 0.7095\n",
      "Stored `local_subgraph_gae`, `local_subgraph_history`, and `score_nodes_with_local_subgraph_gae` for downstream evaluation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Training local-subgraph anonymized GAE (random-walk views)...')\n",
    "all_train_nodes = torch.arange(local_num_nodes)\n",
    "for epoch in range(1, local_gae_config['epochs'] + 1):\n",
    "    epoch_nodes = all_train_nodes[torch.randperm(local_num_nodes)[:samples_per_epoch]]\n",
    "    running_loss = 0.0\n",
    "    local_subgraph_gae.train()\n",
    "    for node_id in epoch_nodes.tolist():\n",
    "        subg, anonymized_feat, target_idx, target_feat = anonymized_random_walk_subgraph(\n",
    "            base_graph_for_local_gae,\n",
    "            local_train_feats,\n",
    "            int(node_id),\n",
    "            walk_length=local_gae_config['rw_length'],\n",
    "            num_traces=local_gae_config['rw_trials'],\n",
    "            restart_prob=local_gae_config['rw_restart_prob'],\n",
    "        )\n",
    "        subg = subg.to(local_device)\n",
    "        anonymized_feat = anonymized_feat.to(local_device)\n",
    "        target_feat = target_feat.to(local_device)\n",
    "\n",
    "        recon, _ = local_subgraph_gae(subg, anonymized_feat)\n",
    "        loss = target_recon_loss(recon[target_idx], target_feat)\n",
    "\n",
    "        local_subgraph_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        local_subgraph_optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(epoch_nodes)\n",
    "    local_subgraph_history.append(avg_loss)\n",
    "    print(f\"[Local-GAE] Epoch {epoch:03d} | Targets {len(epoch_nodes)} | MSE {avg_loss:.4f}\")\n",
    "\n",
    "    if validation_nodes.numel() > 0 and (epoch % validation_interval == 0):\n",
    "        val_scores = score_nodes_with_local_subgraph_gae(\n",
    "            base_graph_for_local_gae,\n",
    "            target_nodes=validation_nodes,\n",
    "            model=local_subgraph_gae,\n",
    "            walk_length=local_gae_config['rw_length'],\n",
    "            num_traces=local_gae_config['rw_trials'],\n",
    "            restart_prob=local_gae_config['rw_restart_prob'],\n",
    "        )\n",
    "        val_mse = val_scores['per_node_mse'].mean().item()\n",
    "        local_subgraph_val_history.append((epoch, val_mse))\n",
    "        print(f\"    Validation ({validation_nodes.numel()} nodes) MSE: {val_mse:.4f}\")\n",
    "\n",
    "print('Stored `local_subgraph_gae`, `local_subgraph_history`, and `score_nodes_with_local_subgraph_gae` for downstream evaluation.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735be9de",
   "metadata": {},
   "source": [
    "* Local GAE Embeddings + Raw Features + GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0cee1b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 1.4039, Train F1 0.2168, Test Loss 0.8035, Test F1 0.1493\n",
      "Epoch 050: Loss 0.2236, Train F1 0.7392, Test Loss 0.3134, Test F1 0.4152\n",
      "Epoch 100: Loss 0.1813, Train F1 0.7784, Test Loss 0.2851, Test F1 0.4602\n",
      "Epoch 150: Loss 0.1612, Train F1 0.8044, Test Loss 0.2814, Test F1 0.4921\n",
      "Epoch 200: Loss 0.1469, Train F1 0.8267, Test Loss 0.2828, Test F1 0.5096\n",
      "Epoch 250: Loss 0.1356, Train F1 0.8445, Test Loss 0.2875, Test F1 0.5208\n",
      "Epoch 300: Loss 0.1262, Train F1 0.8555, Test Loss 0.2963, Test F1 0.5339\n",
      "Epoch 350: Loss 0.1183, Train F1 0.8634, Test Loss 0.3020, Test F1 0.5362\n",
      "Epoch 400: Loss 0.1114, Train F1 0.8716, Test Loss 0.3062, Test F1 0.5359\n",
      "Epoch 450: Loss 0.1054, Train F1 0.8786, Test Loss 0.3098, Test F1 0.5417\n",
      "Epoch 500: Loss 0.1001, Train F1 0.8843, Test Loss 0.3130, Test F1 0.5463\n",
      "Epoch 550: Loss 0.0954, Train F1 0.8909, Test Loss 0.3158, Test F1 0.5475\n",
      "Epoch 600: Loss 0.0911, Train F1 0.8959, Test Loss 0.3181, Test F1 0.5530\n",
      "Local GAE Embeddings + Raw Features Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.78      0.43      0.55      1083\n",
      "       licit       0.96      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.87      0.71      0.76     16670\n",
      "weighted avg       0.95      0.96      0.95     16670\n",
      "\n",
      "Local GAE Embeddings + Raw Features Best Classification Report on Labeled Test Graph at epoch 600:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.78      0.43      0.55      1083\n",
      "       licit       0.96      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.87      0.71      0.76     16670\n",
      "weighted avg       0.95      0.96      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/LocalGAE_gcn_concat/history.json\n",
      " - Best state: checkpoints/LocalGAE_gcn_concat/best_model_state.pt\n",
      " - Latest state: checkpoints/LocalGAE_gcn_concat/latest_model_state.pt\n",
      "[Local GAE Embeddings + Raw Features] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.2340 | P=0.8758 | R=0.7747 | F1=0.8222\n",
      "  t=36 | nodes= 1708 | Loss=0.1031 | P=0.5769 | R=0.4545 | F1=0.5085\n",
      "  t=37 | nodes=  498 | Loss=0.2706 | P=0.8000 | R=0.4000 | F1=0.5333\n",
      "  t=38 | nodes=  756 | Loss=0.2666 | P=0.8854 | R=0.7658 | F1=0.8213\n",
      "  t=39 | nodes= 1183 | Loss=0.2510 | P=0.8000 | R=0.5926 | F1=0.6809\n",
      "  t=40 | nodes= 1211 | Loss=0.5715 | P=0.9706 | R=0.2946 | F1=0.4521\n",
      "  t=41 | nodes= 1132 | Loss=0.5760 | P=0.9595 | R=0.6121 | F1=0.7474\n",
      "  t=42 | nodes= 2154 | Loss=0.4442 | P=0.9200 | R=0.5774 | F1=0.7095\n",
      "  t=43 | nodes= 1370 | Loss=0.2970 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.2662 | P=0.1429 | R=0.0417 | F1=0.0645\n",
      "  t=45 | nodes= 1221 | Loss=0.0781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0233 | P=0.3333 | R=0.5000 | F1=0.4000\n",
      "  t=47 | nodes=  846 | Loss=0.3897 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.9226 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.3283 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.8808 | R=0.5985 | F1=0.7127\n",
      "  Aggregate t>=43 | P=0.0323 | R=0.0118 | F1=0.0173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'local_subgraph_gae' not in globals():\n",
    "    raise RuntimeError('Train the local-subgraph GAE cells before running this step.')\n",
    "\n",
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "local_subgraph_gae = local_subgraph_gae.to(local_device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_graph = base_graph_for_local_gae.to(local_device)\n",
    "    base_feats = base_graph.ndata['feat'].to(local_device)\n",
    "    _, local_train_hidden = local_subgraph_gae(base_graph, base_feats)\n",
    "\n",
    "    test_graph_local = dgl.add_self_loop(dgl.to_bidirected(test_all_graph, copy_ndata=True))\n",
    "    test_graph_local = test_graph_local.to(local_device)\n",
    "    test_graph_local.ndata['feat'] = test_graph_local.ndata['feat'].float()\n",
    "    _, local_test_hidden = local_subgraph_gae(test_graph_local, test_graph_local.ndata['feat'])\n",
    "\n",
    "local_train_hidden = local_train_hidden.to(device)\n",
    "local_test_hidden = local_test_hidden.to(device)\n",
    "\n",
    "local_aug_train = torch.cat([train_feature_all, local_train_hidden], dim=1)\n",
    "local_aug_test = torch.cat([test_features_all, local_test_hidden], dim=1)\n",
    "\n",
    "local_concat_gcn = GCN(local_aug_train.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(local_concat_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_local_concat = train_model(\n",
    "    local_concat_gcn,\n",
    "    'Local GAE Embeddings + Raw Features',\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    local_aug_train,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    local_aug_test,\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=600,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path='checkpoints/LocalGAE/gcn_concat/best_model_state.pt'\n",
    ")\n",
    "\n",
    "save_history(history_local_concat, sub_dir='LocalGAE_gcn_concat')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_local_concat_features(g_eval, feats):\n",
    "    g_local = g_eval.to(local_device)\n",
    "    feats_local = g_local.ndata['feat'].float()\n",
    "    with torch.no_grad():\n",
    "        _, hidden = local_subgraph_gae(g_local, feats_local)\n",
    "    hidden = hidden.to(device)\n",
    "    feats = feats.to(device)\n",
    "    return torch.cat([feats, hidden], dim=1)\n",
    "\n",
    "\n",
    "local_concat_timestep_metrics = report_timestep_performance(\n",
    "    \"Local GAE Embeddings + Raw Features\",\n",
    "    local_concat_gcn,\n",
    "    feature_builder=build_local_concat_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534fc35",
   "metadata": {},
   "source": [
    "* Local GAE Node Score + Raw Features + GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bc0f950d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 1.3220, Train F1 0.1305, Test Loss 1.1495, Test F1 0.1216\n",
      "Epoch 050: Loss 0.2513, Train F1 0.6965, Test Loss 0.4056, Test F1 0.2925\n",
      "Epoch 100: Loss 0.2072, Train F1 0.7596, Test Loss 0.3423, Test F1 0.3681\n",
      "Epoch 150: Loss 0.1830, Train F1 0.7834, Test Loss 0.3062, Test F1 0.4206\n",
      "Epoch 200: Loss 0.1672, Train F1 0.8008, Test Loss 0.2920, Test F1 0.4433\n",
      "Epoch 250: Loss 0.1561, Train F1 0.8149, Test Loss 0.2869, Test F1 0.4502\n",
      "Epoch 300: Loss 0.1476, Train F1 0.8246, Test Loss 0.2850, Test F1 0.4560\n",
      "Epoch 350: Loss 0.1405, Train F1 0.8329, Test Loss 0.2846, Test F1 0.4623\n",
      "Epoch 400: Loss 0.1343, Train F1 0.8405, Test Loss 0.2844, Test F1 0.4686\n",
      "Epoch 450: Loss 0.1285, Train F1 0.8480, Test Loss 0.2850, Test F1 0.4774\n",
      "Epoch 500: Loss 0.1233, Train F1 0.8556, Test Loss 0.2867, Test F1 0.4920\n",
      "Epoch 550: Loss 0.1186, Train F1 0.8624, Test Loss 0.2892, Test F1 0.4995\n",
      "Epoch 600: Loss 0.1142, Train F1 0.8683, Test Loss 0.2916, Test F1 0.5174\n",
      "Local GAE Node Score + Raw Features Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.62      0.45      0.52      1083\n",
      "       licit       0.96      0.98      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.79      0.71      0.74     16670\n",
      "weighted avg       0.94      0.95      0.94     16670\n",
      "\n",
      "Local GAE Node Score + Raw Features Best Classification Report on Labeled Test Graph at epoch 600:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.62      0.45      0.52      1083\n",
      "       licit       0.96      0.98      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.79      0.71      0.74     16670\n",
      "weighted avg       0.94      0.95      0.94     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/LocalGAE_gcn_scores/history.json\n",
      " - Best state: checkpoints/LocalGAE_gcn_scores/best_model_state.pt\n",
      " - Latest state: checkpoints/LocalGAE_gcn_scores/latest_model_state.pt\n",
      "[Local GAE Node Score + Raw Features] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.1922 | P=0.8305 | R=0.8077 | F1=0.8189\n",
      "  t=36 | nodes= 1708 | Loss=0.1208 | P=0.3158 | R=0.5455 | F1=0.4000\n",
      "  t=37 | nodes=  498 | Loss=0.2715 | P=0.7407 | R=0.5000 | F1=0.5970\n",
      "  t=38 | nodes=  756 | Loss=0.3282 | P=0.7190 | R=0.7838 | F1=0.7500\n",
      "  t=39 | nodes= 1183 | Loss=0.2030 | P=0.7403 | R=0.7037 | F1=0.7215\n",
      "  t=40 | nodes= 1211 | Loss=0.4319 | P=0.8723 | R=0.3661 | F1=0.5157\n",
      "  t=41 | nodes= 1132 | Loss=0.5743 | P=0.6299 | R=0.6897 | F1=0.6584\n",
      "  t=42 | nodes= 2154 | Loss=0.3517 | P=0.8368 | R=0.6653 | F1=0.7413\n",
      "  t=43 | nodes= 1370 | Loss=0.2438 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.2573 | P=0.0667 | R=0.0417 | F1=0.0513\n",
      "  t=45 | nodes= 1221 | Loss=0.0940 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0394 | P=0.1667 | R=0.5000 | F1=0.2500\n",
      "  t=47 | nodes=  846 | Loss=0.3037 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.7188 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.1872 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.7400 | R=0.6663 | F1=0.7012\n",
      "  Aggregate t>=43 | P=0.0263 | R=0.0118 | F1=0.0163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "\n",
    "if 'local_subgraph_gae' not in globals() or 'score_nodes_with_local_subgraph_gae' not in globals():\n",
    "    raise RuntimeError('Train the local-subgraph GAE cells before running this step.')\n",
    "\n",
    "train_score_dict = score_nodes_with_local_subgraph_gae(\n",
    "    train_all_graph,\n",
    "    model=local_subgraph_gae,\n",
    "    walk_length=local_gae_config['rw_length'],\n",
    "    num_traces=local_gae_config['rw_trials'],\n",
    "    restart_prob=local_gae_config['rw_restart_prob'],\n",
    ")\n",
    "\n",
    "test_score_dict = score_nodes_with_local_subgraph_gae(\n",
    "    test_all_graph,\n",
    "    model=local_subgraph_gae,\n",
    "    walk_length=local_gae_config['rw_length'],\n",
    "    num_traces=local_gae_config['rw_trials'],\n",
    "    restart_prob=local_gae_config['rw_restart_prob'],\n",
    ")\n",
    "\n",
    "train_scores = train_score_dict['per_node_mse'].view(-1, 1)\n",
    "test_scores = test_score_dict['per_node_mse'].view(-1, 1)\n",
    "\n",
    "local_score_mean = train_scores.mean()\n",
    "local_score_std = train_scores.std().clamp_min(1e-6)\n",
    "train_scores = (train_scores - local_score_mean) / local_score_std\n",
    "test_scores = (test_scores - local_score_mean) / local_score_std\n",
    "\n",
    "local_score_train = torch.cat([train_feature_all.cpu(), train_scores], dim=1).to(device)\n",
    "local_score_test = torch.cat([test_features_all.cpu(), test_scores], dim=1).to(device)\n",
    "\n",
    "local_score_gcn = GCN(local_score_train.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(local_score_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_local_score = train_model(\n",
    "    local_score_gcn,\n",
    "    \"Local GAE Node Score + Raw Features\",\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    local_score_train,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    local_score_test,\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=600,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path='checkpoints/LocalGAE/gcn_scores/best_model_state.pt'\n",
    ")\n",
    "\n",
    "save_history(history_local_score, sub_dir='LocalGAE_gcn_scores')\n",
    "\n",
    "\n",
    "def build_local_score_features(g_eval, feats):\n",
    "    graph_cpu = g_eval.to('cpu')\n",
    "    score_dict = score_nodes_with_local_subgraph_gae(\n",
    "        graph_cpu,\n",
    "        model=local_subgraph_gae,\n",
    "        walk_length=local_gae_config['rw_length'],\n",
    "        num_traces=local_gae_config['rw_trials'],\n",
    "        restart_prob=local_gae_config['rw_restart_prob'],\n",
    "    )\n",
    "    scores = score_dict['per_node_mse'].view(-1, 1)\n",
    "    scores = (scores - local_score_mean) / local_score_std\n",
    "    scores = scores.to(device)\n",
    "    feats = feats.to(device)\n",
    "    return torch.cat([feats, scores], dim=1)\n",
    "\n",
    "\n",
    "local_score_timestep_metrics = report_timestep_performance(\n",
    "    \"Local GAE Node Score + Raw Features\",\n",
    "    local_score_gcn,\n",
    "    feature_builder=build_local_score_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bed648",
   "metadata": {},
   "source": [
    "### CoLA + GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55948d9",
   "metadata": {},
   "source": [
    "* Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "86e02d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'base_graph_for_local_gae' not in globals() or 'anonymized_random_walk_subgraph' not in globals():\n",
    "    raise RuntimeError('Run the local-subgraph GAE cells first to set up the sampling utilities.')\n",
    "\n",
    "cola_config = {\n",
    "    'hidden_dim': 128,\n",
    "    'epochs': 30,\n",
    "    'samples_per_epoch': 4000,\n",
    "    'rw_length': local_gae_config['rw_length'],\n",
    "    'rw_trials': local_gae_config['rw_trials'],\n",
    "    'rw_restart_prob': local_gae_config['rw_restart_prob'],\n",
    "    'lr': 5e-4,\n",
    "    'weight_decay': 0.0,\n",
    "    'neg_trials': 1,\n",
    "    'eval_neg_trials': 2,\n",
    "}\n",
    "\n",
    "cola_graph = base_graph_for_local_gae\n",
    "cola_features = cola_graph.ndata['feat']\n",
    "cola_num_nodes = cola_graph.num_nodes()\n",
    "cola_samples_per_epoch = min(cola_num_nodes, cola_config['samples_per_epoch'])\n",
    "\n",
    "\n",
    "def _group_nodes_by_timestep(timestep_tensor):\n",
    "    sanitized = timestep_tensor.to(torch.long).view(-1).cpu()\n",
    "    groups = {}\n",
    "    for ts in torch.unique(sanitized).tolist():\n",
    "        mask = sanitized == ts\n",
    "        groups[int(ts)] = mask.nonzero(as_tuple=False).view(-1)\n",
    "    return sanitized, groups\n",
    "\n",
    "\n",
    "if 'timestep' not in cola_graph.ndata:\n",
    "    raise KeyError(\n",
    "        \"cola_graph.ndata is missing 'timestep'. Re-run the data loading cell that calls create_dgl_graph so timesteps are attached to each node.\"\n",
    "    )\n",
    "\n",
    "cola_node_timesteps, cola_nodes_by_timestep = _group_nodes_by_timestep(cola_graph.ndata['timestep'])\n",
    "cola_device = local_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "699194e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CoLAEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv = GraphConv(in_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, g, feat):\n",
    "        return torch.relu(self.conv(g, feat))\n",
    "\n",
    "\n",
    "class BilinearDiscriminator(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.scorer = nn.Bilinear(hidden_dim, hidden_dim, 1)\n",
    "\n",
    "    def forward(self, node_embed, graph_embed):\n",
    "        return self.scorer(node_embed, graph_embed).squeeze(-1)\n",
    "\n",
    "\n",
    "def _graph_readout(node_embeddings):\n",
    "    return node_embeddings.mean(dim=0)\n",
    "\n",
    "\n",
    "def _sample_negative_node(exclude_node):\n",
    "    candidate = None\n",
    "    group = cola_nodes_by_timestep.get(int(cola_node_timesteps[exclude_node].item()))\n",
    "    if group is not None and group.numel() > 0:\n",
    "        rand_idx = torch.randint(0, group.numel(), (1,)).item()\n",
    "        candidate = int(group[rand_idx])\n",
    "        if candidate == exclude_node and group.numel() > 1:\n",
    "            candidate = int(group[(rand_idx + 1) % group.numel()])\n",
    "    if candidate is None:\n",
    "        candidate = int(torch.randint(0, cola_num_nodes, (1,)))\n",
    "        if candidate == exclude_node:\n",
    "            candidate = int((candidate + 1) % cola_num_nodes)\n",
    "    return candidate\n",
    "\n",
    "\n",
    "cola_encoder = CoLAEncoder(cola_features.shape[1], cola_config['hidden_dim']).to(cola_device)\n",
    "cola_discriminator = BilinearDiscriminator(cola_config['hidden_dim']).to(cola_device)\n",
    "cola_optimizer = torch.optim.Adam(\n",
    "    list(cola_encoder.parameters()) + list(cola_discriminator.parameters()),\n",
    "    lr=cola_config['lr'],\n",
    "    weight_decay=cola_config['weight_decay'],\n",
    ")\n",
    "cola_bce = nn.BCEWithLogitsLoss()\n",
    "cola_train_history = []\n",
    "\n",
    "\n",
    "def cola_forward_step(node_id):\n",
    "    subg_pos, feat_pos, idx_pos, _ = anonymized_random_walk_subgraph(\n",
    "        cola_graph,\n",
    "        cola_features,\n",
    "        node_id,\n",
    "        walk_length=cola_config['rw_length'],\n",
    "        num_traces=cola_config['rw_trials'],\n",
    "        restart_prob=cola_config['rw_restart_prob'],\n",
    "    )\n",
    "    subg_pos = subg_pos.to(cola_device)\n",
    "    feat_pos = feat_pos.to(cola_device)\n",
    "    embeds_pos = cola_encoder(subg_pos, feat_pos)\n",
    "    target_embed = embeds_pos[idx_pos]\n",
    "    graph_embed_pos = _graph_readout(embeds_pos)\n",
    "\n",
    "    neg_scores = []\n",
    "    neg_labels = []\n",
    "    for _ in range(cola_config['neg_trials']):\n",
    "        neg_node = _sample_negative_node(node_id)\n",
    "        subg_neg, feat_neg, _, _ = anonymized_random_walk_subgraph(\n",
    "            cola_graph,\n",
    "            cola_features,\n",
    "            neg_node,\n",
    "            walk_length=cola_config['rw_length'],\n",
    "            num_traces=cola_config['rw_trials'],\n",
    "            restart_prob=cola_config['rw_restart_prob'],\n",
    "        )\n",
    "        subg_neg = subg_neg.to(cola_device)\n",
    "        feat_neg = feat_neg.to(cola_device)\n",
    "        embeds_neg = cola_encoder(subg_neg, feat_neg)\n",
    "        graph_embed_neg = _graph_readout(embeds_neg)\n",
    "        neg_scores.append(cola_discriminator(target_embed, graph_embed_neg).unsqueeze(0))\n",
    "        neg_labels.append(torch.zeros(1, device=cola_device))\n",
    "\n",
    "    pos_score = cola_discriminator(target_embed, graph_embed_pos)\n",
    "    pos_label = torch.ones(1, device=cola_device)\n",
    "    neg_scores = torch.cat(neg_scores) if neg_scores else torch.tensor([], device=cola_device)\n",
    "    neg_labels = torch.cat(neg_labels) if neg_labels else torch.tensor([], device=cola_device)\n",
    "    return pos_score, pos_label, neg_scores, neg_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7fc63d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CoLA contrastive model...\n",
      "[CoLA] Epoch 001 | Nodes 4000 | Loss 0.5503\n",
      "[CoLA] Epoch 002 | Nodes 4000 | Loss 0.5291\n",
      "[CoLA] Epoch 003 | Nodes 4000 | Loss 0.5839\n",
      "[CoLA] Epoch 004 | Nodes 4000 | Loss 0.5249\n",
      "[CoLA] Epoch 005 | Nodes 4000 | Loss 0.5374\n",
      "[CoLA] Epoch 006 | Nodes 4000 | Loss 0.5318\n",
      "[CoLA] Epoch 007 | Nodes 4000 | Loss 0.4996\n",
      "[CoLA] Epoch 008 | Nodes 4000 | Loss 0.5104\n",
      "[CoLA] Epoch 009 | Nodes 4000 | Loss 0.5195\n",
      "[CoLA] Epoch 010 | Nodes 4000 | Loss 0.4935\n",
      "[CoLA] Epoch 011 | Nodes 4000 | Loss 0.6681\n",
      "[CoLA] Epoch 012 | Nodes 4000 | Loss 0.4543\n",
      "[CoLA] Epoch 013 | Nodes 4000 | Loss 0.4933\n",
      "[CoLA] Epoch 014 | Nodes 4000 | Loss 0.5029\n",
      "[CoLA] Epoch 015 | Nodes 4000 | Loss 0.4658\n",
      "[CoLA] Epoch 016 | Nodes 4000 | Loss 0.4951\n",
      "[CoLA] Epoch 017 | Nodes 4000 | Loss 0.5159\n",
      "[CoLA] Epoch 018 | Nodes 4000 | Loss 0.4801\n",
      "[CoLA] Epoch 019 | Nodes 4000 | Loss 0.4937\n",
      "[CoLA] Epoch 020 | Nodes 4000 | Loss 0.4483\n",
      "[CoLA] Epoch 021 | Nodes 4000 | Loss 0.4557\n",
      "[CoLA] Epoch 022 | Nodes 4000 | Loss 0.5308\n",
      "[CoLA] Epoch 023 | Nodes 4000 | Loss 0.4808\n",
      "[CoLA] Epoch 024 | Nodes 4000 | Loss 0.5072\n",
      "[CoLA] Epoch 025 | Nodes 4000 | Loss 0.4485\n",
      "[CoLA] Epoch 026 | Nodes 4000 | Loss 0.8481\n",
      "[CoLA] Epoch 027 | Nodes 4000 | Loss 0.6443\n",
      "[CoLA] Epoch 028 | Nodes 4000 | Loss 0.5296\n",
      "[CoLA] Epoch 029 | Nodes 4000 | Loss 0.4440\n",
      "[CoLA] Epoch 030 | Nodes 4000 | Loss 0.4847\n",
      "Stored `cola_encoder`, `cola_discriminator`, and `cola_train_history` for downstream scoring.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Training CoLA contrastive model...')\n",
    "all_nodes = torch.arange(cola_num_nodes)\n",
    "for epoch in range(1, cola_config['epochs'] + 1):\n",
    "    epoch_nodes = all_nodes[torch.randperm(cola_num_nodes)[:cola_samples_per_epoch]]\n",
    "    running_loss = 0.0\n",
    "    for node_id in epoch_nodes.tolist():\n",
    "        cola_encoder.train()\n",
    "        cola_discriminator.train()\n",
    "        pos_score, pos_label, neg_scores, neg_labels = cola_forward_step(int(node_id))\n",
    "\n",
    "        loss = cola_bce(pos_score.unsqueeze(0), pos_label)\n",
    "        if neg_scores.numel() > 0:\n",
    "            loss = loss + cola_bce(neg_scores, neg_labels)\n",
    "\n",
    "        cola_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        cola_optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(epoch_nodes)\n",
    "    cola_train_history.append(avg_loss)\n",
    "    print(f\"[CoLA] Epoch {epoch:03d} | Nodes {len(epoch_nodes)} | Loss {avg_loss:.4f}\")\n",
    "\n",
    "print('Stored `cola_encoder`, `cola_discriminator`, and `cola_train_history` for downstream scoring.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b799d6",
   "metadata": {},
   "source": [
    "* CoLA Embeddings + Raw Features + GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e9110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 1.1419, Train F1 0.2184, Test Loss 0.8135, Test F1 0.1335\n",
      "Epoch 050: Loss 0.2274, Train F1 0.7221, Test Loss 0.3582, Test F1 0.3316\n",
      "Epoch 100: Loss 0.1875, Train F1 0.7741, Test Loss 0.3155, Test F1 0.3901\n",
      "Epoch 150: Loss 0.1639, Train F1 0.8060, Test Loss 0.3022, Test F1 0.4100\n",
      "Epoch 200: Loss 0.1466, Train F1 0.8292, Test Loss 0.2979, Test F1 0.4183\n",
      "Epoch 250: Loss 0.1321, Train F1 0.8484, Test Loss 0.2958, Test F1 0.4563\n",
      "Epoch 300: Loss 0.1195, Train F1 0.8635, Test Loss 0.2936, Test F1 0.4977\n",
      "CoLA Embeddings + Raw Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.67      0.40      0.50      1083\n",
      "       licit       0.96      0.99      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.82      0.69      0.74     16670\n",
      "weighted avg       0.94      0.95      0.94     16670\n",
      "\n",
      "CoLA Embeddings + Raw Best Classification Report on Labeled Test Graph at epoch 300:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.67      0.40      0.50      1083\n",
      "       licit       0.96      0.99      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.82      0.69      0.74     16670\n",
      "weighted avg       0.94      0.95      0.94     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/CoLA_gcn_concat/history.json\n",
      " - Best state: checkpoints/CoLA_gcn_concat/best_model_state.pt\n",
      " - Latest state: checkpoints/CoLA_gcn_concat/latest_model_state.pt\n",
      "[CoLA Embeddings + Raw Features] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.2110 | P=0.8659 | R=0.7802 | F1=0.8208\n",
      "  t=36 | nodes= 1708 | Loss=0.1419 | P=0.2857 | R=0.4242 | F1=0.3415\n",
      "  t=37 | nodes=  498 | Loss=0.2815 | P=0.6923 | R=0.4500 | F1=0.5455\n",
      "  t=38 | nodes=  756 | Loss=0.3848 | P=0.6863 | R=0.6306 | F1=0.6573\n",
      "  t=39 | nodes= 1183 | Loss=0.2752 | P=0.7111 | R=0.3951 | F1=0.5079\n",
      "  t=40 | nodes= 1211 | Loss=0.5620 | P=0.8235 | R=0.2500 | F1=0.3836\n",
      "  t=41 | nodes= 1132 | Loss=0.5618 | P=0.6667 | R=0.2759 | F1=0.3902\n",
      "  t=42 | nodes= 2154 | Loss=0.4114 | P=0.8444 | R=0.4770 | F1=0.6096\n",
      "  t=43 | nodes= 1370 | Loss=0.2498 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.2275 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=45 | nodes= 1221 | Loss=0.1012 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0432 | P=0.1250 | R=0.5000 | F1=0.2000\n",
      "  t=47 | nodes=  846 | Loss=0.2924 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.6332 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.5350 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.7463 | R=0.4923 | F1=0.5933\n",
      "  Aggregate t>=43 | P=0.0141 | R=0.0059 | F1=0.0083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'cola_encoder' not in globals():\n",
    "    raise RuntimeError('Train the CoLA encoder cell before running this step.')\n",
    "\n",
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "\n",
    "def _prep_graph(base_graph):\n",
    "    return dgl.add_self_loop(dgl.to_bidirected(base_graph, copy_ndata=True)).to(device)\n",
    "\n",
    "cola_train_graph = _prep_graph(train_all_graph)\n",
    "cola_test_graph = _prep_graph(test_all_graph)\n",
    "train_labels = cola_train_graph.ndata['label']\n",
    "train_mask = (train_labels >= 0)\n",
    "test_labels = cola_test_graph.ndata['label']\n",
    "test_mask = (test_labels >= 0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_train_feats = cola_train_graph.ndata['feat'].float()\n",
    "    base_test_feats = cola_test_graph.ndata['feat'].float()\n",
    "    cola_train_embeddings = cola_encoder(cola_train_graph, base_train_feats)\n",
    "    cola_test_embeddings = cola_encoder(cola_test_graph, base_test_feats)\n",
    "\n",
    "aug_train_feats = torch.cat([base_train_feats, cola_train_embeddings], dim=1)\n",
    "aug_test_feats = torch.cat([base_test_feats, cola_test_embeddings], dim=1)\n",
    "\n",
    "cola_gcn = GCN(aug_train_feats.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(cola_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_cola_gcn = train_model(\n",
    "    cola_gcn,\n",
    "    \"CoLA Embeddings + Raw\",\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    cola_train_graph,\n",
    "    aug_train_feats,\n",
    "    train_labels.to(device),\n",
    "    train_mask,\n",
    "    cola_test_graph,\n",
    "    aug_test_feats,\n",
    "    test_labels.to(device),\n",
    "    test_mask,\n",
    "    num_epochs=300,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=10,\n",
    "    checkpoint_path=\"checkpoints/CoLA/gcn_concat/best_model_state.pt\"\n",
    ")\n",
    "\n",
    "save_history(history_cola_gcn, sub_dir=\"CoLA_gcn_concat\")\n",
    "\n",
    "\n",
    "def build_cola_concat_features(g_eval, feats):\n",
    "    g_for_encoder = g_eval.to(cola_device)\n",
    "    feats_for_encoder = g_for_encoder.ndata['feat'].float()\n",
    "    with torch.no_grad():\n",
    "        embeddings = cola_encoder(g_for_encoder, feats_for_encoder)\n",
    "    embeddings = embeddings.to(device)\n",
    "    feats = feats.to(device)\n",
    "    return torch.cat([feats, embeddings], dim=1)\n",
    "\n",
    "\n",
    "cola_gcn_timestep_metrics = report_timestep_performance(\n",
    "    \"CoLA Embeddings + Raw Features\",\n",
    "    cola_gcn,\n",
    "    feature_builder=build_cola_concat_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d441ce",
   "metadata": {},
   "source": [
    "### CoLA Node Score + Raw Features + GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c19ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.6947, Train F1 0.2167, Test Loss 0.6692, Test F1 0.0666\n",
      "Epoch 050: Loss 0.2198, Train F1 0.7306, Test Loss 0.3207, Test F1 0.4066\n",
      "Epoch 100: Loss 0.1794, Train F1 0.7856, Test Loss 0.2701, Test F1 0.5203\n",
      "Epoch 150: Loss 0.1579, Train F1 0.8090, Test Loss 0.2549, Test F1 0.5770\n",
      "Epoch 200: Loss 0.1436, Train F1 0.8297, Test Loss 0.2587, Test F1 0.5926\n",
      "Epoch 250: Loss 0.1326, Train F1 0.8425, Test Loss 0.2715, Test F1 0.5941\n",
      "Epoch 300: Loss 0.1236, Train F1 0.8557, Test Loss 0.2910, Test F1 0.5714\n",
      "CoLA Node Score + Raw Features Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.81      0.44      0.57      1083\n",
      "       licit       0.96      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.89      0.72      0.77     16670\n",
      "weighted avg       0.95      0.96      0.95     16670\n",
      "\n",
      "CoLA Node Score + Raw Features Best Classification Report on Labeled Test Graph at epoch 250:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.77      0.48      0.59      1083\n",
      "       licit       0.97      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.87      0.74      0.79     16670\n",
      "weighted avg       0.95      0.96      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/CoLA_gcn_scores/history.json\n",
      " - Best state: checkpoints/CoLA_gcn_scores/best_model_state.pt\n",
      " - Latest state: checkpoints/CoLA_gcn_scores/latest_model_state.pt\n",
      "[CoLA Node Score + Raw Features] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.1861 | P=0.9139 | R=0.7582 | F1=0.8288\n",
      "  t=36 | nodes= 1708 | Loss=0.2527 | P=0.5000 | R=0.2727 | F1=0.3529\n",
      "  t=37 | nodes=  498 | Loss=0.4184 | P=0.9375 | R=0.3750 | F1=0.5357\n",
      "  t=38 | nodes=  756 | Loss=0.4126 | P=0.7778 | R=0.5045 | F1=0.6120\n",
      "  t=39 | nodes= 1183 | Loss=0.3543 | P=0.8519 | R=0.2840 | F1=0.4259\n",
      "  t=40 | nodes= 1211 | Loss=1.4727 | P=0.8333 | R=0.0893 | F1=0.1613\n",
      "  t=41 | nodes= 1132 | Loss=0.5232 | P=0.9863 | R=0.6207 | F1=0.7619\n",
      "  t=42 | nodes= 2154 | Loss=0.4182 | P=0.9187 | R=0.6151 | F1=0.7368\n",
      "  t=43 | nodes= 1370 | Loss=0.2460 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.2576 | P=0.1250 | R=0.0417 | F1=0.0625\n",
      "  t=45 | nodes= 1221 | Loss=0.0839 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0335 | P=1.0000 | R=0.5000 | F1=0.6667\n",
      "  t=47 | nodes=  846 | Loss=0.3715 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.7377 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.2466 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.8885 | R=0.5142 | F1=0.6514\n",
      "  Aggregate t>=43 | P=0.0909 | R=0.0118 | F1=0.0209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "\n",
    "if 'cola_encoder' not in globals() or 'cola_discriminator' not in globals():\n",
    "    raise RuntimeError('Train the CoLA encoder cell before running this step.')\n",
    "\n",
    "def compute_cola_scores(graph):\n",
    "    graph_proc = dgl.add_self_loop(dgl.to_bidirected(graph, copy_ndata=True)).to(cola_device)\n",
    "    feats = graph_proc.ndata['feat'].float()\n",
    "    with torch.no_grad():\n",
    "        node_embeddings = cola_encoder(graph_proc, feats)\n",
    "        graph_embedding = _graph_readout(node_embeddings)\n",
    "        repeated = graph_embedding.unsqueeze(0).repeat(node_embeddings.shape[0], 1)\n",
    "        logits = cola_discriminator(node_embeddings, repeated).unsqueeze(1)\n",
    "    return logits.cpu()\n",
    "\n",
    "train_scores = compute_cola_scores(train_all_graph)\n",
    "test_scores = compute_cola_scores(test_all_graph)\n",
    "\n",
    "cola_score_mean = train_scores.mean()\n",
    "cola_score_std = train_scores.std().clamp_min(1e-6)\n",
    "train_scores = (train_scores - cola_score_mean) / cola_score_std\n",
    "test_scores = (test_scores - cola_score_mean) / cola_score_std\n",
    "\n",
    "cola_score_train = torch.cat([train_feature_all.cpu(), train_scores], dim=1).to(device)\n",
    "cola_score_test = torch.cat([test_features_all.cpu(), test_scores], dim=1).to(device)\n",
    "\n",
    "cola_score_graph = dgl.add_self_loop(dgl.to_bidirected(train_all_graph, copy_ndata=True)).to(device)\n",
    "cola_score_test_graph = dgl.add_self_loop(dgl.to_bidirected(test_all_graph, copy_ndata=True)).to(device)\n",
    "train_labels = cola_score_graph.ndata['label']\n",
    "train_mask = (train_labels >= 0)\n",
    "test_labels = cola_score_test_graph.ndata['label']\n",
    "test_mask = (test_labels >= 0)\n",
    "\n",
    "cola_score_gcn = GCN(cola_score_train.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(cola_score_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_cola_score = train_model(\n",
    "    cola_score_gcn,\n",
    "    \"CoLA Node Score + Raw Features\",\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    cola_score_graph,\n",
    "    cola_score_train,\n",
    "    train_labels.to(device),\n",
    "    train_mask,\n",
    "    cola_score_test_graph,\n",
    "    cola_score_test,\n",
    "    test_labels.to(device),\n",
    "    test_mask,\n",
    "    num_epochs=300,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=10,\n",
    "    checkpoint_path='checkpoints/CoLA/gcn_scores/best_model_state.pt'\n",
    ")\n",
    "\n",
    "save_history(history_cola_score, sub_dir='CoLA_gcn_scores')\n",
    "\n",
    "\n",
    "def build_cola_score_features(g_eval, feats):\n",
    "    scores = compute_cola_scores(g_eval)\n",
    "    scores = (scores - cola_score_mean) / cola_score_std\n",
    "    scores = scores.to(device)\n",
    "    feats = feats.to(device)\n",
    "    return torch.cat([feats, scores], dim=1)\n",
    "\n",
    "\n",
    "cola_score_timestep_metrics = report_timestep_performance(\n",
    "    \"CoLA Node Score + Raw Features\",\n",
    "    cola_score_gcn,\n",
    "    feature_builder=build_cola_score_features\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89a6669",
   "metadata": {},
   "source": [
    "## GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fda9a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d118225",
   "metadata": {},
   "source": [
    "## GraphSage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473f7e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c021bf70",
   "metadata": {},
   "source": [
    "## Graph Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ac9e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "048b3cee",
   "metadata": {},
   "source": [
    "# Evaluation and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef0310",
   "metadata": {},
   "source": [
    "### Graphs of results (timestep-wise, algorithm-wise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e4cd04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfb9bb08",
   "metadata": {},
   "source": [
    "### what the results show\n",
    "* dataset character: data drift due to dark market shutdown, highly unbalanced, label not reliable(produced by heuristic) etc\n",
    "* which model performed better, which worst, why\n",
    "* why traditional machine learning outperform deep learning\n",
    "* tricks worked (adding MLP layer, dropout, weight decay, directed->undirected, self-loop)\n",
    "* why supervised learning did help at all \\\n",
    "etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1b73f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33c4da58",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e90b5a7",
   "metadata": {},
   "source": [
    "* (About graph machine learning) What we find/learned in this project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fff1664",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
