{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f9fdf5",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d0bb27",
   "metadata": {},
   "source": [
    "### Load data and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f191da99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data all: (136265, 169), Train data labeled: (29894, 169)\n",
      "Test data all: (67504, 169), Test data labeled: (16670, 169)\n",
      "Train edges all: (156843, 3), Train edges labeled: (22898, 3)\n",
      "Test edges all: (77512, 3), Test edges labeled: (13726, 3)\n"
     ]
    }
   ],
   "source": [
    "# data split, copied from Sev's experiment ipynb\n",
    "import pandas as pd\n",
    "\n",
    "txs_classes = pd.read_csv('../data/elliptic_txs_classes.csv')\n",
    "txs_edges = pd.read_csv('../data/elliptic_txs_edgelist.csv')\n",
    "txs_features = pd.read_csv('../data/elliptic_txs_features.csv', header=None)\n",
    "\n",
    "\n",
    "# join features with classes using tx id (1st column of txs_features)\n",
    "txs_data = txs_features.merge(txs_classes, left_on=0, right_on='txId', how='left')\n",
    "\n",
    "# convert class labels to integers\n",
    "# 1: licit (0), 2: illicit (1), unknown: -1\n",
    "label_mapping = {'1': 0, '2': 1, 'unknown': -1}\n",
    "txs_data['class'] = txs_data['class'].map(label_mapping).astype(int)\n",
    "\n",
    "# split data and edges into train and test according to timestep (2nd column of txs_features)\n",
    "train_data_all = txs_data[txs_data[1] <= 34]\n",
    "test_data_all = txs_data[txs_data[1] > 34]\n",
    "\n",
    "# separate datasets with labels(1 or 2) from those without labels(class=unknown)\n",
    "train_data_labeled = train_data_all[train_data_all['class'].isin([0, 1])]\n",
    "test_data_labeled = test_data_all[test_data_all['class'].isin([0, 1])]\n",
    "\n",
    "# process edges like data: add timestep info and split into train and test\n",
    "txs_edges = txs_edges.merge(txs_features[[0, 1]], left_on='txId1', right_on=0, how='left').rename(columns={1: 'timestep'}).drop(columns=[0])\n",
    "train_edges_all = txs_edges[txs_edges['timestep'] <= 34]\n",
    "test_edges_all = txs_edges[txs_edges['timestep'] > 34]\n",
    "train_edges_labeled = train_edges_all[train_edges_all['txId1'].isin(train_data_labeled['txId']) & train_edges_all['txId2'].isin(train_data_labeled['txId'])]\n",
    "test_edges_labeled = test_edges_all[test_edges_all['txId1'].isin(test_data_labeled['txId']) & test_edges_all['txId2'].isin(test_data_labeled['txId'])]\n",
    "\n",
    "# print sizes of datasets\n",
    "print(f\"Train data all: {train_data_all.shape}, Train data labeled: {train_data_labeled.shape}\")\n",
    "print(f\"Test data all: {test_data_all.shape}, Test data labeled: {test_data_labeled.shape}\")\n",
    "print(f\"Train edges all: {train_edges_all.shape}, Train edges labeled: {train_edges_labeled.shape}\")\n",
    "print(f\"Test edges all: {test_edges_all.shape}, Test edges labeled: {test_edges_labeled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d1786abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data labeled grouped by timestep: [(35, 1341), (36, 1708), (37, 498), (38, 756), (39, 1183), (40, 1211), (41, 1132), (42, 2154), (43, 1370), (44, 1591), (45, 1221), (46, 712), (47, 846), (48, 471), (49, 476)]\n",
      "Test edges labeled grouped by timestep: [(35, 1002), (36, 1148), (37, 423), (38, 653), (39, 1055), (40, 1180), (41, 1048), (42, 1443), (43, 935), (44, 1497), (45, 1346), (46, 388), (47, 822), (48, 371), (49, 415)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test data group dict, group by timestep for evaluation on each timestep\n",
    "test_data_labeled_timestep = {}\n",
    "test_edges_labeled_timestep = {}\n",
    "for t in range(35, 50):\n",
    "    test_data_labeled_timestep[t] = test_data_labeled[test_data_labeled[1] == t]\n",
    "    test_edges_labeled_timestep[t] = test_edges_labeled[test_edges_labeled['timestep'] == t]\n",
    "print(f\"Test data labeled grouped by timestep: {[ (t, df.shape[0]) for t, df in test_data_labeled_timestep.items() ]}\")\n",
    "print(f\"Test edges labeled grouped by timestep: {[ (t, df.shape[0]) for t, df in test_edges_labeled_timestep.items() ]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce4358",
   "metadata": {},
   "source": [
    "### Create DGL graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "26205acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labeled graph: Graph(num_nodes=29894, num_edges=22898,\n",
      "      ndata_schemes={'feat': Scheme(shape=(165,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'timestep': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Test labeled graph: Graph(num_nodes=16670, num_edges=13726,\n",
      "      ndata_schemes={'feat': Scheme(shape=(165,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'timestep': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Train all graph: Graph(num_nodes=136265, num_edges=156843,\n",
      "      ndata_schemes={'feat': Scheme(shape=(165,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'timestep': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Test all graph: Graph(num_nodes=67504, num_edges=77512,\n",
      "      ndata_schemes={'feat': Scheme(shape=(165,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'timestep': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Train labeled graph local features: Graph(num_nodes=29894, num_edges=22898,\n",
      "      ndata_schemes={'feat': Scheme(shape=(94,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'timestep': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Test labeled graph local features: Graph(num_nodes=16670, num_edges=13726,\n",
      "      ndata_schemes={'feat': Scheme(shape=(94,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'timestep': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Train all graph local features: Graph(num_nodes=136265, num_edges=156843,\n",
      "      ndata_schemes={'feat': Scheme(shape=(94,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'timestep': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Test all graph local features: Graph(num_nodes=67504, num_edges=77512,\n",
      "      ndata_schemes={'feat': Scheme(shape=(94,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'timestep': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "# create DGL graphs - copied from Sev's experiment ipynb\n",
    "import warnings\n",
    "import dgl\n",
    "import torch\n",
    "\n",
    "# create DGL graphs for train and test data\n",
    "def create_dgl_graph(data, edges, features=\"all\"):\n",
    "    # features: all or local\n",
    "    # all: all features except txId, timestep, class; \n",
    "    # local: only local features (first 94 features except timestep) (column 2 to 95)\n",
    "    node_ids = data['txId'].tolist()\n",
    "    id_to_idx = {node_id: idx for idx, node_id in enumerate(node_ids)}\n",
    "    \n",
    "    src = edges['txId1'].map(id_to_idx).tolist()\n",
    "    dst = edges['txId2'].map(id_to_idx).tolist()\n",
    "    \n",
    "    g = dgl.graph((src, dst), num_nodes=len(node_ids))\n",
    "    if features == \"local\": # local features\n",
    "        features = torch.tensor(data.iloc[:, 2: 96].values, dtype=torch.float32)\n",
    "    else: # all features include local + one hop features\n",
    "        features = torch.tensor(data.iloc[:, 2:-2].values, dtype=torch.float32)\n",
    "    labels = torch.tensor(data['class'].values, dtype=torch.long)\n",
    "    \n",
    "    g.ndata['feat'] = features\n",
    "    g.ndata['label'] = labels\n",
    "\n",
    "    timestep_col = next((col for col in ('timestep', 1, '1') if col in data.columns), None)\n",
    "    if timestep_col is not None:\n",
    "        timesteps = torch.tensor(data[timestep_col].to_numpy(dtype='int64'), dtype=torch.long)\n",
    "        g.ndata['timestep'] = timesteps\n",
    "    else:\n",
    "        warnings.warn(\n",
    "            'create_dgl_graph could not find a timestep column; downstream modules requiring time context will fail.',\n",
    "            RuntimeWarning,\n",
    "        )\n",
    "    \n",
    "    return g\n",
    "\n",
    "# graph with all features\n",
    "train_labeled_graph = create_dgl_graph(train_data_labeled, train_edges_labeled)\n",
    "test_labeled_graph = create_dgl_graph(test_data_labeled, test_edges_labeled)\n",
    "train_all_graph = create_dgl_graph(train_data_all, train_edges_all)\n",
    "test_all_graph = create_dgl_graph(test_data_all, test_edges_all)\n",
    "\n",
    "print(f\"Train labeled graph: {train_labeled_graph}\")\n",
    "print(f\"Test labeled graph: {test_labeled_graph}\")\n",
    "print(f\"Train all graph: {train_all_graph}\")\n",
    "print(f\"Test all graph: {test_all_graph}\")\n",
    "\n",
    "\n",
    "# graph with local features only\n",
    "train_labeled_graph_local = create_dgl_graph(train_data_labeled, train_edges_labeled, features=\"local\")\n",
    "test_labeled_graph_local = create_dgl_graph(test_data_labeled, test_edges_labeled, features=\"local\")\n",
    "train_all_graph_local = create_dgl_graph(train_data_all, train_edges_all, features=\"local\")\n",
    "test_all_graph_local = create_dgl_graph(test_data_all, test_edges_all, features=\"local\")\n",
    "\n",
    "print(f\"Train labeled graph local features: {train_labeled_graph_local}\")\n",
    "print(f\"Test labeled graph local features: {test_labeled_graph_local}\")\n",
    "print(f\"Train all graph local features: {train_all_graph_local}\")\n",
    "print(f\"Test all graph local features: {test_all_graph_local}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "99711f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "66ac3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Preparing graphs: labeled_only=True, local_feature=False =================\n",
      "Training Graph: Percentage isolated: 21.46%; Isolated nodes: 6415; Total nodes: 29894\n",
      "Test Graph: Percentage isolated: 25.64%; Total nodes: 16670; Isolated nodes: 4275\n",
      "\n",
      "============= Preparing graphs: labeled_only=False, local_feature=False =================\n",
      "Training Graph: Percentage isolated: 0.00%; Isolated nodes: 0; Total nodes: 136265\n",
      "Test Graph: Percentage isolated: 0.00%; Total nodes: 67504; Isolated nodes: 0\n",
      "\n",
      "============= Preparing graphs: labeled_only=True, local_feature=True =================\n",
      "Training Graph: Percentage isolated: 21.46%; Isolated nodes: 6415; Total nodes: 29894\n",
      "Test Graph: Percentage isolated: 25.64%; Total nodes: 16670; Isolated nodes: 4275\n",
      "\n",
      "============= Preparing graphs: labeled_only=False, local_feature=True =================\n",
      "Training Graph: Percentage isolated: 0.00%; Isolated nodes: 0; Total nodes: 136265\n",
      "Test Graph: Percentage isolated: 0.00%; Total nodes: 67504; Isolated nodes: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare_training\n",
    "def prepare_train_graphs(labeled_only=True, local_feature=False):\n",
    "    print(f\"============= Preparing graphs: labeled_only={labeled_only}, local_feature={local_feature} =================\")\n",
    "    if labeled_only:\n",
    "        if local_feature:\n",
    "            train_graph = dgl.to_bidirected(train_labeled_graph_local, copy_ndata=True)\n",
    "            test_graph = dgl.to_bidirected(test_labeled_graph_local, copy_ndata=True)\n",
    "        else:\n",
    "            train_graph = dgl.to_bidirected(train_labeled_graph, copy_ndata=True)\n",
    "            test_graph = dgl.to_bidirected(test_labeled_graph, copy_ndata=True)\n",
    "    else:\n",
    "        if local_feature:\n",
    "            train_graph = dgl.to_bidirected(train_all_graph_local, copy_ndata=True)\n",
    "            test_graph = dgl.to_bidirected(test_all_graph_local, copy_ndata=True)\n",
    "        else:\n",
    "            train_graph = dgl.to_bidirected(train_all_graph, copy_ndata=True)\n",
    "            test_graph = dgl.to_bidirected(test_all_graph, copy_ndata=True)\n",
    "\n",
    "    # --- Check Train Graph ---\n",
    "    train_degrees = train_graph.in_degrees()\n",
    "    train_isolated_nodes = (train_degrees == 0).sum().item()\n",
    "    train_total_nodes = train_graph.num_nodes()\n",
    "    train_percent_isolated = (train_isolated_nodes / train_total_nodes) * 100\n",
    "\n",
    "    print(f\"Training Graph: Percentage isolated: {train_percent_isolated:.2f}%; Isolated nodes: {train_isolated_nodes}; Total nodes: {train_total_nodes}\")\n",
    "\n",
    "    # --- Check Test Graph ---\n",
    "    test_degrees = test_graph.in_degrees()\n",
    "    test_isolated_nodes = (test_degrees == 0).sum().item()\n",
    "    test_total_nodes = test_graph.num_nodes()\n",
    "    test_percent_isolated = (test_isolated_nodes / test_total_nodes) * 100\n",
    "\n",
    "    print(f\"Test Graph: Percentage isolated: {test_percent_isolated:.2f}%; Total nodes: {test_total_nodes}; Isolated nodes: {test_isolated_nodes}\\n\")\n",
    "\n",
    "    train_graph = dgl.add_self_loop(train_graph)\n",
    "    test_graph = dgl.add_self_loop(test_graph)\n",
    "\n",
    "    train_features = train_graph.ndata['feat']\n",
    "    train_labels = train_graph.ndata['label']\n",
    "    train_mask = (train_labels >= 0)\n",
    "    test_features = test_graph.ndata['feat']\n",
    "    test_labels = test_graph.ndata['label']\n",
    "    test_mask = (test_labels >= 0)\n",
    "\n",
    "    train_graph = train_graph.to(device)\n",
    "    test_graph = test_graph.to(device)\n",
    "    train_features = train_features.to(device)\n",
    "    train_labels = train_labels.to(device)\n",
    "    test_features = test_features.to(device)\n",
    "    test_labels = test_labels.to(device)\n",
    "\n",
    "    return train_graph, train_features, train_labels, train_mask, test_graph, test_features, test_labels, test_mask\n",
    "\n",
    "train_graph_labeled, train_feature_labeled, train_labels_labeled, train_mask_labeled, \\\n",
    "    test_graph_labeled, test_features_labeled, test_labels_labeled, test_mask_labeled \\\n",
    "        = prepare_train_graphs(labeled_only=True, local_feature=False)\n",
    "\n",
    "train_graph_all, train_feature_all, train_labels_all, train_mask_all, \\\n",
    "    test_graph_all, test_features_all, test_labels_all, test_mask_all \\\n",
    "        = prepare_train_graphs(labeled_only=False, local_feature=False)\n",
    "\n",
    "train_graph_labeled_local, train_feature_labeled_local, train_labels_labeled_local, train_mask_labeled_local, \\\n",
    "    test_graph_labeled_local, test_features_labeled_local, test_labels_labeled_local, test_mask_labeled_local \\\n",
    "        = prepare_train_graphs(labeled_only=True, local_feature=True)\n",
    "\n",
    "train_graph_all_local, train_feature_all_local, train_labels_all_local, train_mask_all_local, \\\n",
    "    test_graph_all_local, test_features_all_local, test_labels_all_local, test_mask_all_local \\\n",
    "        = prepare_train_graphs(labeled_only=False, local_feature=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "364381e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time steped testing graphs\n",
    "test_labeled_graphs_timestep = {}\n",
    "for t in range(35, 50):\n",
    "    data_t = test_data_labeled_timestep[t]\n",
    "    edges_t = test_edges_labeled_timestep[t]\n",
    "    test_graph_t = create_dgl_graph(data_t, edges_t)\n",
    "    test_graph_t = dgl.to_bidirected(test_graph_t, copy_ndata=True)\n",
    "    test_graph_t = dgl.add_self_loop(test_graph_t)\n",
    "    test_graph_t = test_graph_t.to(device)\n",
    "    test_feature_t = test_graph_t.ndata['feat'].to(device)\n",
    "    test_label_t = test_graph_t.ndata['label'].to(device)\n",
    "    test_mask_t = (test_label_t >= 0).to(device)\n",
    "    test_labeled_graphs_timestep[t] = (test_graph_t, test_feature_t, test_label_t, test_mask_t)\n",
    "\n",
    "# test stepped testing graphs local features\n",
    "# test_labeled_graphs_timestep_local = {}\n",
    "# for t in range(35, 50):\n",
    "#     data_t = test_data_labeled_timestep[t]\n",
    "#     edges_t = test_edges_labeled_timestep[t]\n",
    "#     test_labeled_graphs_timestep_local[t] = create_dgl_graph(data_t, edges_t, features=\"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba07098a",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dc90afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, test_graph, test_features, test_labels, test_mask, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_logits = model(test_graph, test_features)\n",
    "        test_preds = test_logits.argmax(dim=1)\n",
    "        test_loss = criterion(test_logits, test_labels).item()\n",
    "        \n",
    "        # Get masked predictions and labels\n",
    "        masked_preds = test_preds[test_mask].cpu().numpy()\n",
    "        masked_labels = test_labels[test_mask].cpu().numpy()\n",
    "        \n",
    "        # Use sklearn's precision_recall_fscore_support with zero_division handling\n",
    "        # pos_label=0 for illicit class, average=None to get per-class metrics\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            masked_labels, masked_preds, \n",
    "            labels=[0, 1],  # illicit=0, licit=1\n",
    "            average=None,\n",
    "            zero_division=0.0\n",
    "        )\n",
    "        \n",
    "        # Extract illicit (class 0) metrics\n",
    "        test_precision = precision[0] if len(precision) > 0 else 0.0\n",
    "        test_recall = recall[0] if len(recall) > 0 else 0.0\n",
    "        test_f1 = f1[0] if len(f1) > 0 else 0.0\n",
    "        \n",
    "        test_report = classification_report(\n",
    "            masked_labels, masked_preds, \n",
    "            target_names=['illicit', 'licit'],\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        return test_loss, test_precision, test_recall, test_f1, test_report\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "\n",
    "    # --- Create X-Axes ---\n",
    "    \n",
    "    # 1. X-axis for 'train_loss' (1 point per epoch)\n",
    "    # We add a small offset (0.5) to starting_epoch for clearer plotting\n",
    "    # if we resume training, so the first point isn't hidden.\n",
    "    starting_epoch = history.get('test_epochs', [1])[0] - 1\n",
    "    total_train_epochs = len(history.get('train_loss', []))\n",
    "    train_loss_epochs = list(range(starting_epoch + 1, starting_epoch + 1 + total_train_epochs))\n",
    "\n",
    "    # 2. X-axis for all test/validation metrics (sparse)\n",
    "    # This list is saved directly in our history object\n",
    "    eval_epochs = history.get('test_epochs', [])\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # --- Plot 1: Loss (Train vs. Test) ---\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    # Plot Train Loss (dense)\n",
    "    if train_loss_epochs:\n",
    "        plt.plot(train_loss_epochs, history['train_loss'], label='Train Loss', alpha=0.7, zorder=1)\n",
    "    \n",
    "    # Plot Test Loss (sparse)\n",
    "    if eval_epochs:\n",
    "        plt.plot(eval_epochs, history['test_loss'], label='Test Loss', \n",
    "                 marker='o', linestyle='--', linewidth=2, markersize=5, zorder=2)\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{model_name} Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # --- Plot 2: F1 Score (Train vs. Test) ---\n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    # Plot Train F1 (sparse, from eval steps)\n",
    "    if eval_epochs and 'train_f1' in history:\n",
    "        plt.plot(eval_epochs, history['train_f1'], label='Train F1 Score', \n",
    "                 marker='s', linestyle=':', linewidth=2, markersize=5)\n",
    "    \n",
    "    # Plot Test F1 (sparse, from eval steps)\n",
    "    if eval_epochs and 'test_f1' in history:\n",
    "        plt.plot(eval_epochs, history['test_f1'], label='Test F1 Score', \n",
    "                 color='orange', marker='o', linestyle='--', linewidth=2, markersize=5)\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title(f'{model_name} F1 Score at Evaluation Steps')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_model(model, model_name, optimizer, criterion, train_graph, train_features, train_labels, train_mask,\n",
    "                test_graph, test_features, test_labels, test_mask,\n",
    "                num_epochs, test_every=100, previous_history=None, print_best_report=True, show_plots=False,\n",
    "                early_stopping_patience=None, checkpoint_path=None):\n",
    "    # if previous_history is provided, resume training from there\n",
    "    if previous_history is not None:\n",
    "        history = previous_history\n",
    "        starting_epoch = len(history[\"train_loss\"]) + 1\n",
    "    else:\n",
    "        history = {\"train_loss\": [], \"train_f1\": [], \"train_precision\": [], \"train_recall\": [],\n",
    "               \"test_loss\": [], \"test_f1\": [], \"test_precision\": [], \"test_recall\": [],\n",
    "               \"test_epochs\": [],\n",
    "               \"best_test_f1\": 0.0, \"best_report\": None, \"best_model_state\": None, \"best_epoch\": -1, \n",
    "               \"last_test_f1\": 0.0, \"last_report\": None, \"latest_model_state\": None}\n",
    "        starting_epoch = 1\n",
    "\n",
    "    epochs_since_improvement = 0\n",
    "    checkpoint_target = checkpoint_path\n",
    "    if checkpoint_target:\n",
    "        checkpoint_dir = os.path.dirname(checkpoint_target)\n",
    "        if checkpoint_dir:\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    final_epoch_of_training = starting_epoch + num_epochs - 1\n",
    "    # training \n",
    "    for epoch in range(starting_epoch, final_epoch_of_training + 1):\n",
    "        model.train()\n",
    "        logits = model(train_graph, train_features)\n",
    "        loss = criterion(logits, train_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history[\"train_loss\"].append(loss.item())\n",
    "\n",
    "        if epoch == 1 or epoch % test_every == 0 or epoch == final_epoch_of_training:\n",
    "            history[\"test_epochs\"].append(epoch)\n",
    "            train_pred = logits.argmax(dim=1)\n",
    "            train_precision = ((train_pred[train_mask] == 0) & (train_labels[train_mask] == 0)).sum().item() / (train_pred[train_mask] == 0).sum().item()\n",
    "            train_recall = ((train_pred[train_mask] == 0) & (train_labels[train_mask] == 0)).sum().item() / (train_labels[train_mask] == 0).sum().item()\n",
    "            train_f1 = 2 * train_precision * train_recall / (train_precision + train_recall)\n",
    "            history[\"train_f1\"].append(train_f1)\n",
    "            history[\"train_precision\"].append(train_precision)\n",
    "            history[\"train_recall\"].append(train_recall)\n",
    "            \n",
    "            test_loss, test_precision, test_recall, test_f1, test_report = evaluate_model(model, test_graph, test_features, test_labels, test_mask, criterion)\n",
    "            history[\"test_loss\"].append(test_loss)\n",
    "            history[\"test_f1\"].append(test_f1)\n",
    "            history[\"test_precision\"].append(test_precision)\n",
    "            history[\"test_recall\"].append(test_recall)\n",
    "            print(f\"Epoch {epoch:03d}: Loss {loss.item():.4f}, Train F1 {train_f1:.4f}, Test Loss {test_loss:.4f}, Test F1 {test_f1:.4f}\")\n",
    "            improved = False\n",
    "        \n",
    "            if test_f1 > history[\"best_test_f1\"]:\n",
    "                history[\"best_test_f1\"] = test_f1\n",
    "                history[\"best_report\"] = test_report\n",
    "                history[\"best_epoch\"] = epoch\n",
    "                history[\"best_model_state\"] = model.state_dict()\n",
    "                improved = True\n",
    "                if checkpoint_target:\n",
    "                    torch.save(history[\"best_model_state\"], checkpoint_target)\n",
    "            if early_stopping_patience is not None:\n",
    "                if improved:\n",
    "                    epochs_since_improvement = 0\n",
    "                else:\n",
    "                    epochs_since_improvement += 1\n",
    "                    if epochs_since_improvement >= early_stopping_patience:\n",
    "                        print(f\"Early stopping at epoch {epoch} after {early_stopping_patience} eval steps without improvement\")\n",
    "                        break\n",
    "\n",
    "    # final evaluation on test set\n",
    "    # final_test_loss, final_test_precision, final_test_recall, final_test_f1, final_test_report = evaluate_model(model, test_graph, test_features, test_labels, test_mask, criterion)\n",
    "    history[\"latest_model_state\"] = model.state_dict()\n",
    "    history[\"last_test_f1\"] = history[\"test_f1\"][-1]\n",
    "    history[\"last_report\"] = test_report\n",
    "\n",
    "    print(f\"{model_name} Last Classification Report on Labeled Test Graph:\")\n",
    "    print(history[\"last_report\"])\n",
    "\n",
    "    if print_best_report:\n",
    "        print(f\"{model_name} Best Classification Report on Labeled Test Graph at epoch {history['best_epoch']}:\")\n",
    "        print(history[\"best_report\"])\n",
    "\n",
    "    if show_plots:\n",
    "        plot_training_history(history, model_name)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "227009df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate a trained model on per-timestep labeled test graphs and aggregate before/after t=43.\n",
    "# Uses existing variables: test_labeled_graphs_timestep, gt (model), device, classification_report, dgl, torch, criterion\n",
    "def evaluate_timestep(model_to_test, feature_builder=None):\n",
    "    model_to_test.eval()\n",
    "\n",
    "    if feature_builder is None:\n",
    "        def _identity_builder(_, feats):\n",
    "            return feats\n",
    "        feature_builder = _identity_builder\n",
    "\n",
    "    per_t_metrics = {}\n",
    "    all_preds_before = []\n",
    "    all_trues_before = []\n",
    "    all_preds_after = []\n",
    "    all_trues_after = []\n",
    "    f1_list = []\n",
    "\n",
    "    for t in sorted(test_labeled_graphs_timestep.keys()):\n",
    "        g_eval, feats, labels, mask = test_labeled_graphs_timestep[t]\n",
    "        if g_eval is None or g_eval.num_nodes() == 0:\n",
    "            print(f\"t={t}: empty graph, skipping\")\n",
    "            continue\n",
    "\n",
    "        feats_for_model = feature_builder(g_eval, feats)\n",
    "        loss, precision, recall, f1, report = evaluate_model(model_to_test, g_eval, feats_for_model, labels, mask, criterion)\n",
    "        f1_list.append(f1)\n",
    "        per_t_metrics[t] = {\n",
    "            \"n_nodes\": int(g_eval.num_nodes()),\n",
    "            \"loss\": loss,\n",
    "            \"precision_illicit\": precision,\n",
    "            \"recall_illicit\": recall,\n",
    "            \"f1_illicit\": f1,\n",
    "            \"classification_report\": report\n",
    "        }\n",
    "\n",
    "        # record masked count\n",
    "        per_t_metrics[t][\"n_masked\"] = int(mask.sum().item())\n",
    "\n",
    "        # get predictions for masked nodes\n",
    "        with torch.no_grad():\n",
    "            logits = model_to_test(g_eval, feats_for_model)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "        masked_preds = preds[mask]\n",
    "        masked_labels = labels[mask]\n",
    "\n",
    "        # accumulate for before/after 43\n",
    "        arr_preds = masked_preds.cpu().numpy()\n",
    "        arr_trues = masked_labels.cpu().numpy()\n",
    "        if t < 43:\n",
    "            all_preds_before.append(arr_preds)\n",
    "            all_trues_before.append(arr_trues)\n",
    "        else:\n",
    "            all_preds_after.append(arr_preds)\n",
    "            all_trues_after.append(arr_trues)\n",
    "\n",
    "    # --- After the loop: aggregate and print reports ---\n",
    "    def aggregate_and_report(preds_list, trues_list, label=\"\"):\n",
    "        if not preds_list:\n",
    "            print(f\"No data for {label}\")\n",
    "            return None\n",
    "        preds_all = np.concatenate(preds_list)\n",
    "        trues_all = np.concatenate(trues_list)\n",
    "        # use sklearn to compute illicit-focused metrics (class 0 == illicit)\n",
    "\n",
    "        precision = float(precision_score(trues_all, preds_all, pos_label=0, zero_division=0))\n",
    "        recall = float(recall_score(trues_all, preds_all, pos_label=0, zero_division=0))\n",
    "        f1 = float(f1_score(trues_all, preds_all, pos_label=0, zero_division=0))\n",
    "        report = classification_report(trues_all, preds_all, target_names=['illicit', 'licit'], zero_division=0)\n",
    "        return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"report\": report}\n",
    "\n",
    "    agg_before = aggregate_and_report(all_preds_before, all_trues_before, label=\"t < 43\")\n",
    "    agg_after  = aggregate_and_report(all_preds_after,  all_trues_after,  label=\"t >= 43\")\n",
    "\n",
    "    return {\n",
    "        \"f1_list\": f1_list,\n",
    "        \"agg_before_f1\": agg_before[\"f1\"] if agg_before else None,\n",
    "        \"agg_after_f1\": agg_after[\"f1\"] if agg_after else None,\n",
    "        \"per_t_metrics\": per_t_metrics,\n",
    "        \"agg_before\": agg_before,\n",
    "        \"agg_after\": agg_after\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c2ef3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def report_timestep_performance(model_name, model, feature_builder=None):\n",
    "    metrics = evaluate_timestep(model, feature_builder=feature_builder)\n",
    "    print(f\"[{model_name}] per-timestep illicit metrics:\")\n",
    "    for t in sorted(metrics['per_t_metrics'].keys()):\n",
    "        metric = metrics['per_t_metrics'][t]\n",
    "        print(f\"  t={t:02d} | nodes={metric['n_masked']:5d} | Loss={metric['loss']:.4f} | P={metric['precision_illicit']:.4f} | R={metric['recall_illicit']:.4f} | F1={metric['f1_illicit']:.4f}\")\n",
    "    agg_before = metrics.get('agg_before')\n",
    "    if agg_before:\n",
    "        print(f\"  Aggregate t<43 | P={agg_before['precision']:.4f} | R={agg_before['recall']:.4f} | F1={agg_before['f1']:.4f}\")\n",
    "    agg_after = metrics.get('agg_after')\n",
    "    if agg_after:\n",
    "        print(f\"  Aggregate t>=43 | P={agg_after['precision']:.4f} | R={agg_after['recall']:.4f} | F1={agg_after['f1']:.4f}\")\n",
    "    print()\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "eba65436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "\n",
    "\n",
    "def save_history(history, sub_dir, save_dir = \"checkpoints\"):\n",
    "    # Save training history (JSON-safe) and model states (.pt)\n",
    "    os.makedirs(os.path.join(save_dir, sub_dir), exist_ok=True)\n",
    "\n",
    "    # Extract and remove model-state entries from history before JSON serialization\n",
    "    best_state = history.get(\"best_model_state\", None)\n",
    "    latest_state = history.get(\"latest_model_state\", None)\n",
    "\n",
    "    history_copy = {k: v for k, v in history.items() if k not in (\"best_model_state\", \"latest_model_state\")}\n",
    "\n",
    "    history_path = os.path.join(save_dir, sub_dir, \"history.json\")\n",
    "    with open(history_path, \"w\") as f:\n",
    "        json.dump(history_copy, f, indent=2)\n",
    "\n",
    "    # Save model state dicts (if present)\n",
    "\n",
    "    if best_state is not None:\n",
    "        torch.save(best_state, os.path.join(save_dir, sub_dir, \"best_model_state.pt\"))\n",
    "    if latest_state is not None:\n",
    "        torch.save(latest_state, os.path.join(save_dir, sub_dir, \"latest_model_state.pt\"))\n",
    "\n",
    "    print(\"Saved:\")\n",
    "    print(f\" - History JSON: {history_path}\")\n",
    "    if best_state is not None:\n",
    "        print(f\" - Best state: {os.path.join(save_dir, sub_dir, 'best_model_state.pt')}\")\n",
    "    if latest_state is not None:\n",
    "        print(f\" - Latest state: {os.path.join(save_dir, sub_dir, 'latest_model_state.pt')}\")\n",
    "\n",
    "def load_history(sub_dir, save_dir=\"checkpoints\", model=None, map_location=None):\n",
    "    \"\"\"\n",
    "    Load saved training history and model state dicts from disk.\n",
    "\n",
    "    Args:\n",
    "        sub_dir (str): subdirectory under save_dir where files are stored.\n",
    "        save_dir (str): root checkpoints directory (default \"checkpoints\").\n",
    "        model (nn.Module, optional): if provided and a gt_state_dict exists, it will be loaded into this model.\n",
    "        map_location (str or torch.device, optional): passed to torch.load (default \"cpu\" if None).\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"history\": dict or None,\n",
    "            \"best_model_state\": state_dict or None,\n",
    "            \"latest_model_state\": state_dict or None,\n",
    "            \"gt_state_dict\": state_dict or None,\n",
    "            \"model_loaded\": bool\n",
    "        }\n",
    "    \"\"\"\n",
    "    base_path = os.path.join(save_dir, sub_dir)\n",
    "    if map_location is None:\n",
    "        map_location = \"cpu\"\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    if not os.path.isdir(base_path):\n",
    "        raise FileNotFoundError(f\"Directory not found: {base_path}\")\n",
    "\n",
    "    # history JSON (name used by save_history)\n",
    "    history_path = os.path.join(base_path, \"history.json\")\n",
    "    if os.path.exists(history_path):\n",
    "        with open(history_path, \"r\") as f:\n",
    "            result = json.load(f)\n",
    "\n",
    "    def _load_pt(fname):\n",
    "        p = os.path.join(base_path, fname)\n",
    "        if os.path.exists(p):\n",
    "            return torch.load(p, map_location=map_location)\n",
    "        return None\n",
    "\n",
    "    result[\"best_model_state\"] = _load_pt(\"best_model_state.pt\")\n",
    "    result[\"latest_model_state\"] = _load_pt(\"latest_model_state.pt\")\n",
    "\n",
    "    # brief prints for confirmation\n",
    "    print(f\"Loaded history from {base_path}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fb1fe",
   "metadata": {},
   "source": [
    "Random Forest Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1431c3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Report on Labeled Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.91      0.73      0.81      1083\n",
      "       licit       0.98      1.00      0.99     15587\n",
      "\n",
      "    accuracy                           0.98     16670\n",
      "   macro avg       0.95      0.86      0.90     16670\n",
      "weighted avg       0.98      0.98      0.98     16670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# licit node class=1, illicit node class=0\n",
    "# train and evaluate a random forest classifier on the train and test data with labels\n",
    "# n_estimators=50, max_features=50\n",
    "# evaluate on both licit and illicit nodes' precision and recall and f1-score, also include micro and macro averages\n",
    "clf = RandomForestClassifier(n_estimators=50, max_features=50, random_state=42)\n",
    "clf.fit(train_data_labeled.iloc[:, 1:-2], train_data_labeled['class'])\n",
    "test_preds = clf.predict(test_data_labeled.iloc[:, 1:-2]) \n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(test_data_labeled['class'], test_preds, target_names=['illicit', 'licit'])\n",
    "print(\"Random Forest Classifier Report on Labeled Test Data:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32ce98",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8eaac0",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "563c9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size, allow_zero_in_degree=True)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes, allow_zero_in_degree=True)\n",
    "\n",
    "    def forward(self, g, feat):\n",
    "        h = self.conv1(g, feat)\n",
    "        h = torch.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26323bc",
   "metadata": {},
   "source": [
    "### Training without unknown nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f19f20da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.4449, Train F1 0.0086, Test Loss 0.3614, Test F1 0.2572\n",
      "Epoch 050: Loss 0.1756, Train F1 0.7916, Test Loss 0.2952, Test F1 0.4281\n",
      "Epoch 100: Loss 0.1390, Train F1 0.8346, Test Loss 0.2781, Test F1 0.5230\n",
      "Epoch 150: Loss 0.1190, Train F1 0.8612, Test Loss 0.2864, Test F1 0.5432\n",
      "Epoch 200: Loss 0.1053, Train F1 0.8763, Test Loss 0.2971, Test F1 0.5420\n",
      "Epoch 250: Loss 0.0947, Train F1 0.8900, Test Loss 0.3105, Test F1 0.5469\n",
      "Epoch 300: Loss 0.0862, Train F1 0.8976, Test Loss 0.3254, Test F1 0.5529\n",
      "Epoch 350: Loss 0.0794, Train F1 0.9048, Test Loss 0.3406, Test F1 0.5605\n",
      "Epoch 400: Loss 0.0739, Train F1 0.9127, Test Loss 0.3553, Test F1 0.5593\n",
      "Epoch 450: Loss 0.0694, Train F1 0.9191, Test Loss 0.3679, Test F1 0.5606\n",
      "Epoch 500: Loss 0.0657, Train F1 0.9235, Test Loss 0.3789, Test F1 0.5544\n",
      "Epoch 550: Loss 0.0625, Train F1 0.9267, Test Loss 0.3875, Test F1 0.5489\n",
      "Epoch 600: Loss 0.0597, Train F1 0.9299, Test Loss 0.3940, Test F1 0.5508\n",
      "Epoch 650: Loss 0.0572, Train F1 0.9330, Test Loss 0.4027, Test F1 0.5511\n",
      "Epoch 700: Loss 0.0551, Train F1 0.9356, Test Loss 0.4088, Test F1 0.5628\n",
      "Epoch 750: Loss 0.0532, Train F1 0.9384, Test Loss 0.4164, Test F1 0.5573\n",
      "Epoch 800: Loss 0.0515, Train F1 0.9412, Test Loss 0.4238, Test F1 0.5520\n",
      "GCN - Labeled Only Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.65      0.48      0.55      1083\n",
      "       licit       0.96      0.98      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.81      0.73      0.76     16670\n",
      "weighted avg       0.94      0.95      0.95     16670\n",
      "\n",
      "GCN - Labeled Only Best Classification Report on Labeled Test Graph at epoch 700:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.66      0.49      0.56      1083\n",
      "       licit       0.97      0.98      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.81      0.74      0.77     16670\n",
      "weighted avg       0.95      0.95      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/GCN_base_labeled/history.json\n",
      " - Best state: checkpoints/GCN_base_labeled/best_model_state.pt\n",
      " - Latest state: checkpoints/GCN_base_labeled/latest_model_state.pt\n",
      "[GCN - Labeled Only] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.1736 | P=0.8596 | R=0.8407 | F1=0.8500\n",
      "  t=36 | nodes= 1708 | Loss=0.1246 | P=0.2826 | R=0.3939 | F1=0.3291\n",
      "  t=37 | nodes=  498 | Loss=0.2685 | P=0.8400 | R=0.5250 | F1=0.6462\n",
      "  t=38 | nodes=  756 | Loss=0.3997 | P=0.7692 | R=0.5405 | F1=0.6349\n",
      "  t=39 | nodes= 1183 | Loss=0.2499 | P=0.5161 | R=0.3951 | F1=0.4476\n",
      "  t=40 | nodes= 1211 | Loss=0.6916 | P=0.8889 | R=0.2857 | F1=0.4324\n",
      "  t=41 | nodes= 1132 | Loss=1.1762 | P=0.7045 | R=0.5345 | F1=0.6078\n",
      "  t=42 | nodes= 2154 | Loss=0.4378 | P=0.8909 | R=0.6151 | F1=0.7277\n",
      "  t=43 | nodes= 1370 | Loss=0.2466 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.3090 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=45 | nodes= 1221 | Loss=0.1129 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0565 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=47 | nodes=  846 | Loss=0.4247 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.9442 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.6841 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.7670 | R=0.5689 | F1=0.6533\n",
      "  Aggregate t>=43 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "gcn_labeled = GCN(train_feature_labeled.shape[1], embedding_dim, 2).to(device)\n",
    "gcn_labeled_optimizer = torch.optim.Adam(gcn_labeled.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_gcn_labeled = train_model(\n",
    "    gcn_labeled,\n",
    "    \"GCN - Labeled Only\",\n",
    "    gcn_labeled_optimizer,\n",
    "    criterion,\n",
    "    train_graph_labeled,\n",
    "    train_feature_labeled,\n",
    "    train_labels_labeled,\n",
    "    train_mask_labeled,\n",
    "    test_graph_labeled,\n",
    "    test_features_labeled,\n",
    "    test_labels_labeled,\n",
    "    test_mask_labeled,\n",
    "    num_epochs=800,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path=\"checkpoints/GCN/base_labeled/best_model_state.pt\"\n",
    ")\n",
    "\n",
    "save_history(history_gcn_labeled, sub_dir=\"GCN_base_labeled\")\n",
    "\n",
    "gcn_labeled_timestep_metrics = report_timestep_performance(\"GCN - Labeled Only\", gcn_labeled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d64a4e1",
   "metadata": {},
   "source": [
    "### Training with unknown nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3860cb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.9807, Train F1 0.2198, Test Loss 0.9828, Test F1 0.1355\n",
      "Epoch 050: Loss 0.2360, Train F1 0.7031, Test Loss 0.4017, Test F1 0.3109\n",
      "Epoch 100: Loss 0.1978, Train F1 0.7648, Test Loss 0.3351, Test F1 0.3862\n",
      "Epoch 150: Loss 0.1752, Train F1 0.7952, Test Loss 0.2977, Test F1 0.4368\n",
      "Epoch 200: Loss 0.1600, Train F1 0.8126, Test Loss 0.2815, Test F1 0.4541\n",
      "Epoch 250: Loss 0.1487, Train F1 0.8240, Test Loss 0.2761, Test F1 0.4643\n",
      "Epoch 300: Loss 0.1399, Train F1 0.8364, Test Loss 0.2757, Test F1 0.4779\n",
      "Epoch 350: Loss 0.1327, Train F1 0.8470, Test Loss 0.2772, Test F1 0.4849\n",
      "Epoch 400: Loss 0.1266, Train F1 0.8544, Test Loss 0.2788, Test F1 0.4980\n",
      "Epoch 450: Loss 0.1212, Train F1 0.8628, Test Loss 0.2795, Test F1 0.5211\n",
      "Epoch 500: Loss 0.1164, Train F1 0.8675, Test Loss 0.2806, Test F1 0.5505\n",
      "Epoch 550: Loss 0.1120, Train F1 0.8740, Test Loss 0.2839, Test F1 0.5559\n",
      "Epoch 600: Loss 0.1080, Train F1 0.8787, Test Loss 0.2886, Test F1 0.5580\n",
      "Epoch 650: Loss 0.1043, Train F1 0.8825, Test Loss 0.2932, Test F1 0.5628\n",
      "Epoch 700: Loss 0.1009, Train F1 0.8865, Test Loss 0.2984, Test F1 0.5647\n",
      "Epoch 750: Loss 0.0977, Train F1 0.8893, Test Loss 0.3018, Test F1 0.5678\n",
      "Epoch 800: Loss 0.0947, Train F1 0.8930, Test Loss 0.3052, Test F1 0.5711\n",
      "GCN - All Nodes Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.72      0.47      0.57      1083\n",
      "       licit       0.96      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.84      0.73      0.77     16670\n",
      "weighted avg       0.95      0.95      0.95     16670\n",
      "\n",
      "GCN - All Nodes Best Classification Report on Labeled Test Graph at epoch 800:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.72      0.47      0.57      1083\n",
      "       licit       0.96      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.84      0.73      0.77     16670\n",
      "weighted avg       0.95      0.95      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/GCN_base_all/history.json\n",
      " - Best state: checkpoints/GCN_base_all/best_model_state.pt\n",
      " - Latest state: checkpoints/GCN_base_all/latest_model_state.pt\n",
      "[GCN - All Nodes] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.1895 | P=0.8750 | R=0.8077 | F1=0.8400\n",
      "  t=36 | nodes= 1708 | Loss=0.1062 | P=0.5455 | R=0.5455 | F1=0.5455\n",
      "  t=37 | nodes=  498 | Loss=0.2533 | P=0.7600 | R=0.4750 | F1=0.5846\n",
      "  t=38 | nodes=  756 | Loss=0.2611 | P=0.8364 | R=0.8288 | F1=0.8326\n",
      "  t=39 | nodes= 1183 | Loss=0.1853 | P=0.8243 | R=0.7531 | F1=0.7871\n",
      "  t=40 | nodes= 1211 | Loss=0.4640 | P=0.9000 | R=0.4018 | F1=0.5556\n",
      "  t=41 | nodes= 1132 | Loss=0.5825 | P=0.8421 | R=0.6897 | F1=0.7583\n",
      "  t=42 | nodes= 2154 | Loss=0.3797 | P=0.9029 | R=0.6611 | F1=0.7633\n",
      "  t=43 | nodes= 1370 | Loss=0.2951 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.2800 | P=0.0769 | R=0.0417 | F1=0.0541\n",
      "  t=45 | nodes= 1221 | Loss=0.0765 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0463 | P=0.1429 | R=0.5000 | F1=0.2222\n",
      "  t=47 | nodes=  846 | Loss=0.3469 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.8643 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.5327 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.8493 | R=0.6783 | F1=0.7543\n",
      "  Aggregate t>=43 | P=0.0171 | R=0.0118 | F1=0.0140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "gcn_all = GCN(train_feature_all.shape[1], embedding_dim, 2).to(device)\n",
    "gcn_all_optimizer = torch.optim.Adam(gcn_all.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_gcn_all = train_model(\n",
    "    gcn_all,\n",
    "    \"GCN - All Nodes\",\n",
    "    gcn_all_optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    train_feature_all,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    test_features_all,\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=800,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path=\"checkpoints/GCN/base_all/best_model_state.pt\"\n",
    ")\n",
    "\n",
    "save_history(history_gcn_all, sub_dir=\"GCN_base_all\")\n",
    "\n",
    "gcn_all_timestep_metrics = report_timestep_performance(\"GCN - All Nodes\", gcn_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739b760",
   "metadata": {},
   "source": [
    "## BGRL + GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb6579b",
   "metadata": {},
   "source": [
    "### Pretraining Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "53071ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train graph for SSL: Graph(num_nodes=136265, num_edges=156843,\n",
      "      ndata_schemes={'feat': Scheme(shape=(165,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "# BGRL self-supervised pretraining on the full training set\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ssl_embedding_dim = 100\n",
    "pretrain_epochs = 30\n",
    "edge_drop_prob = 0.2\n",
    "feat_drop_prob = 0.1\n",
    "momentum = 0.99\n",
    "ssl_lr = 1e-3\n",
    "raw_feature_count = 94\n",
    "\n",
    "base_pretrain_graph = train_all_graph.to(device)\n",
    "pretrain_features = base_pretrain_graph.ndata['feat']\n",
    "num_total_features = pretrain_features.shape[1]\n",
    "agg_feature_count = num_total_features - raw_feature_count\n",
    "if agg_feature_count < 0:\n",
    "    raise ValueError(\"raw_feature_count is larger than available feature dimensions\")\n",
    "\n",
    "print(f\"Train graph for SSL: {base_pretrain_graph}\")\n",
    "\n",
    "# Pre-compute adjacency helper so edge dropout never isolates a node\n",
    "src_full, dst_full = base_pretrain_graph.edges()\n",
    "edge_indices = torch.arange(base_pretrain_graph.num_edges(), device=device)\n",
    "node_edge_nodes = torch.cat([src_full, dst_full])\n",
    "node_edge_edges = torch.cat([edge_indices, edge_indices])\n",
    "order = torch.argsort(node_edge_nodes)\n",
    "node_edge_nodes = node_edge_nodes[order]\n",
    "node_edge_edges = node_edge_edges[order]\n",
    "node_ptr = torch.searchsorted(node_edge_nodes, torch.arange(base_pretrain_graph.num_nodes() + 1, device=device))\n",
    "edge_helper = {\n",
    "    'src': src_full,\n",
    "    'dst': dst_full,\n",
    "    'node_edge_nodes': node_edge_nodes,\n",
    "    'node_edge_edges': node_edge_edges,\n",
    "    'node_ptr': node_ptr,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280a34f",
   "metadata": {},
   "source": [
    "### Augmentation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "78fc7a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_masks(features):\n",
    "    raw_part = features[:, :raw_feature_count]\n",
    "    raw_mask = torch.empty_like(raw_part).uniform_(0.2, 0.4)\n",
    "    masked_raw = raw_part * raw_mask\n",
    "    if agg_feature_count == 0:\n",
    "        return masked_raw\n",
    "    agg_part = features[:, raw_feature_count:]\n",
    "    agg_mask = torch.empty_like(agg_part).uniform_(0.05, 0.2)\n",
    "    masked_agg = agg_part * agg_mask\n",
    "    return torch.cat([masked_raw, masked_agg], dim=1)\n",
    "\n",
    "\n",
    "def ensure_connectivity(keep_mask):\n",
    "    kept_idx = torch.nonzero(keep_mask, as_tuple=False).squeeze(1)\n",
    "    if kept_idx.numel() == 0:\n",
    "        random_edge = torch.randint(0, keep_mask.shape[0], (1,), device=keep_mask.device)\n",
    "        keep_mask[random_edge] = True\n",
    "        kept_idx = random_edge\n",
    "    num_nodes = base_pretrain_graph.num_nodes()\n",
    "    deg = torch.zeros(num_nodes, device=keep_mask.device, dtype=torch.int64)\n",
    "    deg.scatter_add_(0, src_full[kept_idx], torch.ones_like(kept_idx, dtype=torch.int64))\n",
    "    deg.scatter_add_(0, dst_full[kept_idx], torch.ones_like(kept_idx, dtype=torch.int64))\n",
    "    zero_nodes = (deg == 0).nonzero(as_tuple=False).squeeze(1)\n",
    "    if zero_nodes.numel() == 0:\n",
    "        return keep_mask\n",
    "    node_edge_nodes = edge_helper['node_edge_nodes']\n",
    "    node_edge_edges = edge_helper['node_edge_edges']\n",
    "    node_ptr = edge_helper['node_ptr']\n",
    "    for node in zero_nodes.tolist():\n",
    "        start = int(node_ptr[node].item())\n",
    "        end = int(node_ptr[node + 1].item())\n",
    "        if start == end:\n",
    "            continue\n",
    "        candidates = node_edge_edges[start:end]\n",
    "        chosen = candidates[torch.randint(0, candidates.shape[0], (1,), device=candidates.device)]\n",
    "        keep_mask[chosen] = True\n",
    "    return keep_mask\n",
    "\n",
    "\n",
    "def random_edge_dropout_preserve(drop_prob):\n",
    "    if drop_prob <= 0 or base_pretrain_graph.num_edges() == 0:\n",
    "        return base_pretrain_graph\n",
    "    mask = torch.rand(src_full.shape[0], device=device) > drop_prob\n",
    "    mask = ensure_connectivity(mask)\n",
    "    kept_idx = torch.nonzero(mask, as_tuple=False).squeeze(1)\n",
    "    aug = dgl.graph((src_full[kept_idx], dst_full[kept_idx]), num_nodes=base_pretrain_graph.num_nodes(), device=device)\n",
    "    return aug\n",
    "\n",
    "\n",
    "def graph_augment():\n",
    "    aug_graph = random_edge_dropout_preserve(edge_drop_prob)\n",
    "    aug_graph = dgl.add_self_loop(aug_graph)\n",
    "    masked_feats = apply_feature_masks(pretrain_features)\n",
    "    if feat_drop_prob > 0:\n",
    "        masked_feats = F.dropout(masked_feats, p=feat_drop_prob, training=True)\n",
    "    return aug_graph, masked_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9c2c8",
   "metadata": {},
   "source": [
    "### BGRL Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5f840ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projector(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class BGRL(nn.Module):\n",
    "    def __init__(self, encoder, hidden_dim, momentum=0.99):\n",
    "        super().__init__()\n",
    "        self.online_encoder = encoder\n",
    "        self.online_projector = Projector(hidden_dim)\n",
    "        self.target_encoder = copy.deepcopy(encoder)\n",
    "        self.target_projector = copy.deepcopy(self.online_projector)\n",
    "        for p in self.target_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.target_projector.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.predictor = Predictor(hidden_dim)\n",
    "        self.momentum = momentum\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_target(self):\n",
    "        for target_param, online_param in zip(self.target_encoder.parameters(), self.online_encoder.parameters()):\n",
    "            target_param.data = self.momentum * target_param.data + (1 - self.momentum) * online_param.data\n",
    "        for target_param, online_param in zip(self.target_projector.parameters(), self.online_projector.parameters()):\n",
    "            target_param.data = self.momentum * target_param.data + (1 - self.momentum) * online_param.data\n",
    "\n",
    "    def loss_fn(self, p, z):\n",
    "        p = F.normalize(p, dim=1)\n",
    "        z = F.normalize(z.detach(), dim=1)\n",
    "        return 2 - 2 * (p * z).sum(dim=1).mean()\n",
    "\n",
    "    def forward(self, g1, x1, g2, x2):\n",
    "        h1 = self.online_encoder(g1, x1)\n",
    "        h2 = self.online_encoder(g2, x2)\n",
    "        z1 = self.online_projector(h1)\n",
    "        z2 = self.online_projector(h2)\n",
    "        p1 = self.predictor(z1)\n",
    "        p2 = self.predictor(z2)\n",
    "        with torch.no_grad():\n",
    "            t1 = self.target_projector(self.target_encoder(g1, x1))\n",
    "            t2 = self.target_projector(self.target_encoder(g2, x2))\n",
    "        return self.loss_fn(p1, t2) + self.loss_fn(p2, t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5f4004",
   "metadata": {},
   "source": [
    "### Pretraining Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cd766c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BGRL pretraining...\n",
      "SSL Epoch 0001 | Loss: 4.0515\n",
      "SSL Epoch 0020 | Loss: 0.2649\n",
      "Finished BGRL pretraining. The frozen encoder is available as `pretrained_encoder`.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "ssl_model = BGRL(GCN(num_total_features, ssl_embedding_dim, ssl_embedding_dim).to(device), ssl_embedding_dim, momentum=momentum).to(device)\n",
    "optimizer = torch.optim.Adam(ssl_model.parameters(), lr=ssl_lr, weight_decay=1e-4)\n",
    "\n",
    "print(\"Starting BGRL pretraining...\")\n",
    "for epoch in range(1, pretrain_epochs + 1):\n",
    "    g1, x1 = graph_augment()\n",
    "    g2, x2 = graph_augment()\n",
    "    loss = ssl_model(g1, x1, g2, x2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    ssl_model.update_target()\n",
    "\n",
    "    if epoch == 1 or epoch % 20 == 0:\n",
    "        print(f\"SSL Epoch {epoch:04d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "pretrained_encoder = ssl_model.online_encoder\n",
    "pretrained_encoder.eval()\n",
    "for param in pretrained_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Finished BGRL pretraining. The frozen encoder is available as `pretrained_encoder`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5953f3c",
   "metadata": {},
   "source": [
    "### BGRL Embeddings + Raw Features + GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ad22986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.5833, Train F1 0.2572, Test Loss 0.4547, Test F1 0.1174\n",
      "Epoch 050: Loss 0.2051, Train F1 0.7528, Test Loss 0.2981, Test F1 0.3854\n",
      "Epoch 100: Loss 0.1682, Train F1 0.7954, Test Loss 0.2755, Test F1 0.4434\n",
      "Epoch 150: Loss 0.1438, Train F1 0.8290, Test Loss 0.2742, Test F1 0.4808\n",
      "Epoch 200: Loss 0.1254, Train F1 0.8533, Test Loss 0.2736, Test F1 0.5224\n",
      "Epoch 250: Loss 0.1114, Train F1 0.8728, Test Loss 0.2776, Test F1 0.5821\n",
      "Epoch 300: Loss 0.1002, Train F1 0.8881, Test Loss 0.2850, Test F1 0.5979\n",
      "Epoch 350: Loss 0.0913, Train F1 0.8989, Test Loss 0.2916, Test F1 0.6043\n",
      "Epoch 400: Loss 0.0843, Train F1 0.9078, Test Loss 0.2998, Test F1 0.6057\n",
      "Epoch 450: Loss 0.0784, Train F1 0.9145, Test Loss 0.3056, Test F1 0.5996\n",
      "Epoch 500: Loss 0.0735, Train F1 0.9219, Test Loss 0.3088, Test F1 0.5999\n",
      "Epoch 550: Loss 0.0694, Train F1 0.9280, Test Loss 0.3113, Test F1 0.6037\n",
      "Epoch 600: Loss 0.0657, Train F1 0.9306, Test Loss 0.3179, Test F1 0.6018\n",
      "BGRL Embeddings + Raw Features Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.73      0.51      0.60      1083\n",
      "       licit       0.97      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.85      0.75      0.79     16670\n",
      "weighted avg       0.95      0.96      0.95     16670\n",
      "\n",
      "BGRL Embeddings + Raw Features Best Classification Report on Labeled Test Graph at epoch 400:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.76      0.50      0.61      1083\n",
      "       licit       0.97      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.86      0.75      0.79     16670\n",
      "weighted avg       0.95      0.96      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/BGRL_gcn_concat/history.json\n",
      " - Best state: checkpoints/BGRL_gcn_concat/best_model_state.pt\n",
      " - Latest state: checkpoints/BGRL_gcn_concat/latest_model_state.pt\n",
      "[BGRL Embeddings + Raw Features] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.1816 | P=0.9024 | R=0.8132 | F1=0.8555\n",
      "  t=36 | nodes= 1708 | Loss=0.0754 | P=0.6000 | R=0.6364 | F1=0.6176\n",
      "  t=37 | nodes=  498 | Loss=0.3061 | P=0.6957 | R=0.4000 | F1=0.5079\n",
      "  t=38 | nodes=  756 | Loss=0.2461 | P=0.8198 | R=0.8198 | F1=0.8198\n",
      "  t=39 | nodes= 1183 | Loss=0.2123 | P=0.7200 | R=0.6667 | F1=0.6923\n",
      "  t=40 | nodes= 1211 | Loss=0.5373 | P=0.8028 | R=0.5089 | F1=0.6230\n",
      "  t=41 | nodes= 1132 | Loss=0.7331 | P=0.8100 | R=0.6983 | F1=0.7500\n",
      "  t=42 | nodes= 2154 | Loss=0.4711 | P=0.8316 | R=0.6820 | F1=0.7494\n",
      "  t=43 | nodes= 1370 | Loss=0.3149 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.3464 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=45 | nodes= 1221 | Loss=0.0835 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0589 | P=0.1111 | R=0.5000 | F1=0.1818\n",
      "  t=47 | nodes=  846 | Loss=0.3136 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.8630 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.6344 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.8142 | R=0.6904 | F1=0.7472\n",
      "  Aggregate t>=43 | P=0.0100 | R=0.0059 | F1=0.0074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "if 'pretrained_encoder' not in globals():\n",
    "    raise RuntimeError('Run the BGRL pretraining cells first to populate `pretrained_encoder`.')\n",
    "\n",
    "pretrained_encoder = pretrained_encoder.to(device).eval()\n",
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ssl_train_embeddings = pretrained_encoder(train_graph_all, train_feature_all).detach()\n",
    "    ssl_test_embeddings = pretrained_encoder(test_graph_all, test_features_all).detach()\n",
    "\n",
    "ssl_aug_train = torch.cat([train_feature_all, ssl_train_embeddings], dim=1)\n",
    "ssl_aug_test = torch.cat([test_features_all, ssl_test_embeddings], dim=1)\n",
    "\n",
    "bgrl_concat_gcn = GCN(ssl_aug_train.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(bgrl_concat_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_bgrl_concat = train_model(\n",
    "    bgrl_concat_gcn,\n",
    "    'BGRL Embeddings + Raw Features',\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    ssl_aug_train,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    ssl_aug_test,\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=600,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path='checkpoints/BGRL/gcn_concat/best_model_state.pt'\n",
    ")\n",
    "\n",
    "save_history(history_bgrl_concat, sub_dir='BGRL_gcn_concat')\n",
    "\n",
    "\n",
    "def build_bgrl_concat_features(g_eval, feats):\n",
    "    g_eval = g_eval.to(device)\n",
    "    feats = feats.to(device)\n",
    "    with torch.no_grad():\n",
    "        ssl_embeds = pretrained_encoder(g_eval, feats)\n",
    "    return torch.cat([feats, ssl_embeds], dim=1)\n",
    "\n",
    "\n",
    "bgrl_concat_timestep_metrics = report_timestep_performance(\n",
    "    \"BGRL Embeddings + Raw Features\",\n",
    "    bgrl_concat_gcn,\n",
    "    feature_builder=build_bgrl_concat_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24715a69",
   "metadata": {},
   "source": [
    "### BGRL Node Score + Raw Features + GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ba411e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.5961, Train F1 0.1110, Test Loss 0.4229, Test F1 0.0649\n",
      "Epoch 050: Loss 0.2151, Train F1 0.7518, Test Loss 0.3368, Test F1 0.3586\n",
      "Epoch 100: Loss 0.1766, Train F1 0.7866, Test Loss 0.2932, Test F1 0.4241\n",
      "Epoch 150: Loss 0.1564, Train F1 0.8105, Test Loss 0.2869, Test F1 0.4455\n",
      "Epoch 200: Loss 0.1424, Train F1 0.8278, Test Loss 0.2862, Test F1 0.4560\n",
      "Epoch 250: Loss 0.1313, Train F1 0.8448, Test Loss 0.2859, Test F1 0.4851\n",
      "Epoch 300: Loss 0.1221, Train F1 0.8591, Test Loss 0.2859, Test F1 0.5142\n",
      "Epoch 350: Loss 0.1143, Train F1 0.8688, Test Loss 0.2888, Test F1 0.5482\n",
      "Epoch 400: Loss 0.1075, Train F1 0.8769, Test Loss 0.2935, Test F1 0.5651\n",
      "Epoch 450: Loss 0.1017, Train F1 0.8834, Test Loss 0.2975, Test F1 0.5793\n",
      "Epoch 500: Loss 0.0965, Train F1 0.8901, Test Loss 0.3007, Test F1 0.5926\n",
      "Epoch 550: Loss 0.0917, Train F1 0.8950, Test Loss 0.3042, Test F1 0.5943\n",
      "Epoch 600: Loss 0.0874, Train F1 0.9019, Test Loss 0.3085, Test F1 0.5938\n",
      "BGRL Node Score + Raw Features Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.71      0.51      0.59      1083\n",
      "       licit       0.97      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.84      0.75      0.78     16670\n",
      "weighted avg       0.95      0.95      0.95     16670\n",
      "\n",
      "BGRL Node Score + Raw Features Best Classification Report on Labeled Test Graph at epoch 550:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.73      0.50      0.59      1083\n",
      "       licit       0.97      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.85      0.75      0.79     16670\n",
      "weighted avg       0.95      0.96      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/BGRL_gcn_scores/history.json\n",
      " - Best state: checkpoints/BGRL_gcn_scores/best_model_state.pt\n",
      " - Latest state: checkpoints/BGRL_gcn_scores/latest_model_state.pt\n",
      "[BGRL Node Score + Raw Features] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.1654 | P=0.8621 | R=0.8242 | F1=0.8427\n",
      "  t=36 | nodes= 1708 | Loss=0.1057 | P=0.4524 | R=0.5758 | F1=0.5067\n",
      "  t=37 | nodes=  498 | Loss=0.2301 | P=0.8148 | R=0.5500 | F1=0.6567\n",
      "  t=38 | nodes=  756 | Loss=0.2750 | P=0.8349 | R=0.8198 | F1=0.8273\n",
      "  t=39 | nodes= 1183 | Loss=0.1608 | P=0.7875 | R=0.7778 | F1=0.7826\n",
      "  t=40 | nodes= 1211 | Loss=0.4538 | P=0.9474 | R=0.4821 | F1=0.6391\n",
      "  t=41 | nodes= 1132 | Loss=0.8317 | P=0.8438 | R=0.6983 | F1=0.7642\n",
      "  t=42 | nodes= 2154 | Loss=0.3723 | P=0.8830 | R=0.6946 | F1=0.7775\n",
      "  t=43 | nodes= 1370 | Loss=0.2709 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.2900 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=45 | nodes= 1221 | Loss=0.0676 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0468 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=47 | nodes=  846 | Loss=0.3231 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.8428 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.4155 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.8357 | R=0.7068 | F1=0.7659\n",
      "  Aggregate t>=43 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "\n",
    "if 'pretrained_encoder' not in globals():\n",
    "    raise RuntimeError('Run the BGRL pretraining cells first to populate `pretrained_encoder`.')\n",
    "\n",
    "if 'ssl_train_embeddings' not in globals() or 'ssl_test_embeddings' not in globals():\n",
    "    raise RuntimeError('Generate SSL embeddings before training the score-based head.')\n",
    "\n",
    "with torch.no_grad():\n",
    "    bgrl_train_scores = torch.norm(ssl_train_embeddings, dim=1, keepdim=True)\n",
    "    bgrl_test_scores = torch.norm(ssl_test_embeddings, dim=1, keepdim=True)\n",
    "\n",
    "bgrl_score_mean = bgrl_train_scores.mean()\n",
    "bgrl_score_std = bgrl_train_scores.std().clamp_min(1e-6)\n",
    "bgrl_train_scores = (bgrl_train_scores - bgrl_score_mean) / bgrl_score_std\n",
    "bgrl_test_scores = (bgrl_test_scores - bgrl_score_mean) / bgrl_score_std\n",
    "\n",
    "bgrl_score_train = torch.cat([train_feature_all, bgrl_train_scores.to(device)], dim=1)\n",
    "bgrl_score_test = torch.cat([test_features_all, bgrl_test_scores.to(device)], dim=1)\n",
    "\n",
    "bgrl_score_gcn = GCN(bgrl_score_train.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(bgrl_score_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_bgrl_score = train_model(\n",
    "    bgrl_score_gcn,\n",
    "    \"BGRL Node Score + Raw Features\",\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    bgrl_score_train,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    bgrl_score_test,\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=600,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path='checkpoints/BGRL/gcn_scores/best_model_state.pt'\n",
    ")\n",
    "\n",
    "save_history(history_bgrl_score, sub_dir='BGRL_gcn_scores')\n",
    "\n",
    "\n",
    "def build_bgrl_score_features(g_eval, feats):\n",
    "    g_eval = g_eval.to(device)\n",
    "    feats = feats.to(device)\n",
    "    with torch.no_grad():\n",
    "        ssl_embeds = pretrained_encoder(g_eval, feats)\n",
    "        scores = torch.norm(ssl_embeds, dim=1, keepdim=True)\n",
    "    scores = (scores - bgrl_score_mean) / bgrl_score_std\n",
    "    return torch.cat([feats, scores], dim=1)\n",
    "\n",
    "\n",
    "bgrl_score_timestep_metrics = report_timestep_performance(\n",
    "    \"BGRL Node Score + Raw Features\",\n",
    "    bgrl_score_gcn,\n",
    "    feature_builder=build_bgrl_score_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fde264",
   "metadata": {},
   "source": [
    "## GAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc341e",
   "metadata": {},
   "source": [
    "### Pretraining Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bae2439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if 'train_all_graph' not in globals():\n",
    "    raise RuntimeError('Load the graphs first so `train_all_graph` is defined.')\n",
    "if 'GCN' not in globals():\n",
    "    raise RuntimeError('Run the GCN baseline cell to define the `GCN` encoder class.')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "gae_hidden_dim = 256\n",
    "gae_latent_dim = 128\n",
    "gae_epochs = 30\n",
    "gae_lr = 1e-3\n",
    "alpha_attr = 0.5\n",
    "neg_sample_ratio = 1.0\n",
    "max_struct_samples = 200_000\n",
    "\n",
    "\n",
    "def prepare_graph_for_gae(graph):\n",
    "    graph = dgl.to_bidirected(graph, copy_ndata=True)\n",
    "    return dgl.add_self_loop(graph)\n",
    "\n",
    "\n",
    "gae_graph = prepare_graph_for_gae(train_all_graph).to(device)\n",
    "gae_features = gae_graph.ndata['feat'].float().to(device)\n",
    "num_nodes = gae_graph.num_nodes()\n",
    "in_feats = gae_features.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf8b145",
   "metadata": {},
   "source": [
    "### Encoder and Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a5d2b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InnerProductDecoder(nn.Module):\n",
    "    def forward(self, z, src, dst):\n",
    "        return (z[src] * z[dst]).sum(dim=1)\n",
    "\n",
    "\n",
    "gae_encoder = GCN(in_feats, gae_hidden_dim, gae_latent_dim).to(device)\n",
    "gae_structure_decoder = InnerProductDecoder().to(device)\n",
    "gae_attribute_decoder = nn.Sequential(\n",
    "    nn.Linear(gae_latent_dim, gae_hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(gae_hidden_dim, in_feats)\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(gae_encoder.parameters()) + list(gae_attribute_decoder.parameters()),\n",
    "    lr=gae_lr,\n",
    "    weight_decay=5e-4,\n",
    ")\n",
    "\n",
    "src_all, dst_all = gae_graph.edges()\n",
    "src_all = src_all.to(device)\n",
    "dst_all = dst_all.to(device)\n",
    "\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "mse_loss = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae39c1",
   "metadata": {},
   "source": [
    "### Sampling Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a039182",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_positive_edges(num_samples):\n",
    "    if num_samples >= src_all.shape[0]:\n",
    "        return src_all, dst_all\n",
    "    idx = torch.randint(0, src_all.shape[0], (num_samples,), device=device)\n",
    "    return src_all[idx], dst_all[idx]\n",
    "\n",
    "\n",
    "def sample_negative_edges(num_samples):\n",
    "    neg_src = torch.randint(0, num_nodes, (num_samples,), device=device)\n",
    "    neg_dst = torch.randint(0, num_nodes, (num_samples,), device=device)\n",
    "    return neg_src, neg_dst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face4e6f",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cdc24d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Graph Autoencoder (GAE) pretraining on train_all_graph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAE Epoch 0001 | Total: 4.0057 | Struct: 3.5143 | Attr: 0.9829\n",
      "GAE Epoch 0020 | Total: 0.9296 | Struct: 0.5007 | Attr: 0.8580\n",
      "GAE Epoch 0030 | Total: 0.8355 | Struct: 0.4401 | Attr: 0.7909\n",
      "Finished GAE pretraining. Saved embeddings for downstream tasks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Starting Graph Autoencoder (GAE) pretraining on train_all_graph...')\n",
    "for epoch in range(1, gae_epochs + 1):\n",
    "    gae_encoder.train()\n",
    "    gae_attribute_decoder.train()\n",
    "\n",
    "    latent_z = gae_encoder(gae_graph, gae_features)\n",
    "\n",
    "    num_pos_samples = min(src_all.shape[0], max_struct_samples)\n",
    "    pos_src, pos_dst = sample_positive_edges(num_pos_samples)\n",
    "    pos_logits = gae_structure_decoder(latent_z, pos_src, pos_dst)\n",
    "    pos_labels = torch.ones_like(pos_logits)\n",
    "\n",
    "    num_neg_samples = max(1, int(neg_sample_ratio * num_pos_samples))\n",
    "    neg_src, neg_dst = sample_negative_edges(num_neg_samples)\n",
    "    neg_logits = gae_structure_decoder(latent_z, neg_src, neg_dst)\n",
    "    neg_labels = torch.zeros_like(neg_logits)\n",
    "\n",
    "    struct_loss = bce_loss(\n",
    "        torch.cat([pos_logits, neg_logits], dim=0),\n",
    "        torch.cat([pos_labels, neg_labels], dim=0)\n",
    "    )\n",
    "\n",
    "    recon_features = gae_attribute_decoder(latent_z)\n",
    "    attr_loss = mse_loss(recon_features, gae_features)\n",
    "\n",
    "    loss = struct_loss + alpha_attr * attr_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch == 1 or epoch % 20 == 0 or epoch == gae_epochs:\n",
    "        print(\n",
    "            f'GAE Epoch {epoch:04d} | Total: {loss.item():.4f} | '\n",
    "            f'Struct: {struct_loss.item():.4f} | Attr: {attr_loss.item():.4f}'\n",
    "        )\n",
    "\n",
    "gae_encoder.eval()\n",
    "gae_structure_decoder.eval()\n",
    "gae_attribute_decoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    gae_train_embeddings = gae_encoder(gae_graph, gae_features).detach()\n",
    "    gae_train_attr_recon = gae_attribute_decoder(gae_train_embeddings).detach()\n",
    "\n",
    "print('Finished GAE pretraining. Saved embeddings for downstream tasks.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c7bcea",
   "metadata": {},
   "source": [
    "### Reconstruction Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "94738c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score_graph_with_gae(graph, feature_key='feat'):\n",
    "    graph = prepare_graph_for_gae(graph).to(device)\n",
    "    features = graph.ndata[feature_key].float().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = gae_encoder(graph, features)\n",
    "        feat_recon = gae_attribute_decoder(z)\n",
    "\n",
    "        attr_err = ((feat_recon - features) ** 2).sum(dim=1)\n",
    "\n",
    "        src, dst = graph.edges()\n",
    "        src = src.to(device)\n",
    "        dst = dst.to(device)\n",
    "        logits = gae_structure_decoder(z, src, dst)\n",
    "        labels = torch.ones_like(logits)\n",
    "        edge_errors = F.binary_cross_entropy_with_logits(logits, labels, reduction='none')\n",
    "\n",
    "        node_struct_error = torch.zeros(graph.num_nodes(), device=device)\n",
    "        node_struct_error.scatter_add_(0, src, edge_errors)\n",
    "        node_struct_error.scatter_add_(0, dst, edge_errors)\n",
    "        degrees = (graph.out_degrees() + graph.in_degrees()).float().to(device)\n",
    "        degrees = torch.clamp(degrees, min=1.0)\n",
    "        node_struct_error = node_struct_error / degrees\n",
    "\n",
    "        combined_error = node_struct_error + alpha_attr * attr_err\n",
    "\n",
    "    return {\n",
    "        'struct_error': node_struct_error.detach().cpu(),\n",
    "        'attr_error': attr_err.detach().cpu(),\n",
    "        'combined_error': combined_error.detach().cpu(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6fba56",
   "metadata": {},
   "source": [
    "### GAE Embeddings + GCN (All Nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "66a3157a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.6896, Train F1 0.2153, Test Loss 0.5461, Test F1 0.1260\n",
      "Epoch 050: Loss 0.3138, Train F1 0.6457, Test Loss 0.3916, Test F1 0.2688\n",
      "Epoch 100: Loss 0.2479, Train F1 0.7197, Test Loss 0.3534, Test F1 0.3219\n",
      "Epoch 150: Loss 0.2190, Train F1 0.7429, Test Loss 0.3270, Test F1 0.3544\n",
      "Epoch 200: Loss 0.2027, Train F1 0.7567, Test Loss 0.3168, Test F1 0.3654\n",
      "Epoch 250: Loss 0.1914, Train F1 0.7712, Test Loss 0.3107, Test F1 0.3712\n",
      "Epoch 300: Loss 0.1829, Train F1 0.7817, Test Loss 0.3072, Test F1 0.3753\n",
      "Epoch 350: Loss 0.1760, Train F1 0.7911, Test Loss 0.3042, Test F1 0.3841\n",
      "Epoch 400: Loss 0.1701, Train F1 0.7989, Test Loss 0.3025, Test F1 0.3887\n",
      "Epoch 450: Loss 0.1651, Train F1 0.8040, Test Loss 0.3018, Test F1 0.3997\n",
      "Epoch 500: Loss 0.1606, Train F1 0.8100, Test Loss 0.3009, Test F1 0.4031\n",
      "Epoch 550: Loss 0.1566, Train F1 0.8153, Test Loss 0.3001, Test F1 0.4109\n",
      "Epoch 600: Loss 0.1530, Train F1 0.8190, Test Loss 0.3002, Test F1 0.4202\n",
      "Epoch 650: Loss 0.1496, Train F1 0.8243, Test Loss 0.3004, Test F1 0.4230\n",
      "Epoch 700: Loss 0.1465, Train F1 0.8296, Test Loss 0.3015, Test F1 0.4305\n",
      "Epoch 750: Loss 0.1436, Train F1 0.8323, Test Loss 0.3023, Test F1 0.4288\n",
      "Epoch 800: Loss 0.1409, Train F1 0.8346, Test Loss 0.3034, Test F1 0.4309\n",
      "GAE Embeddings -> GCN Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.72      0.31      0.43      1083\n",
      "       licit       0.95      0.99      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.84      0.65      0.70     16670\n",
      "weighted avg       0.94      0.95      0.94     16670\n",
      "\n",
      "GAE Embeddings -> GCN Best Classification Report on Labeled Test Graph at epoch 800:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.72      0.31      0.43      1083\n",
      "       licit       0.95      0.99      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.84      0.65      0.70     16670\n",
      "weighted avg       0.94      0.95      0.94     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/GAE_gcn_embeddings/history.json\n",
      " - Best state: checkpoints/GAE_gcn_embeddings/best_model_state.pt\n",
      " - Latest state: checkpoints/GAE_gcn_embeddings/latest_model_state.pt\n",
      "[GAE Embeddings -> GCN] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.2548 | P=0.8137 | R=0.7198 | F1=0.7638\n",
      "  t=36 | nodes= 1708 | Loss=0.1634 | P=0.4242 | R=0.4242 | F1=0.4242\n",
      "  t=37 | nodes=  498 | Loss=0.3379 | P=0.6667 | R=0.3500 | F1=0.4590\n",
      "  t=38 | nodes=  756 | Loss=0.5142 | P=0.6471 | R=0.2973 | F1=0.4074\n",
      "  t=39 | nodes= 1183 | Loss=0.3344 | P=0.7647 | R=0.3210 | F1=0.4522\n",
      "  t=40 | nodes= 1211 | Loss=0.6078 | P=0.8571 | R=0.2679 | F1=0.4082\n",
      "  t=41 | nodes= 1132 | Loss=0.7098 | P=0.9211 | R=0.6034 | F1=0.7292\n",
      "  t=42 | nodes= 2154 | Loss=0.4009 | P=0.8855 | R=0.6151 | F1=0.7259\n",
      "  t=43 | nodes= 1370 | Loss=0.2517 | P=0.0400 | R=0.0417 | F1=0.0408\n",
      "  t=44 | nodes= 1591 | Loss=0.2665 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=45 | nodes= 1221 | Loss=0.0781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0685 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=47 | nodes=  846 | Loss=0.2779 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.7056 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.1196 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.8059 | R=0.5088 | F1=0.6237\n",
      "  Aggregate t>=43 | P=0.0217 | R=0.0059 | F1=0.0093\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'gae_train_embeddings' not in globals():\n",
    "    raise RuntimeError('Run the GAE pretraining cell before launching this training step.')\n",
    "\n",
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "gae_train_features = gae_train_embeddings.to(device)\n",
    "\n",
    "_gae_test_graph = prepare_graph_for_gae(test_all_graph).to(device)\n",
    "with torch.no_grad():\n",
    "    gae_test_embeddings = gae_encoder(_gae_test_graph, _gae_test_graph.ndata['feat'].float()).detach()\n",
    "\n",
    "gae_gcn = GCN(gae_train_features.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(gae_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_gae_gcn = train_model(\n",
    "    gae_gcn,\n",
    "    \"GAE Embeddings -> GCN\",\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    gae_train_features,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    gae_test_embeddings.to(device),\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=800,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path=\"checkpoints/GAE/gcn_embeddings/best_model_state.pt\"\n",
    ")\n",
    "\n",
    "save_history(history_gae_gcn, sub_dir=\"GAE_gcn_embeddings\")\n",
    "\n",
    "\n",
    "def build_gae_embedding_features(g_eval, feats):\n",
    "    g_eval = g_eval.to(device)\n",
    "    feats = feats.float().to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = gae_encoder(g_eval, feats)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "gae_gcn_timestep_metrics = report_timestep_performance(\n",
    "    \"GAE Embeddings -> GCN\",\n",
    "    gae_gcn,\n",
    "    feature_builder=build_gae_embedding_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7689f5e4",
   "metadata": {},
   "source": [
    "### GAE Node Score + Raw Features + GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bfa509d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 1.2533, Train F1 0.1792, Test Loss 0.6454, Test F1 0.0439\n",
      "Epoch 050: Loss 0.2427, Train F1 0.7036, Test Loss 0.4302, Test F1 0.3104\n",
      "Epoch 100: Loss 0.2024, Train F1 0.7656, Test Loss 0.3564, Test F1 0.3670\n",
      "Epoch 150: Loss 0.1804, Train F1 0.7919, Test Loss 0.3101, Test F1 0.4309\n",
      "Epoch 200: Loss 0.1659, Train F1 0.8074, Test Loss 0.2856, Test F1 0.4784\n",
      "Epoch 250: Loss 0.1551, Train F1 0.8175, Test Loss 0.2734, Test F1 0.5086\n",
      "Epoch 300: Loss 0.1465, Train F1 0.8289, Test Loss 0.2670, Test F1 0.5247\n",
      "Epoch 350: Loss 0.1394, Train F1 0.8372, Test Loss 0.2640, Test F1 0.5459\n",
      "Epoch 400: Loss 0.1333, Train F1 0.8432, Test Loss 0.2639, Test F1 0.5600\n",
      "Epoch 450: Loss 0.1281, Train F1 0.8501, Test Loss 0.2656, Test F1 0.5758\n",
      "Epoch 500: Loss 0.1234, Train F1 0.8561, Test Loss 0.2682, Test F1 0.5870\n",
      "Epoch 550: Loss 0.1191, Train F1 0.8621, Test Loss 0.2713, Test F1 0.5891\n",
      "Epoch 600: Loss 0.1151, Train F1 0.8653, Test Loss 0.2745, Test F1 0.5875\n",
      "Epoch 650: Loss 0.1114, Train F1 0.8696, Test Loss 0.2775, Test F1 0.5892\n",
      "Epoch 700: Loss 0.1080, Train F1 0.8745, Test Loss 0.2809, Test F1 0.5908\n",
      "Epoch 750: Loss 0.1048, Train F1 0.8792, Test Loss 0.2846, Test F1 0.5877\n",
      "Epoch 800: Loss 0.1017, Train F1 0.8836, Test Loss 0.2888, Test F1 0.5837\n",
      "GAE Node Score + Raw Features Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.73      0.49      0.58      1083\n",
      "       licit       0.97      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.85      0.74      0.78     16670\n",
      "weighted avg       0.95      0.95      0.95     16670\n",
      "\n",
      "GAE Node Score + Raw Features Best Classification Report on Labeled Test Graph at epoch 700:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.74      0.49      0.59      1083\n",
      "       licit       0.97      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.85      0.74      0.78     16670\n",
      "weighted avg       0.95      0.96      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/GAE_gcn_scores/history.json\n",
      " - Best state: checkpoints/GAE_gcn_scores/best_model_state.pt\n",
      " - Latest state: checkpoints/GAE_gcn_scores/latest_model_state.pt\n",
      "[GAE Node Score + Raw Features] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.1662 | P=0.9085 | R=0.8187 | F1=0.8613\n",
      "  t=36 | nodes= 1708 | Loss=0.1009 | P=0.5625 | R=0.5455 | F1=0.5538\n",
      "  t=37 | nodes=  498 | Loss=0.2710 | P=0.8400 | R=0.5250 | F1=0.6462\n",
      "  t=38 | nodes=  756 | Loss=0.2682 | P=0.8381 | R=0.7928 | F1=0.8148\n",
      "  t=39 | nodes= 1183 | Loss=0.1823 | P=0.7683 | R=0.7778 | F1=0.7730\n",
      "  t=40 | nodes= 1211 | Loss=0.4363 | P=0.9600 | R=0.4286 | F1=0.5926\n",
      "  t=41 | nodes= 1132 | Loss=0.5867 | P=0.9000 | R=0.6983 | F1=0.7864\n",
      "  t=42 | nodes= 2154 | Loss=0.3807 | P=0.8750 | R=0.6736 | F1=0.7612\n",
      "  t=43 | nodes= 1370 | Loss=0.2796 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.2736 | P=0.0769 | R=0.0417 | F1=0.0541\n",
      "  t=45 | nodes= 1221 | Loss=0.0789 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0369 | P=0.2500 | R=0.5000 | F1=0.3333\n",
      "  t=47 | nodes=  846 | Loss=0.3336 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.7650 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.2628 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.8593 | R=0.6882 | F1=0.7643\n",
      "  Aggregate t>=43 | P=0.0241 | R=0.0118 | F1=0.0159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if 'gae_encoder' not in globals():\n",
    "    raise RuntimeError('Run the GAE pretraining cell before launching this training step.')\n",
    "\n",
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "train_scores = score_graph_with_gae(train_all_graph)\n",
    "test_scores = score_graph_with_gae(test_all_graph)\n",
    "\n",
    "gae_score_scaler = StandardScaler()\n",
    "train_anomaly_np = gae_score_scaler.fit_transform(train_scores['combined_error'].numpy().reshape(-1, 1))\n",
    "test_anomaly_np = gae_score_scaler.transform(test_scores['combined_error'].numpy().reshape(-1, 1))\n",
    "\n",
    "train_score_features = torch.tensor(train_anomaly_np, dtype=torch.float32)\n",
    "test_score_features = torch.tensor(test_anomaly_np, dtype=torch.float32)\n",
    "\n",
    "train_aug_features = torch.cat([train_feature_all.cpu(), train_score_features], dim=1).to(device)\n",
    "test_aug_features = torch.cat([test_features_all.cpu(), test_score_features], dim=1).to(device)\n",
    "\n",
    "gae_score_gcn = GCN(train_aug_features.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(gae_score_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_gae_score = train_model(\n",
    "    gae_score_gcn,\n",
    "    \"GAE Node Score + Raw Features\",\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    train_aug_features,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    test_aug_features,\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=800,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path='checkpoints/GAE/gcn_scores/best_model_state.pt'\n",
    ")\n",
    "\n",
    "save_history(history_gae_score, sub_dir='GAE_gcn_scores')\n",
    "\n",
    "\n",
    "def build_gae_score_features(g_eval, feats):\n",
    "    graph_cpu = g_eval.to('cpu')\n",
    "    score_outputs = score_graph_with_gae(graph_cpu)\n",
    "    anomaly_np = gae_score_scaler.transform(score_outputs['combined_error'].numpy().reshape(-1, 1))\n",
    "    anomaly_tensor = torch.tensor(anomaly_np, dtype=torch.float32, device=feats.device)\n",
    "    feats = feats.to(device)\n",
    "    return torch.cat([feats, anomaly_tensor], dim=1)\n",
    "\n",
    "\n",
    "gae_score_timestep_metrics = report_timestep_performance(\n",
    "    \"GAE Node Score + Raw Features\",\n",
    "    gae_score_gcn,\n",
    "    feature_builder=build_gae_score_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a69860",
   "metadata": {},
   "source": [
    "### Raw + GAE Embeddings Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "18b47ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.6485, Train F1 0.1848, Test Loss 0.6000, Test F1 0.1321\n",
      "Epoch 050: Loss 0.2101, Train F1 0.7509, Test Loss 0.3079, Test F1 0.3762\n",
      "Epoch 100: Loss 0.1703, Train F1 0.7993, Test Loss 0.2837, Test F1 0.4059\n",
      "Epoch 150: Loss 0.1483, Train F1 0.8265, Test Loss 0.2862, Test F1 0.4230\n",
      "Epoch 200: Loss 0.1330, Train F1 0.8463, Test Loss 0.2868, Test F1 0.4501\n",
      "Epoch 250: Loss 0.1213, Train F1 0.8586, Test Loss 0.2857, Test F1 0.4759\n",
      "Epoch 300: Loss 0.1118, Train F1 0.8706, Test Loss 0.2864, Test F1 0.5119\n",
      "Epoch 350: Loss 0.1037, Train F1 0.8825, Test Loss 0.2875, Test F1 0.5445\n",
      "Epoch 400: Loss 0.0967, Train F1 0.8907, Test Loss 0.2894, Test F1 0.5537\n",
      "Epoch 450: Loss 0.0907, Train F1 0.8988, Test Loss 0.2935, Test F1 0.5675\n",
      "Epoch 500: Loss 0.0854, Train F1 0.9046, Test Loss 0.2993, Test F1 0.5682\n",
      "Epoch 550: Loss 0.0808, Train F1 0.9098, Test Loss 0.3044, Test F1 0.5668\n",
      "Epoch 600: Loss 0.0768, Train F1 0.9152, Test Loss 0.3108, Test F1 0.5651\n",
      "Epoch 650: Loss 0.0734, Train F1 0.9199, Test Loss 0.3173, Test F1 0.5620\n",
      "Epoch 700: Loss 0.0703, Train F1 0.9243, Test Loss 0.3238, Test F1 0.5583\n",
      "Epoch 750: Loss 0.0675, Train F1 0.9295, Test Loss 0.3299, Test F1 0.5543\n",
      "Epoch 800: Loss 0.0651, Train F1 0.9327, Test Loss 0.3348, Test F1 0.5564\n",
      "Raw + GAE Embeddings Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.61      0.51      0.56      1083\n",
      "       licit       0.97      0.98      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.79      0.74      0.76     16670\n",
      "weighted avg       0.94      0.95      0.94     16670\n",
      "\n",
      "Raw + GAE Embeddings Best Classification Report on Labeled Test Graph at epoch 500:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.67      0.49      0.57      1083\n",
      "       licit       0.97      0.98      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.82      0.74      0.77     16670\n",
      "weighted avg       0.95      0.95      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/GAE_gcn_concat/history.json\n",
      " - Best state: checkpoints/GAE_gcn_concat/best_model_state.pt\n",
      " - Latest state: checkpoints/GAE_gcn_concat/latest_model_state.pt\n",
      "[Raw + GAE Embeddings] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.2179 | P=0.8343 | R=0.8022 | F1=0.8179\n",
      "  t=36 | nodes= 1708 | Loss=0.1210 | P=0.2807 | R=0.4848 | F1=0.3556\n",
      "  t=37 | nodes=  498 | Loss=0.2690 | P=0.6250 | R=0.3750 | F1=0.4688\n",
      "  t=38 | nodes=  756 | Loss=0.2767 | P=0.8333 | R=0.8108 | F1=0.8219\n",
      "  t=39 | nodes= 1183 | Loss=0.2674 | P=0.4706 | R=0.5926 | F1=0.5246\n",
      "  t=40 | nodes= 1211 | Loss=0.5281 | P=0.7778 | R=0.4375 | F1=0.5600\n",
      "  t=41 | nodes= 1132 | Loss=0.8555 | P=0.7895 | R=0.6466 | F1=0.7109\n",
      "  t=42 | nodes= 2154 | Loss=0.4127 | P=0.8342 | R=0.6946 | F1=0.7580\n",
      "  t=43 | nodes= 1370 | Loss=0.4342 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.3081 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=45 | nodes= 1221 | Loss=0.0649 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0511 | P=0.0909 | R=0.5000 | F1=0.1538\n",
      "  t=47 | nodes=  846 | Loss=0.3652 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.9959 | P=0.0500 | R=0.0278 | F1=0.0357\n",
      "  t=49 | nodes=  476 | Loss=1.8849 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.7351 | R=0.6619 | F1=0.6966\n",
      "  Aggregate t>=43 | P=0.0078 | R=0.0118 | F1=0.0094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "train_concat_features = torch.cat([train_feature_all, gae_train_embeddings.to(device)], dim=1)\n",
    "\n",
    "_gae_test_graph = prepare_graph_for_gae(test_all_graph).to(device)\n",
    "with torch.no_grad():\n",
    "    gae_test_embeddings = gae_encoder(_gae_test_graph, _gae_test_graph.ndata['feat'].float()).detach()\n",
    "\n",
    "test_concat_features = torch.cat([test_features_all, gae_test_embeddings.to(device)], dim=1)\n",
    "\n",
    "concat_gcn = GCN(train_concat_features.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(concat_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_concat_gcn = train_model(\n",
    "    concat_gcn,\n",
    "    \"Raw + GAE Embeddings\",\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    train_concat_features,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    test_concat_features,\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=800,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path=\"checkpoints/GAE/gcn_concat/best_model_state.pt\"\n",
    ")\n",
    "\n",
    "save_history(history_concat_gcn, sub_dir=\"GAE_gcn_concat\")\n",
    "\n",
    "\n",
    "def build_gae_concat_features(g_eval, feats):\n",
    "    g_eval = g_eval.to(device)\n",
    "    feats = feats.float().to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = gae_encoder(g_eval, feats)\n",
    "    return torch.cat([feats, embeddings], dim=1)\n",
    "\n",
    "\n",
    "gae_concat_timestep_metrics = report_timestep_performance(\n",
    "    \"Raw + GAE Embeddings\",\n",
    "    concat_gcn,\n",
    "    feature_builder=build_gae_concat_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c72a238",
   "metadata": {},
   "source": [
    "## Local-subgraph GAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea9293",
   "metadata": {},
   "source": [
    "### Pretraining Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8dbb26ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['feat', 'label', 'timestep'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import dgl\n",
    "import dgl.sampling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "if 'train_all_graph' not in globals():\n",
    "    raise RuntimeError('Run the data loading and graph construction cells first so `train_all_graph` exists.')\n",
    "\n",
    "local_gae_config = {\n",
    "    'rw_length': 24,\n",
    "    'rw_trials': 4,\n",
    "    'rw_restart_prob': 0.3,\n",
    "    'hidden_dim': 128,\n",
    "    'epochs': 20,\n",
    "    'samples_per_epoch': 4000,\n",
    "    'lr': 5e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'val_sample_size': 1024,\n",
    "    'val_interval': 5,\n",
    "}\n",
    "\n",
    "local_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "base_graph_for_local_gae = dgl.add_self_loop(dgl.to_bidirected(train_all_graph, copy_ndata=True))\n",
    "base_graph_for_local_gae.ndata['feat'] = base_graph_for_local_gae.ndata['feat'].float()\n",
    "local_train_feats = base_graph_for_local_gae.ndata['feat']\n",
    "local_num_nodes = base_graph_for_local_gae.num_nodes()\n",
    "\n",
    "print(base_graph_for_local_gae.ndata.keys())\n",
    "\n",
    "samples_per_epoch = local_gae_config['samples_per_epoch']\n",
    "if samples_per_epoch is None:\n",
    "    samples_per_epoch = local_num_nodes\n",
    "samples_per_epoch = max(1, min(local_num_nodes, samples_per_epoch))\n",
    "\n",
    "val_sample_size = max(0, int(local_gae_config.get('val_sample_size', 0)))\n",
    "if val_sample_size > 0:\n",
    "    validation_nodes = torch.randperm(local_num_nodes)[:min(local_num_nodes, val_sample_size)]\n",
    "else:\n",
    "    validation_nodes = torch.tensor([], dtype=torch.long)\n",
    "validation_interval = max(1, int(local_gae_config.get('val_interval', 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f37f6b",
   "metadata": {},
   "source": [
    "### Random-Walk Sampling Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2bed4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def anonymized_random_walk_subgraph(\n",
    "    graph,\n",
    "    feature_tensor,\n",
    "    target_nid,\n",
    "    *,\n",
    "    walk_length,\n",
    "    num_traces,\n",
    "    restart_prob,\n",
    "    fallback_khop=2,\n",
    "):\n",
    "    \"\"\"Sample a random-walk-with-restart ego graph and anonymize the target node.\"\"\"\n",
    "    seeds = torch.full((num_traces,), int(target_nid), dtype=torch.long)\n",
    "    try:\n",
    "        traces, _ = dgl.sampling.random_walk(\n",
    "            graph,\n",
    "            seeds,\n",
    "            length=walk_length,\n",
    "            restart_prob=restart_prob,\n",
    "        )\n",
    "        visited = traces.reshape(-1)\n",
    "        visited = visited[visited >= 0]\n",
    "        if visited.numel() == 0:\n",
    "            visited = torch.tensor([int(target_nid)], dtype=torch.long)\n",
    "        else:\n",
    "            visited = visited.unique()\n",
    "    except dgl.DGLError:\n",
    "        khop_result = dgl.khop_in_subgraph(graph, torch.tensor([int(target_nid)]), fallback_khop)\n",
    "        if isinstance(khop_result, tuple):\n",
    "            subg_tmp, induced = khop_result\n",
    "            if isinstance(induced, (list, tuple)):\n",
    "                visited = torch.as_tensor(induced[0], dtype=torch.long)\n",
    "            else:\n",
    "                visited = torch.as_tensor(induced, dtype=torch.long)\n",
    "            graph = subg_tmp\n",
    "        else:\n",
    "            graph = khop_result\n",
    "            visited = graph.ndata[dgl.NID].long()\n",
    "    target_tensor = torch.tensor([int(target_nid)], dtype=torch.long)\n",
    "    unique_nodes = torch.unique(torch.cat([visited, target_tensor]))\n",
    "\n",
    "    subg = dgl.node_subgraph(graph, unique_nodes)\n",
    "    parent_nids = subg.ndata[dgl.NID].long()\n",
    "    sub_feats = feature_tensor[parent_nids].clone()\n",
    "    target_mask = (parent_nids == int(target_nid))\n",
    "    target_indices = target_mask.nonzero(as_tuple=False).view(-1)\n",
    "    if target_indices.numel() == 0:\n",
    "        raise RuntimeError(f'Unable to locate target node {target_nid} inside the sampled subgraph.')\n",
    "    target_idx = target_indices[0].item()\n",
    "    target_feat = sub_feats[target_idx].clone()\n",
    "    sub_feats[target_idx] = 0.0\n",
    "    subg = dgl.add_self_loop(subg)\n",
    "    return subg, sub_feats, target_idx, target_feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca02f323",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "07a13f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LocalSubgraphGAE(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = GraphConv(in_feats, hidden_dim, allow_zero_in_degree=True)\n",
    "        self.decoder = GraphConv(hidden_dim, in_feats, allow_zero_in_degree=True)\n",
    "\n",
    "    def forward(self, g, feat):\n",
    "        h = torch.relu(self.encoder(g, feat))\n",
    "        recon = self.decoder(g, h)\n",
    "        return recon, h\n",
    "\n",
    "\n",
    "local_subgraph_gae = LocalSubgraphGAE(local_train_feats.shape[1], local_gae_config['hidden_dim']).to(local_device)\n",
    "local_subgraph_optimizer = torch.optim.Adam(\n",
    "    local_subgraph_gae.parameters(),\n",
    "    lr=local_gae_config['lr'],\n",
    "    weight_decay=local_gae_config['weight_decay'],\n",
    ")\n",
    "target_recon_loss = nn.MSELoss()\n",
    "local_subgraph_history = []\n",
    "local_subgraph_val_history = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e83a3a",
   "metadata": {},
   "source": [
    "### Node Scoring Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ed4e41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score_nodes_with_local_subgraph_gae(\n",
    "    graph,\n",
    "    target_nodes=None,\n",
    "    *,\n",
    "    walk_length=None,\n",
    "    num_traces=None,\n",
    "    restart_prob=None,\n",
    "    model=None,\n",
    "):\n",
    "    if model is None:\n",
    "        model = local_subgraph_gae\n",
    "    if walk_length is None:\n",
    "        walk_length = local_gae_config['rw_length']\n",
    "    if num_traces is None:\n",
    "        num_traces = local_gae_config['rw_trials']\n",
    "    if restart_prob is None:\n",
    "        restart_prob = local_gae_config['rw_restart_prob']\n",
    "\n",
    "    graph_cpu = dgl.add_self_loop(dgl.to_bidirected(graph, copy_ndata=True))\n",
    "    feature_tensor = graph_cpu.ndata['feat'].float()\n",
    "\n",
    "    if target_nodes is None:\n",
    "        target_nodes = torch.arange(graph_cpu.num_nodes())\n",
    "    else:\n",
    "        target_nodes = torch.as_tensor(target_nodes, dtype=torch.long)\n",
    "\n",
    "    model.eval()\n",
    "    l2_scores = torch.zeros(target_nodes.shape[0])\n",
    "    mse_scores = torch.zeros_like(l2_scores)\n",
    "    with torch.no_grad():\n",
    "        for idx, node_id in enumerate(target_nodes.tolist()):\n",
    "            subg, anonymized_feat, target_idx, target_feat = anonymized_random_walk_subgraph(\n",
    "                graph_cpu,\n",
    "                feature_tensor,\n",
    "                int(node_id),\n",
    "                walk_length=walk_length,\n",
    "                num_traces=num_traces,\n",
    "                restart_prob=restart_prob,\n",
    "            )\n",
    "            recon, _ = model(subg.to(local_device), anonymized_feat.to(local_device))\n",
    "            diff = recon[target_idx] - target_feat.to(local_device)\n",
    "            l2_scores[idx] = torch.norm(diff, p=2).item()\n",
    "            mse_scores[idx] = torch.mean(diff.pow(2)).item()\n",
    "    return {\n",
    "        'node_ids': target_nodes,\n",
    "        'l2_recon_error': l2_scores,\n",
    "        'per_node_mse': mse_scores,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c29231",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "54071da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training local-subgraph anonymized GAE (random-walk views)...\n",
      "[Local-GAE] Epoch 001 | Targets 4000 | MSE 0.8302\n",
      "[Local-GAE] Epoch 002 | Targets 4000 | MSE 0.6930\n",
      "[Local-GAE] Epoch 003 | Targets 4000 | MSE 0.9344\n",
      "[Local-GAE] Epoch 004 | Targets 4000 | MSE 0.7114\n",
      "[Local-GAE] Epoch 005 | Targets 4000 | MSE 0.6895\n",
      "    Validation (1024 nodes) MSE: 0.7323\n",
      "[Local-GAE] Epoch 006 | Targets 4000 | MSE 0.7172\n",
      "[Local-GAE] Epoch 007 | Targets 4000 | MSE 0.7084\n",
      "[Local-GAE] Epoch 008 | Targets 4000 | MSE 0.6718\n",
      "[Local-GAE] Epoch 009 | Targets 4000 | MSE 0.7491\n",
      "[Local-GAE] Epoch 010 | Targets 4000 | MSE 0.7281\n",
      "    Validation (1024 nodes) MSE: 0.7404\n",
      "[Local-GAE] Epoch 011 | Targets 4000 | MSE 0.6645\n",
      "[Local-GAE] Epoch 012 | Targets 4000 | MSE 0.7693\n",
      "[Local-GAE] Epoch 013 | Targets 4000 | MSE 0.7336\n",
      "[Local-GAE] Epoch 014 | Targets 4000 | MSE 0.6663\n",
      "[Local-GAE] Epoch 015 | Targets 4000 | MSE 0.7629\n",
      "    Validation (1024 nodes) MSE: 0.7225\n",
      "[Local-GAE] Epoch 016 | Targets 4000 | MSE 0.6901\n",
      "[Local-GAE] Epoch 017 | Targets 4000 | MSE 0.6943\n",
      "[Local-GAE] Epoch 018 | Targets 4000 | MSE 0.7809\n",
      "[Local-GAE] Epoch 019 | Targets 4000 | MSE 0.8369\n",
      "[Local-GAE] Epoch 020 | Targets 4000 | MSE 0.8524\n",
      "    Validation (1024 nodes) MSE: 0.7374\n",
      "Stored `local_subgraph_gae`, `local_subgraph_history`, and `score_nodes_with_local_subgraph_gae` for downstream evaluation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Training local-subgraph anonymized GAE (random-walk views)...')\n",
    "all_train_nodes = torch.arange(local_num_nodes)\n",
    "for epoch in range(1, local_gae_config['epochs'] + 1):\n",
    "    epoch_nodes = all_train_nodes[torch.randperm(local_num_nodes)[:samples_per_epoch]]\n",
    "    running_loss = 0.0\n",
    "    local_subgraph_gae.train()\n",
    "    for node_id in epoch_nodes.tolist():\n",
    "        subg, anonymized_feat, target_idx, target_feat = anonymized_random_walk_subgraph(\n",
    "            base_graph_for_local_gae,\n",
    "            local_train_feats,\n",
    "            int(node_id),\n",
    "            walk_length=local_gae_config['rw_length'],\n",
    "            num_traces=local_gae_config['rw_trials'],\n",
    "            restart_prob=local_gae_config['rw_restart_prob'],\n",
    "        )\n",
    "        subg = subg.to(local_device)\n",
    "        anonymized_feat = anonymized_feat.to(local_device)\n",
    "        target_feat = target_feat.to(local_device)\n",
    "\n",
    "        recon, _ = local_subgraph_gae(subg, anonymized_feat)\n",
    "        loss = target_recon_loss(recon[target_idx], target_feat)\n",
    "\n",
    "        local_subgraph_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        local_subgraph_optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(epoch_nodes)\n",
    "    local_subgraph_history.append(avg_loss)\n",
    "    print(f\"[Local-GAE] Epoch {epoch:03d} | Targets {len(epoch_nodes)} | MSE {avg_loss:.4f}\")\n",
    "\n",
    "    if validation_nodes.numel() > 0 and (epoch % validation_interval == 0):\n",
    "        val_scores = score_nodes_with_local_subgraph_gae(\n",
    "            base_graph_for_local_gae,\n",
    "            target_nodes=validation_nodes,\n",
    "            model=local_subgraph_gae,\n",
    "            walk_length=local_gae_config['rw_length'],\n",
    "            num_traces=local_gae_config['rw_trials'],\n",
    "            restart_prob=local_gae_config['rw_restart_prob'],\n",
    "        )\n",
    "        val_mse = val_scores['per_node_mse'].mean().item()\n",
    "        local_subgraph_val_history.append((epoch, val_mse))\n",
    "        print(f\"    Validation ({validation_nodes.numel()} nodes) MSE: {val_mse:.4f}\")\n",
    "\n",
    "print('Stored `local_subgraph_gae`, `local_subgraph_history`, and `score_nodes_with_local_subgraph_gae` for downstream evaluation.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606c5c1",
   "metadata": {},
   "source": [
    "### Local GAE Embeddings + Raw Features + GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "58e09f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.7805, Train F1 0.2749, Test Loss 0.7512, Test F1 0.1666\n",
      "Epoch 050: Loss 0.2076, Train F1 0.7472, Test Loss 0.2943, Test F1 0.4182\n",
      "Epoch 100: Loss 0.1713, Train F1 0.7946, Test Loss 0.2684, Test F1 0.5194\n",
      "Epoch 150: Loss 0.1510, Train F1 0.8204, Test Loss 0.2646, Test F1 0.5582\n",
      "Epoch 200: Loss 0.1358, Train F1 0.8402, Test Loss 0.2635, Test F1 0.5692\n",
      "Epoch 250: Loss 0.1234, Train F1 0.8577, Test Loss 0.2653, Test F1 0.5746\n",
      "Epoch 300: Loss 0.1128, Train F1 0.8702, Test Loss 0.2710, Test F1 0.5811\n",
      "Epoch 350: Loss 0.1039, Train F1 0.8811, Test Loss 0.2805, Test F1 0.5848\n",
      "Epoch 400: Loss 0.0963, Train F1 0.8896, Test Loss 0.2917, Test F1 0.5872\n",
      "Epoch 450: Loss 0.0898, Train F1 0.8970, Test Loss 0.3005, Test F1 0.5918\n",
      "Epoch 500: Loss 0.0844, Train F1 0.9044, Test Loss 0.3066, Test F1 0.5938\n",
      "Epoch 550: Loss 0.0796, Train F1 0.9114, Test Loss 0.3115, Test F1 0.5971\n",
      "Epoch 600: Loss 0.0755, Train F1 0.9151, Test Loss 0.3156, Test F1 0.5996\n",
      "Local GAE Embeddings + Raw Features Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.86      0.46      0.60      1083\n",
      "       licit       0.96      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.91      0.73      0.79     16670\n",
      "weighted avg       0.96      0.96      0.95     16670\n",
      "\n",
      "Local GAE Embeddings + Raw Features Best Classification Report on Labeled Test Graph at epoch 600:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.86      0.46      0.60      1083\n",
      "       licit       0.96      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.91      0.73      0.79     16670\n",
      "weighted avg       0.96      0.96      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/LocalGAE_gcn_concat/history.json\n",
      " - Best state: checkpoints/LocalGAE_gcn_concat/best_model_state.pt\n",
      " - Latest state: checkpoints/LocalGAE_gcn_concat/latest_model_state.pt\n",
      "[Local GAE Embeddings + Raw Features] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.1759 | P=0.8924 | R=0.7747 | F1=0.8294\n",
      "  t=36 | nodes= 1708 | Loss=0.0864 | P=0.6129 | R=0.5758 | F1=0.5938\n",
      "  t=37 | nodes=  498 | Loss=0.2367 | P=0.8095 | R=0.4250 | F1=0.5574\n",
      "  t=38 | nodes=  756 | Loss=0.2629 | P=0.9368 | R=0.8018 | F1=0.8641\n",
      "  t=39 | nodes= 1183 | Loss=0.2295 | P=0.9000 | R=0.5556 | F1=0.6870\n",
      "  t=40 | nodes= 1211 | Loss=0.5884 | P=0.9773 | R=0.3839 | F1=0.5513\n",
      "  t=41 | nodes= 1132 | Loss=0.7121 | P=0.9710 | R=0.5776 | F1=0.7243\n",
      "  t=42 | nodes= 2154 | Loss=0.4605 | P=0.9384 | R=0.5732 | F1=0.7117\n",
      "  t=43 | nodes= 1370 | Loss=0.3210 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.2960 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=45 | nodes= 1221 | Loss=0.0705 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0467 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=47 | nodes=  846 | Loss=0.3406 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.8868 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.6202 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.9088 | R=0.6105 | F1=0.7304\n",
      "  Aggregate t>=43 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "if 'local_subgraph_gae' not in globals():\n",
    "    raise RuntimeError('Train the local-subgraph GAE cells before running this step.')\n",
    "\n",
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "local_subgraph_gae = local_subgraph_gae.to(local_device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_graph = base_graph_for_local_gae.to(local_device)\n",
    "    base_feats = base_graph.ndata['feat'].to(local_device)\n",
    "    _, local_train_hidden = local_subgraph_gae(base_graph, base_feats)\n",
    "\n",
    "    test_graph_local = dgl.add_self_loop(dgl.to_bidirected(test_all_graph, copy_ndata=True))\n",
    "    test_graph_local = test_graph_local.to(local_device)\n",
    "    test_graph_local.ndata['feat'] = test_graph_local.ndata['feat'].float()\n",
    "    _, local_test_hidden = local_subgraph_gae(test_graph_local, test_graph_local.ndata['feat'])\n",
    "\n",
    "local_train_hidden = local_train_hidden.to(device)\n",
    "local_test_hidden = local_test_hidden.to(device)\n",
    "\n",
    "local_aug_train = torch.cat([train_feature_all, local_train_hidden], dim=1)\n",
    "local_aug_test = torch.cat([test_features_all, local_test_hidden], dim=1)\n",
    "\n",
    "local_concat_gcn = GCN(local_aug_train.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(local_concat_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_local_concat = train_model(\n",
    "    local_concat_gcn,\n",
    "    'Local GAE Embeddings + Raw Features',\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    local_aug_train,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    local_aug_test,\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=600,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path='checkpoints/LocalGAE/gcn_concat/best_model_state.pt'\n",
    ")\n",
    "\n",
    "save_history(history_local_concat, sub_dir='LocalGAE_gcn_concat')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_local_concat_features(g_eval, feats):\n",
    "    g_local = g_eval.to(local_device)\n",
    "    feats_local = g_local.ndata['feat'].float()\n",
    "    with torch.no_grad():\n",
    "        _, hidden = local_subgraph_gae(g_local, feats_local)\n",
    "    hidden = hidden.to(device)\n",
    "    feats = feats.to(device)\n",
    "    return torch.cat([feats, hidden], dim=1)\n",
    "\n",
    "\n",
    "local_concat_timestep_metrics = report_timestep_performance(\n",
    "    \"Local GAE Embeddings + Raw Features\",\n",
    "    local_concat_gcn,\n",
    "    feature_builder=build_local_concat_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b85ec87",
   "metadata": {},
   "source": [
    "### Local GAE Node Score + Raw Features + GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "19268db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.6842, Train F1 0.1954, Test Loss 0.9821, Test F1 0.1193\n",
      "Epoch 050: Loss 0.2252, Train F1 0.7286, Test Loss 0.3711, Test F1 0.3175\n",
      "Epoch 100: Loss 0.1839, Train F1 0.7871, Test Loss 0.3033, Test F1 0.4139\n",
      "Epoch 150: Loss 0.1621, Train F1 0.8102, Test Loss 0.2842, Test F1 0.4532\n",
      "Epoch 200: Loss 0.1481, Train F1 0.8255, Test Loss 0.2823, Test F1 0.4722\n",
      "Epoch 250: Loss 0.1374, Train F1 0.8398, Test Loss 0.2856, Test F1 0.4892\n",
      "Epoch 300: Loss 0.1286, Train F1 0.8482, Test Loss 0.2894, Test F1 0.5078\n",
      "Epoch 350: Loss 0.1214, Train F1 0.8591, Test Loss 0.2926, Test F1 0.5298\n",
      "Epoch 400: Loss 0.1150, Train F1 0.8665, Test Loss 0.2972, Test F1 0.5494\n",
      "Epoch 450: Loss 0.1095, Train F1 0.8757, Test Loss 0.3015, Test F1 0.5622\n",
      "Epoch 500: Loss 0.1045, Train F1 0.8818, Test Loss 0.3050, Test F1 0.5667\n",
      "Epoch 550: Loss 0.1000, Train F1 0.8881, Test Loss 0.3074, Test F1 0.5664\n",
      "Epoch 600: Loss 0.0958, Train F1 0.8922, Test Loss 0.3092, Test F1 0.5686\n",
      "Local GAE Node Score + Raw Features Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.69      0.48      0.57      1083\n",
      "       licit       0.96      0.98      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.83      0.73      0.77     16670\n",
      "weighted avg       0.95      0.95      0.95     16670\n",
      "\n",
      "Local GAE Node Score + Raw Features Best Classification Report on Labeled Test Graph at epoch 600:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.69      0.48      0.57      1083\n",
      "       licit       0.96      0.98      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.83      0.73      0.77     16670\n",
      "weighted avg       0.95      0.95      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/LocalGAE_gcn_scores/history.json\n",
      " - Best state: checkpoints/LocalGAE_gcn_scores/best_model_state.pt\n",
      " - Latest state: checkpoints/LocalGAE_gcn_scores/latest_model_state.pt\n",
      "[Local GAE Node Score + Raw Features] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.1613 | P=0.9042 | R=0.8297 | F1=0.8653\n",
      "  t=36 | nodes= 1708 | Loss=0.1012 | P=0.5806 | R=0.5455 | F1=0.5625\n",
      "  t=37 | nodes=  498 | Loss=0.2495 | P=0.7917 | R=0.4750 | F1=0.5937\n",
      "  t=38 | nodes=  756 | Loss=0.2594 | P=0.8000 | R=0.8288 | F1=0.8142\n",
      "  t=39 | nodes= 1183 | Loss=0.1950 | P=0.7500 | R=0.7407 | F1=0.7453\n",
      "  t=40 | nodes= 1211 | Loss=0.4575 | P=0.9348 | R=0.3839 | F1=0.5443\n",
      "  t=41 | nodes= 1132 | Loss=0.6335 | P=0.8351 | R=0.6983 | F1=0.7606\n",
      "  t=42 | nodes= 2154 | Loss=0.3642 | P=0.9006 | R=0.6820 | F1=0.7762\n",
      "  t=43 | nodes= 1370 | Loss=0.3265 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.2825 | P=0.0833 | R=0.0417 | F1=0.0556\n",
      "  t=45 | nodes= 1221 | Loss=0.0712 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0421 | P=0.2000 | R=0.5000 | F1=0.2857\n",
      "  t=47 | nodes=  846 | Loss=0.3637 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.8891 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.4784 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.8462 | R=0.6860 | F1=0.7577\n",
      "  Aggregate t>=43 | P=0.0136 | R=0.0118 | F1=0.0127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "\n",
    "if 'local_subgraph_gae' not in globals() or 'score_nodes_with_local_subgraph_gae' not in globals():\n",
    "    raise RuntimeError('Train the local-subgraph GAE cells before running this step.')\n",
    "\n",
    "train_score_dict = score_nodes_with_local_subgraph_gae(\n",
    "    train_all_graph,\n",
    "    model=local_subgraph_gae,\n",
    "    walk_length=local_gae_config['rw_length'],\n",
    "    num_traces=local_gae_config['rw_trials'],\n",
    "    restart_prob=local_gae_config['rw_restart_prob'],\n",
    ")\n",
    "\n",
    "test_score_dict = score_nodes_with_local_subgraph_gae(\n",
    "    test_all_graph,\n",
    "    model=local_subgraph_gae,\n",
    "    walk_length=local_gae_config['rw_length'],\n",
    "    num_traces=local_gae_config['rw_trials'],\n",
    "    restart_prob=local_gae_config['rw_restart_prob'],\n",
    ")\n",
    "\n",
    "train_scores = train_score_dict['per_node_mse'].view(-1, 1)\n",
    "test_scores = test_score_dict['per_node_mse'].view(-1, 1)\n",
    "\n",
    "local_score_mean = train_scores.mean()\n",
    "local_score_std = train_scores.std().clamp_min(1e-6)\n",
    "train_scores = (train_scores - local_score_mean) / local_score_std\n",
    "test_scores = (test_scores - local_score_mean) / local_score_std\n",
    "\n",
    "local_score_train = torch.cat([train_feature_all.cpu(), train_scores], dim=1).to(device)\n",
    "local_score_test = torch.cat([test_features_all.cpu(), test_scores], dim=1).to(device)\n",
    "\n",
    "local_score_gcn = GCN(local_score_train.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(local_score_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_local_score = train_model(\n",
    "    local_score_gcn,\n",
    "    \"Local GAE Node Score + Raw Features\",\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_graph_all,\n",
    "    local_score_train,\n",
    "    train_labels_all,\n",
    "    train_mask_all,\n",
    "    test_graph_all,\n",
    "    local_score_test,\n",
    "    test_labels_all,\n",
    "    test_mask_all,\n",
    "    num_epochs=600,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=20,\n",
    "    checkpoint_path='checkpoints/LocalGAE/gcn_scores/best_model_state.pt'\n",
    ")\n",
    "\n",
    "save_history(history_local_score, sub_dir='LocalGAE_gcn_scores')\n",
    "\n",
    "\n",
    "def build_local_score_features(g_eval, feats):\n",
    "    graph_cpu = g_eval.to('cpu')\n",
    "    score_dict = score_nodes_with_local_subgraph_gae(\n",
    "        graph_cpu,\n",
    "        model=local_subgraph_gae,\n",
    "        walk_length=local_gae_config['rw_length'],\n",
    "        num_traces=local_gae_config['rw_trials'],\n",
    "        restart_prob=local_gae_config['rw_restart_prob'],\n",
    "    )\n",
    "    scores = score_dict['per_node_mse'].view(-1, 1)\n",
    "    scores = (scores - local_score_mean) / local_score_std\n",
    "    scores = scores.to(device)\n",
    "    feats = feats.to(device)\n",
    "    return torch.cat([feats, scores], dim=1)\n",
    "\n",
    "\n",
    "local_score_timestep_metrics = report_timestep_performance(\n",
    "    \"Local GAE Node Score + Raw Features\",\n",
    "    local_score_gcn,\n",
    "    feature_builder=build_local_score_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d8622",
   "metadata": {},
   "source": [
    "## CoLA + GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b735803",
   "metadata": {},
   "source": [
    "### Pretraining Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f7e0e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.sampling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "if 'base_graph_for_local_gae' not in globals() or 'anonymized_random_walk_subgraph' not in globals():\n",
    "    raise RuntimeError('Run the local-subgraph GAE cells first to set up the sampling utilities.')\n",
    "\n",
    "cola_config = {\n",
    "    'hidden_dim': 128,\n",
    "    'epochs': 30,\n",
    "    'samples_per_epoch': 4000,\n",
    "    'rw_length': local_gae_config['rw_length'],\n",
    "    'rw_trials': local_gae_config['rw_trials'],\n",
    "    'rw_restart_prob': local_gae_config['rw_restart_prob'],\n",
    "    'lr': 5e-4,\n",
    "    'weight_decay': 0.0,\n",
    "    'neg_trials': 1,\n",
    "    'eval_neg_trials': 2,\n",
    "}\n",
    "\n",
    "cola_graph = base_graph_for_local_gae\n",
    "cola_features = cola_graph.ndata['feat']\n",
    "cola_num_nodes = cola_graph.num_nodes()\n",
    "cola_samples_per_epoch = min(cola_num_nodes, cola_config['samples_per_epoch'])\n",
    "\n",
    "\n",
    "def _group_nodes_by_timestep(timestep_tensor):\n",
    "    sanitized = timestep_tensor.to(torch.long).view(-1).cpu()\n",
    "    groups = {}\n",
    "    for ts in torch.unique(sanitized).tolist():\n",
    "        mask = sanitized == ts\n",
    "        groups[int(ts)] = mask.nonzero(as_tuple=False).view(-1)\n",
    "    return sanitized, groups\n",
    "\n",
    "\n",
    "if 'timestep' not in cola_graph.ndata:\n",
    "    raise KeyError(\n",
    "        \"cola_graph.ndata is missing 'timestep'. Re-run the data loading cell that calls create_dgl_graph so timesteps are attached to each node.\"\n",
    "    )\n",
    "\n",
    "cola_node_timesteps, cola_nodes_by_timestep = _group_nodes_by_timestep(cola_graph.ndata['timestep'])\n",
    "cola_device = local_device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac76bd",
   "metadata": {},
   "source": [
    "### Contrastive Modules and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b1856513",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CoLAEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv = GraphConv(in_dim, hidden_dim, allow_zero_in_degree=True)\n",
    "\n",
    "    def forward(self, g, feat):\n",
    "        return torch.relu(self.conv(g, feat))\n",
    "\n",
    "\n",
    "class BilinearDiscriminator(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.scorer = nn.Bilinear(hidden_dim, hidden_dim, 1)\n",
    "\n",
    "    def forward(self, node_embed, graph_embed):\n",
    "        return self.scorer(node_embed, graph_embed).squeeze(-1)\n",
    "\n",
    "\n",
    "def _graph_readout(node_embeddings):\n",
    "    return node_embeddings.mean(dim=0)\n",
    "\n",
    "\n",
    "def _sample_negative_node(exclude_node):\n",
    "    candidate = None\n",
    "    group = cola_nodes_by_timestep.get(int(cola_node_timesteps[exclude_node].item()))\n",
    "    if group is not None and group.numel() > 0:\n",
    "        rand_idx = torch.randint(0, group.numel(), (1,)).item()\n",
    "        candidate = int(group[rand_idx])\n",
    "        if candidate == exclude_node and group.numel() > 1:\n",
    "            candidate = int(group[(rand_idx + 1) % group.numel()])\n",
    "    if candidate is None:\n",
    "        candidate = int(torch.randint(0, cola_num_nodes, (1,)))\n",
    "        if candidate == exclude_node:\n",
    "            candidate = int((candidate + 1) % cola_num_nodes)\n",
    "    return candidate\n",
    "\n",
    "\n",
    "cola_encoder = CoLAEncoder(cola_features.shape[1], cola_config['hidden_dim']).to(cola_device)\n",
    "cola_discriminator = BilinearDiscriminator(cola_config['hidden_dim']).to(cola_device)\n",
    "cola_optimizer = torch.optim.Adam(\n",
    "    list(cola_encoder.parameters()) + list(cola_discriminator.parameters()),\n",
    "    lr=cola_config['lr'],\n",
    "    weight_decay=cola_config['weight_decay'],\n",
    ")\n",
    "cola_bce = nn.BCEWithLogitsLoss()\n",
    "cola_train_history = []\n",
    "\n",
    "\n",
    "def cola_forward_step(node_id):\n",
    "    subg_pos, feat_pos, idx_pos, _ = anonymized_random_walk_subgraph(\n",
    "        cola_graph,\n",
    "        cola_features,\n",
    "        node_id,\n",
    "        walk_length=cola_config['rw_length'],\n",
    "        num_traces=cola_config['rw_trials'],\n",
    "        restart_prob=cola_config['rw_restart_prob'],\n",
    "    )\n",
    "    subg_pos = subg_pos.to(cola_device)\n",
    "    feat_pos = feat_pos.to(cola_device)\n",
    "    embeds_pos = cola_encoder(subg_pos, feat_pos)\n",
    "    target_embed = embeds_pos[idx_pos]\n",
    "    graph_embed_pos = _graph_readout(embeds_pos)\n",
    "\n",
    "    neg_scores = []\n",
    "    neg_labels = []\n",
    "    for _ in range(cola_config['neg_trials']):\n",
    "        neg_node = _sample_negative_node(node_id)\n",
    "        subg_neg, feat_neg, _, _ = anonymized_random_walk_subgraph(\n",
    "            cola_graph,\n",
    "            cola_features,\n",
    "            neg_node,\n",
    "            walk_length=cola_config['rw_length'],\n",
    "            num_traces=cola_config['rw_trials'],\n",
    "            restart_prob=cola_config['rw_restart_prob'],\n",
    "        )\n",
    "        subg_neg = subg_neg.to(cola_device)\n",
    "        feat_neg = feat_neg.to(cola_device)\n",
    "        embeds_neg = cola_encoder(subg_neg, feat_neg)\n",
    "        graph_embed_neg = _graph_readout(embeds_neg)\n",
    "        neg_scores.append(cola_discriminator(target_embed, graph_embed_neg).unsqueeze(0))\n",
    "        neg_labels.append(torch.zeros(1, device=cola_device))\n",
    "\n",
    "    pos_score = cola_discriminator(target_embed, graph_embed_pos)\n",
    "    pos_label = torch.ones(1, device=cola_device)\n",
    "    neg_scores = torch.cat(neg_scores) if neg_scores else torch.tensor([], device=cola_device)\n",
    "    neg_labels = torch.cat(neg_labels) if neg_labels else torch.tensor([], device=cola_device)\n",
    "    return pos_score, pos_label, neg_scores, neg_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691a1f3",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4b536d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CoLA contrastive model...\n",
      "[CoLA] Epoch 001 | Nodes 4000 | Loss 1.0934\n",
      "[CoLA] Epoch 002 | Nodes 4000 | Loss 0.8198\n",
      "[CoLA] Epoch 003 | Nodes 4000 | Loss 0.6772\n",
      "[CoLA] Epoch 004 | Nodes 4000 | Loss 0.6095\n",
      "[CoLA] Epoch 005 | Nodes 4000 | Loss 0.7126\n",
      "[CoLA] Epoch 006 | Nodes 4000 | Loss 0.5519\n",
      "[CoLA] Epoch 007 | Nodes 4000 | Loss 0.5419\n",
      "[CoLA] Epoch 008 | Nodes 4000 | Loss 0.4986\n",
      "[CoLA] Epoch 009 | Nodes 4000 | Loss 0.5730\n",
      "[CoLA] Epoch 010 | Nodes 4000 | Loss 0.5166\n",
      "[CoLA] Epoch 011 | Nodes 4000 | Loss 0.5217\n",
      "[CoLA] Epoch 012 | Nodes 4000 | Loss 0.5966\n",
      "[CoLA] Epoch 013 | Nodes 4000 | Loss 0.5128\n",
      "[CoLA] Epoch 014 | Nodes 4000 | Loss 0.5035\n",
      "[CoLA] Epoch 015 | Nodes 4000 | Loss 0.5033\n",
      "[CoLA] Epoch 016 | Nodes 4000 | Loss 0.5220\n",
      "[CoLA] Epoch 017 | Nodes 4000 | Loss 0.4844\n",
      "[CoLA] Epoch 018 | Nodes 4000 | Loss 0.5948\n",
      "[CoLA] Epoch 019 | Nodes 4000 | Loss 0.4952\n",
      "[CoLA] Epoch 020 | Nodes 4000 | Loss 0.4948\n",
      "[CoLA] Epoch 021 | Nodes 4000 | Loss 1.3308\n",
      "[CoLA] Epoch 022 | Nodes 4000 | Loss 0.5418\n",
      "[CoLA] Epoch 023 | Nodes 4000 | Loss 0.5253\n",
      "[CoLA] Epoch 024 | Nodes 4000 | Loss 0.4668\n",
      "[CoLA] Epoch 025 | Nodes 4000 | Loss 0.4841\n",
      "[CoLA] Epoch 026 | Nodes 4000 | Loss 0.4603\n",
      "[CoLA] Epoch 027 | Nodes 4000 | Loss 0.4778\n",
      "[CoLA] Epoch 028 | Nodes 4000 | Loss 0.4865\n",
      "[CoLA] Epoch 029 | Nodes 4000 | Loss 0.5244\n",
      "[CoLA] Epoch 030 | Nodes 4000 | Loss 0.4655\n",
      "Stored `cola_encoder`, `cola_discriminator`, and `cola_train_history` for downstream scoring.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Training CoLA contrastive model...')\n",
    "all_nodes = torch.arange(cola_num_nodes)\n",
    "for epoch in range(1, cola_config['epochs'] + 1):\n",
    "    epoch_nodes = all_nodes[torch.randperm(cola_num_nodes)[:cola_samples_per_epoch]]\n",
    "    running_loss = 0.0\n",
    "    for node_id in epoch_nodes.tolist():\n",
    "        cola_encoder.train()\n",
    "        cola_discriminator.train()\n",
    "        pos_score, pos_label, neg_scores, neg_labels = cola_forward_step(int(node_id))\n",
    "\n",
    "        loss = cola_bce(pos_score.unsqueeze(0), pos_label)\n",
    "        if neg_scores.numel() > 0:\n",
    "            loss = loss + cola_bce(neg_scores, neg_labels)\n",
    "\n",
    "        cola_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        cola_optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(epoch_nodes)\n",
    "    cola_train_history.append(avg_loss)\n",
    "    print(f\"[CoLA] Epoch {epoch:03d} | Nodes {len(epoch_nodes)} | Loss {avg_loss:.4f}\")\n",
    "\n",
    "print('Stored `cola_encoder`, `cola_discriminator`, and `cola_train_history` for downstream scoring.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd09b64",
   "metadata": {},
   "source": [
    "### CoLA Embeddings + Raw Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "42cc8eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 1.1419, Train F1 0.2184, Test Loss 0.8135, Test F1 0.1335\n",
      "Epoch 050: Loss 0.2274, Train F1 0.7221, Test Loss 0.3582, Test F1 0.3316\n",
      "Epoch 100: Loss 0.1875, Train F1 0.7741, Test Loss 0.3155, Test F1 0.3901\n",
      "Epoch 150: Loss 0.1639, Train F1 0.8060, Test Loss 0.3022, Test F1 0.4100\n",
      "Epoch 200: Loss 0.1466, Train F1 0.8292, Test Loss 0.2979, Test F1 0.4183\n",
      "Epoch 250: Loss 0.1321, Train F1 0.8484, Test Loss 0.2958, Test F1 0.4563\n",
      "Epoch 300: Loss 0.1195, Train F1 0.8635, Test Loss 0.2936, Test F1 0.4977\n",
      "CoLA Embeddings + Raw Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.67      0.40      0.50      1083\n",
      "       licit       0.96      0.99      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.82      0.69      0.74     16670\n",
      "weighted avg       0.94      0.95      0.94     16670\n",
      "\n",
      "CoLA Embeddings + Raw Best Classification Report on Labeled Test Graph at epoch 300:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.67      0.40      0.50      1083\n",
      "       licit       0.96      0.99      0.97     15587\n",
      "\n",
      "    accuracy                           0.95     16670\n",
      "   macro avg       0.82      0.69      0.74     16670\n",
      "weighted avg       0.94      0.95      0.94     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/CoLA_gcn_concat/history.json\n",
      " - Best state: checkpoints/CoLA_gcn_concat/best_model_state.pt\n",
      " - Latest state: checkpoints/CoLA_gcn_concat/latest_model_state.pt\n",
      "[CoLA Embeddings + Raw Features] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.2110 | P=0.8659 | R=0.7802 | F1=0.8208\n",
      "  t=36 | nodes= 1708 | Loss=0.1419 | P=0.2857 | R=0.4242 | F1=0.3415\n",
      "  t=37 | nodes=  498 | Loss=0.2815 | P=0.6923 | R=0.4500 | F1=0.5455\n",
      "  t=38 | nodes=  756 | Loss=0.3848 | P=0.6863 | R=0.6306 | F1=0.6573\n",
      "  t=39 | nodes= 1183 | Loss=0.2752 | P=0.7111 | R=0.3951 | F1=0.5079\n",
      "  t=40 | nodes= 1211 | Loss=0.5620 | P=0.8235 | R=0.2500 | F1=0.3836\n",
      "  t=41 | nodes= 1132 | Loss=0.5618 | P=0.6667 | R=0.2759 | F1=0.3902\n",
      "  t=42 | nodes= 2154 | Loss=0.4114 | P=0.8444 | R=0.4770 | F1=0.6096\n",
      "  t=43 | nodes= 1370 | Loss=0.2498 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.2275 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=45 | nodes= 1221 | Loss=0.1012 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0432 | P=0.1250 | R=0.5000 | F1=0.2000\n",
      "  t=47 | nodes=  846 | Loss=0.2924 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.6332 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.5350 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.7463 | R=0.4923 | F1=0.5933\n",
      "  Aggregate t>=43 | P=0.0141 | R=0.0059 | F1=0.0083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "if 'cola_encoder' not in globals():\n",
    "    raise RuntimeError('Train the CoLA encoder cell before running this step.')\n",
    "\n",
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "\n",
    "def _prep_graph(base_graph):\n",
    "    return dgl.add_self_loop(dgl.to_bidirected(base_graph, copy_ndata=True)).to(device)\n",
    "\n",
    "cola_train_graph = _prep_graph(train_all_graph)\n",
    "cola_test_graph = _prep_graph(test_all_graph)\n",
    "train_labels = cola_train_graph.ndata['label']\n",
    "train_mask = (train_labels >= 0)\n",
    "test_labels = cola_test_graph.ndata['label']\n",
    "test_mask = (test_labels >= 0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_train_feats = cola_train_graph.ndata['feat'].float()\n",
    "    base_test_feats = cola_test_graph.ndata['feat'].float()\n",
    "    cola_train_embeddings = cola_encoder(cola_train_graph, base_train_feats)\n",
    "    cola_test_embeddings = cola_encoder(cola_test_graph, base_test_feats)\n",
    "\n",
    "aug_train_feats = torch.cat([base_train_feats, cola_train_embeddings], dim=1)\n",
    "aug_test_feats = torch.cat([base_test_feats, cola_test_embeddings], dim=1)\n",
    "\n",
    "cola_gcn = GCN(aug_train_feats.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(cola_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_cola_gcn = train_model(\n",
    "    cola_gcn,\n",
    "    \"CoLA Embeddings + Raw\",\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    cola_train_graph,\n",
    "    aug_train_feats,\n",
    "    train_labels.to(device),\n",
    "    train_mask,\n",
    "    cola_test_graph,\n",
    "    aug_test_feats,\n",
    "    test_labels.to(device),\n",
    "    test_mask,\n",
    "    num_epochs=300,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=10,\n",
    "    checkpoint_path=\"checkpoints/CoLA/gcn_concat/best_model_state.pt\"\n",
    ")\n",
    "\n",
    "save_history(history_cola_gcn, sub_dir=\"CoLA_gcn_concat\")\n",
    "\n",
    "\n",
    "def build_cola_concat_features(g_eval, feats):\n",
    "    g_for_encoder = g_eval.to(cola_device)\n",
    "    feats_for_encoder = g_for_encoder.ndata['feat'].float()\n",
    "    with torch.no_grad():\n",
    "        embeddings = cola_encoder(g_for_encoder, feats_for_encoder)\n",
    "    embeddings = embeddings.to(device)\n",
    "    feats = feats.to(device)\n",
    "    return torch.cat([feats, embeddings], dim=1)\n",
    "\n",
    "\n",
    "cola_gcn_timestep_metrics = report_timestep_performance(\n",
    "    \"CoLA Embeddings + Raw Features\",\n",
    "    cola_gcn,\n",
    "    feature_builder=build_cola_concat_features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a745f14b",
   "metadata": {},
   "source": [
    "### CoLA Node Score + Raw Features + GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ab47ce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss 0.6947, Train F1 0.2167, Test Loss 0.6692, Test F1 0.0666\n",
      "Epoch 050: Loss 0.2198, Train F1 0.7306, Test Loss 0.3207, Test F1 0.4066\n",
      "Epoch 100: Loss 0.1794, Train F1 0.7856, Test Loss 0.2701, Test F1 0.5203\n",
      "Epoch 150: Loss 0.1579, Train F1 0.8090, Test Loss 0.2549, Test F1 0.5770\n",
      "Epoch 200: Loss 0.1436, Train F1 0.8297, Test Loss 0.2587, Test F1 0.5926\n",
      "Epoch 250: Loss 0.1326, Train F1 0.8425, Test Loss 0.2715, Test F1 0.5941\n",
      "Epoch 300: Loss 0.1236, Train F1 0.8557, Test Loss 0.2910, Test F1 0.5714\n",
      "CoLA Node Score + Raw Features Last Classification Report on Labeled Test Graph:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.81      0.44      0.57      1083\n",
      "       licit       0.96      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.89      0.72      0.77     16670\n",
      "weighted avg       0.95      0.96      0.95     16670\n",
      "\n",
      "CoLA Node Score + Raw Features Best Classification Report on Labeled Test Graph at epoch 250:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     illicit       0.77      0.48      0.59      1083\n",
      "       licit       0.97      0.99      0.98     15587\n",
      "\n",
      "    accuracy                           0.96     16670\n",
      "   macro avg       0.87      0.74      0.79     16670\n",
      "weighted avg       0.95      0.96      0.95     16670\n",
      "\n",
      "Saved:\n",
      " - History JSON: checkpoints/CoLA_gcn_scores/history.json\n",
      " - Best state: checkpoints/CoLA_gcn_scores/best_model_state.pt\n",
      " - Latest state: checkpoints/CoLA_gcn_scores/latest_model_state.pt\n",
      "[CoLA Node Score + Raw Features] per-timestep illicit metrics:\n",
      "  t=35 | nodes= 1341 | Loss=0.1861 | P=0.9139 | R=0.7582 | F1=0.8288\n",
      "  t=36 | nodes= 1708 | Loss=0.2527 | P=0.5000 | R=0.2727 | F1=0.3529\n",
      "  t=37 | nodes=  498 | Loss=0.4184 | P=0.9375 | R=0.3750 | F1=0.5357\n",
      "  t=38 | nodes=  756 | Loss=0.4126 | P=0.7778 | R=0.5045 | F1=0.6120\n",
      "  t=39 | nodes= 1183 | Loss=0.3543 | P=0.8519 | R=0.2840 | F1=0.4259\n",
      "  t=40 | nodes= 1211 | Loss=1.4727 | P=0.8333 | R=0.0893 | F1=0.1613\n",
      "  t=41 | nodes= 1132 | Loss=0.5232 | P=0.9863 | R=0.6207 | F1=0.7619\n",
      "  t=42 | nodes= 2154 | Loss=0.4182 | P=0.9187 | R=0.6151 | F1=0.7368\n",
      "  t=43 | nodes= 1370 | Loss=0.2460 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=44 | nodes= 1591 | Loss=0.2576 | P=0.1250 | R=0.0417 | F1=0.0625\n",
      "  t=45 | nodes= 1221 | Loss=0.0839 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=46 | nodes=  712 | Loss=0.0335 | P=1.0000 | R=0.5000 | F1=0.6667\n",
      "  t=47 | nodes=  846 | Loss=0.3715 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=48 | nodes=  471 | Loss=0.7377 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  t=49 | nodes=  476 | Loss=1.2466 | P=0.0000 | R=0.0000 | F1=0.0000\n",
      "  Aggregate t<43 | P=0.8885 | R=0.5142 | F1=0.6514\n",
      "  Aggregate t>=43 | P=0.0909 | R=0.0118 | F1=0.0209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = torch.tensor([0.7, 0.3], dtype=torch.float32, device=device)\n",
    "\n",
    "if 'cola_encoder' not in globals() or 'cola_discriminator' not in globals():\n",
    "    raise RuntimeError('Train the CoLA encoder cell before running this step.')\n",
    "\n",
    "def compute_cola_scores(graph):\n",
    "    graph_proc = dgl.add_self_loop(dgl.to_bidirected(graph, copy_ndata=True)).to(cola_device)\n",
    "    feats = graph_proc.ndata['feat'].float()\n",
    "    with torch.no_grad():\n",
    "        node_embeddings = cola_encoder(graph_proc, feats)\n",
    "        graph_embedding = _graph_readout(node_embeddings)\n",
    "        repeated = graph_embedding.unsqueeze(0).repeat(node_embeddings.shape[0], 1)\n",
    "        logits = cola_discriminator(node_embeddings, repeated).unsqueeze(1)\n",
    "    return logits.cpu()\n",
    "\n",
    "train_scores = compute_cola_scores(train_all_graph)\n",
    "test_scores = compute_cola_scores(test_all_graph)\n",
    "\n",
    "cola_score_mean = train_scores.mean()\n",
    "cola_score_std = train_scores.std().clamp_min(1e-6)\n",
    "train_scores = (train_scores - cola_score_mean) / cola_score_std\n",
    "test_scores = (test_scores - cola_score_mean) / cola_score_std\n",
    "\n",
    "cola_score_train = torch.cat([train_feature_all.cpu(), train_scores], dim=1).to(device)\n",
    "cola_score_test = torch.cat([test_features_all.cpu(), test_scores], dim=1).to(device)\n",
    "\n",
    "cola_score_graph = dgl.add_self_loop(dgl.to_bidirected(train_all_graph, copy_ndata=True)).to(device)\n",
    "cola_score_test_graph = dgl.add_self_loop(dgl.to_bidirected(test_all_graph, copy_ndata=True)).to(device)\n",
    "train_labels = cola_score_graph.ndata['label']\n",
    "train_mask = (train_labels >= 0)\n",
    "test_labels = cola_score_test_graph.ndata['label']\n",
    "test_mask = (test_labels >= 0)\n",
    "\n",
    "cola_score_gcn = GCN(cola_score_train.shape[1], embedding_dim, 2).to(device)\n",
    "optimizer = torch.optim.Adam(cola_score_gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "history_cola_score = train_model(\n",
    "    cola_score_gcn,\n",
    "    \"CoLA Node Score + Raw Features\",\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    cola_score_graph,\n",
    "    cola_score_train,\n",
    "    train_labels.to(device),\n",
    "    train_mask,\n",
    "    cola_score_test_graph,\n",
    "    cola_score_test,\n",
    "    test_labels.to(device),\n",
    "    test_mask,\n",
    "    num_epochs=300,\n",
    "    test_every=50,\n",
    "    early_stopping_patience=10,\n",
    "    checkpoint_path='checkpoints/CoLA/gcn_scores/best_model_state.pt'\n",
    ")\n",
    "\n",
    "save_history(history_cola_score, sub_dir='CoLA_gcn_scores')\n",
    "\n",
    "\n",
    "def build_cola_score_features(g_eval, feats):\n",
    "    scores = compute_cola_scores(g_eval)\n",
    "    scores = (scores - cola_score_mean) / cola_score_std\n",
    "    scores = scores.to(device)\n",
    "    feats = feats.to(device)\n",
    "    return torch.cat([feats, scores], dim=1)\n",
    "\n",
    "\n",
    "cola_score_timestep_metrics = report_timestep_performance(\n",
    "    \"CoLA Node Score + Raw Features\",\n",
    "    cola_score_gcn,\n",
    "    feature_builder=build_cola_score_features\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
