{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GraphSAGE"
      ],
      "metadata": {
        "id": "L1pXvMBymFpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "4D5S0Bzbmsce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us-TC5nwqGDm",
        "outputId": "0285cb66-56fa-4404-efda-8c3989913bca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 14 15:23:10 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   76C    P0             36W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl -f https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html # Install appropriate verson see - https://www.dgl.ai/pages/start.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzpyMmh8nD-p",
        "outputId": "5c3312ac-3f29-46f7-9377-b8403fb32205"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.12/dist-packages (2.4.0+cu124)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.12/dist-packages (from dgl) (3.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from dgl) (25.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from dgl) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from dgl) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from dgl) (4.67.1)\n",
            "Requirement already satisfied: torch<=2.4.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.4.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (2025.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (3.20.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (75.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<=2.4.0->dgl) (12.6.85)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->dgl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->dgl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->dgl) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.4.0->dgl) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch<=2.4.0->dgl) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gf66s9nemCqE"
      },
      "outputs": [],
      "source": [
        "import torch, traceback, gc\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Aggregators"
      ],
      "metadata": {
        "id": "IeOWI6Y4m4W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MeanAggregator(nn.Module):\n",
        "    def __init__(self, input_dim: int, output_dim: int):\n",
        "      super().__init__()\n",
        "      self.input_dim = input_dim\n",
        "      self.output_dim = output_dim\n",
        "      self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, neighbor_features: torch.Tensor) -> torch.Tensor:\n",
        "      # neighbor_features shape: [batch_size, num_neighbors, input_dim]\n",
        "      # Compute mean along the neighbor dimension\n",
        "      aggregated = torch.mean(neighbor_features, dim=1)  # [batch_size, input_dim]\n",
        "      return self.linear(aggregated)\n",
        "\n",
        "class MaxPoolAggregator(nn.Module): # No difference with min see paper - https://arxiv.org/pdf/1706.02216\n",
        "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = None):\n",
        "      super().__init__()\n",
        "      self.input_dim = input_dim\n",
        "      self.output_dim = output_dim\n",
        "      if hidden_dim is None:\n",
        "          hidden_dim = output_dim\n",
        "\n",
        "      self.mlp = nn.Sequential(\n",
        "          nn.Linear(input_dim, hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_dim, output_dim)\n",
        "      )\n",
        "\n",
        "    def forward(self, neighbor_features: torch.Tensor) -> torch.Tensor:\n",
        "        # neighbor_features shape: [batch_size, num_neighbors, input_dim]\n",
        "        batch_size, num_neighbors, input_dim = neighbor_features.shape\n",
        "\n",
        "        # Reshape to apply MLP to each neighbor\n",
        "        neighbor_features = neighbor_features.view(-1, input_dim)  # [batch_size * num_neighbors, input_dim]\n",
        "        neighbor_features = self.mlp(neighbor_features)  # [batch_size * num_neighbors, output_dim]\n",
        "\n",
        "        # Reshape back and apply max pooling\n",
        "        neighbor_features = neighbor_features.view(batch_size, num_neighbors, -1)\n",
        "        aggregated = torch.max(neighbor_features, dim=1)[0]  # [batch_size, output_dim]\n",
        "\n",
        "        return aggregated\n",
        "\n",
        "class LSTMAggregator(nn.Module):\n",
        "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = None):\n",
        "      super().__init__()\n",
        "      self.input_dim = input_dim\n",
        "      self.output_dim = output_dim\n",
        "      if hidden_dim is None:\n",
        "          hidden_dim = output_dim\n",
        "\n",
        "      self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "      self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, neighbor_features: torch.Tensor) -> torch.Tensor:\n",
        "        # neighbor_features shape: [batch_size, num_neighbors, input_dim]\n",
        "        # LSTM expects input in the same format\n",
        "        _, (h_n, _) = self.lstm(neighbor_features)\n",
        "\n",
        "        # Use the last hidden state\n",
        "        h_n = h_n.squeeze(0)  # [batch_size, hidden_dim]\n",
        "        return self.linear(h_n)\n"
      ],
      "metadata": {
        "id": "LkOHLXsOmrz1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphSAGELayer(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    input_dim: int,\n",
        "    output_dim: int,\n",
        "    aggregator_type: str = 'mean',\n",
        "    num_samples: int = 10,\n",
        "    dropout: float = 0.5,\n",
        "    normalize: bool = True):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.num_samples = num_samples\n",
        "    self.normalize = normalize\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Initialize aggregator\n",
        "    if aggregator_type == 'mean':\n",
        "        self.aggregator = MeanAggregator(input_dim, output_dim)\n",
        "    elif aggregator_type == 'maxpool':\n",
        "        self.aggregator = MaxPoolAggregator(input_dim, output_dim)\n",
        "    elif aggregator_type == 'lstm':\n",
        "        self.aggregator = LSTMAggregator(input_dim, output_dim)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown aggregator type: {aggregator_type}\")\n",
        "\n",
        "    # Self transformation\n",
        "    self.self_linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    # Final transformation (concatenation of self + neighbor)\n",
        "    self.final_linear = nn.Linear(output_dim * 2, output_dim)\n",
        "\n",
        "  def sample_neighbors(self, graph: dgl.DGLGraph, nodes: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "      # Use DGL's sampling functionality\n",
        "      sampled_graph = dgl.sampling.sample_neighbors(\n",
        "          graph, nodes, self.num_samples, replace=True\n",
        "      )\n",
        "\n",
        "      # Get source and destination nodes\n",
        "      src, dst = sampled_graph.edges()\n",
        "      return src, dst\n",
        "\n",
        "  def forward(self, graph: dgl.DGLGraph, features: torch.Tensor, nodes: torch.Tensor) -> torch.Tensor:\n",
        "      # if nodes is None:\n",
        "      #     nodes = torch.arange(graph.number_of_nodes(), device=features.device)\n",
        "\n",
        "      batch_size = len(nodes)\n",
        "      print(f\"Batch Size: {batch_size}\")\n",
        "\n",
        "      # Sample neighbors for each target node\n",
        "      src_nodes, dst_nodes = self.sample_neighbors(graph, nodes)\n",
        "\n",
        "      # Group neighbors by destination node\n",
        "      neighbor_features_list = []\n",
        "\n",
        "      for i, node in enumerate(nodes):\n",
        "          # Find neighbors of current node\n",
        "          neighbor_mask = (dst_nodes == node)\n",
        "          node_neighbors = src_nodes[neighbor_mask]\n",
        "\n",
        "          if len(node_neighbors) == 0:\n",
        "              # If no neighbors, use zero features\n",
        "              neighbor_feats = torch.zeros(self.num_samples, self.input_dim, device=features.device)\n",
        "          else:\n",
        "              # Get neighbor features\n",
        "              neighbor_feats = features[node_neighbors]\n",
        "\n",
        "              # Pad or truncate to max neighbours\n",
        "              # If neighbours < num_samples, pad with features from random existing neighbours\n",
        "              # Else if neighbours > num_samples, we randomly pick X(num_samples) number of neighbour features\n",
        "              if len(neighbor_feats) < self.num_samples:\n",
        "                  # Pad with repetition of existing neighbors\n",
        "                  indices = torch.randint(0, len(neighbor_feats), (self.num_samples - len(neighbor_feats),))\n",
        "                  padding = neighbor_feats[indices]\n",
        "                  neighbor_feats = torch.cat([neighbor_feats, padding], dim=0)\n",
        "              elif len(neighbor_feats) > self.num_samples:\n",
        "                  # Random sampling\n",
        "                  indices = torch.randperm(len(neighbor_feats))[:self.num_samples]\n",
        "                  neighbor_feats = neighbor_feats[indices]\n",
        "\n",
        "          neighbor_features_list.append(neighbor_feats)\n",
        "\n",
        "      # Stack neighbor features\n",
        "      neighbor_features = torch.stack(neighbor_features_list, dim=0)  # [batch_size, num_samples, input_dim]\n",
        "\n",
        "      # Apply dropout\n",
        "      neighbor_features = self.dropout(neighbor_features)\n",
        "\n",
        "      # Aggregate neighbor features\n",
        "      aggregated_neighbors = self.aggregator(neighbor_features)  # [batch_size, output_dim]\n",
        "\n",
        "      # Transform self features\n",
        "      self_features = features[nodes]  # [batch_size, input_dim]\n",
        "      self_features = self.dropout(self_features)\n",
        "      transformed_self = self.self_linear(self_features)  # [batch_size, output_dim]\n",
        "\n",
        "      # Concatenate self and neighbor features\n",
        "      combined = torch.cat([transformed_self, aggregated_neighbors], dim=1)  # [batch_size, 2*output_dim]\n",
        "\n",
        "      # Final transformation\n",
        "      output = self.final_linear(combined)  # [batch_size, output_dim]\n",
        "\n",
        "      # Apply activation\n",
        "      output = F.relu(output)\n",
        "\n",
        "      # L2 normalization\n",
        "      if self.normalize:\n",
        "          output = F.normalize(output, p=2, dim=1)\n",
        "\n",
        "      return output\n",
        "\n",
        "class GraphSAGE(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_dim: int,\n",
        "      hidden_dims: List[int],\n",
        "      output_dim: int,\n",
        "      aggregator_type: str = 'mean',\n",
        "      # num_samples: List[int] = None,\n",
        "      num_samples: int = 2,\n",
        "      dropout: float = 0.5,\n",
        "      normalize: bool = True):\n",
        "    super().__init__()\n",
        "\n",
        "    # if num_samples is None:\n",
        "    #     num_samples = [10] * len(hidden_dims)\n",
        "\n",
        "    # assert len(hidden_dims) == len(num_samples), \"Length mismatch between hidden_dims and num_samples\"\n",
        "\n",
        "    self.layers = nn.ModuleList()\n",
        "\n",
        "    # Concatenate for stacking multiple layers\n",
        "    dims = [input_dim] + hidden_dims + [output_dim]\n",
        "    print(dims)\n",
        "\n",
        "    # Create GraphSAGE layers\n",
        "    for i in range(len(dims) - 1):\n",
        "      layer = GraphSAGELayer(\n",
        "          input_dim=dims[i],\n",
        "          output_dim=dims[i + 1],\n",
        "          aggregator_type=aggregator_type,\n",
        "          # num_samples=num_samples[min(i, len(num_samples) - 1)],\n",
        "          num_samples=num_samples,\n",
        "          dropout=dropout if i < len(dims) - 2 else 0,  # No dropout in last layer\n",
        "          normalize=normalize if i < len(dims) - 2 else False  # No normalization in last layer\n",
        "      )\n",
        "      self.layers.append(layer)\n",
        "\n",
        "  def forward(self, graph: dgl.DGLGraph, features: torch.Tensor, nodes: torch.Tensor) -> torch.Tensor:\n",
        "    h = features\n",
        "    for layer in self.layers:\n",
        "        h_new = layer(graph, h, nodes)\n",
        "\n",
        "        # For next layer, we need to update the full feature matrix\n",
        "        if nodes is not None:\n",
        "            h_full = h.clone()\n",
        "            h_full[nodes] = h_new\n",
        "            h = h_full\n",
        "        else:\n",
        "            h = h_new\n",
        "\n",
        "    return h if nodes is None else h[nodes]\n",
        "\n",
        "class GraphSAGENodeClassifier(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dims: List[int], num_classes: int, **kwargs):\n",
        "      super().__init__()\n",
        "      self.graphsage = GraphSAGE(input_dim, hidden_dims, hidden_dims[-1], **kwargs)\n",
        "      self.classifier = nn.Linear(hidden_dims[-1], num_classes)\n",
        "\n",
        "    def forward(self, graph: dgl.DGLGraph, features: torch.Tensor, nodes: torch.Tensor):\n",
        "      embeddings = self.graphsage(graph, features, nodes)\n",
        "      return self.classifier(embeddings)\n",
        "\n",
        "def train(graph, features, labels, nodes, graph_v, features_v, labels_v, nodes_v, epoch=5):\n",
        "  input_dim = features.shape[1]\n",
        "  num_classes = 2 # Illicit or Licit only\n",
        "\n",
        "  # Create model\n",
        "  model = GraphSAGENodeClassifier(\n",
        "      input_dim=input_dim,\n",
        "      hidden_dims=[input_dim, input_dim],\n",
        "      num_classes=num_classes,\n",
        "      aggregator_type='maxpool', # Define type of aggregator here - mean, maxpool, lstm\n",
        "      num_samples=2 # Define the number of neighbours to sample\n",
        "  )\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    features = features.cuda()\n",
        "    labels = labels.cuda()\n",
        "    nodes = nodes.cuda()\n",
        "    graph = graph.to(\"cuda\")\n",
        "\n",
        "    features_v = features_v.cuda()\n",
        "    labels_v = labels_v.cuda()\n",
        "    nodes_v = nodes_v.cuda()\n",
        "    graph_v = graph_v.to(\"cuda\")\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  for epoch in range(epoch):\n",
        "      # Training\n",
        "      model.train()\n",
        "      print(\"TRAINING\")\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      logits = model(graph, features, nodes)\n",
        "      loss = criterion(logits, labels)\n",
        "\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Validation\n",
        "      model.eval()\n",
        "      print(\"VALIDATING\")\n",
        "      logits = model(graph_v, features_v, nodes_v)\n",
        "      val_loss = criterion(logits, labels_v)\n",
        "\n",
        "      print(f\"Epoch {epoch}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
        "  print(\"Returning model...\")\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "L99ppMfzm83y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation"
      ],
      "metadata": {
        "id": "P6xICMWADdnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub[pandas-datasets]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btTIq8HkDYI7",
        "outputId": "f3d89514-d2da-4b46-a52f-77ed23de2692"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "# LOAD FEATURES\n",
        "file_path = \"elliptic_bitcoin_dataset/elliptic_txs_features.csv\"\n",
        "data = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"ellipticco/elliptic-data-set\",\n",
        "  file_path,\n",
        "  pandas_kwargs={\"header\": None},\n",
        ")\n",
        "# LOAD CLASSES\n",
        "file_path = \"elliptic_bitcoin_dataset/elliptic_txs_classes.csv\"\n",
        "classes = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"ellipticco/elliptic-data-set\",\n",
        "  file_path,\n",
        ")\n",
        "# LOAD EDGELIST\n",
        "file_path = \"elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv\"\n",
        "edgelist = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"ellipticco/elliptic-data-set\",\n",
        "  file_path,\n",
        ")\n",
        "print(data.shape)\n",
        "print(classes.shape)\n",
        "print(edgelist.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRaR-b02DGsL",
        "outputId": "a330350f-ec1b-49a4-ca93-02ec0aa9768a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3839568209.py:7: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  data = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'elliptic-data-set' dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3839568209.py:15: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  classes = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'elliptic-data-set' dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3839568209.py:22: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  edgelist = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'elliptic-data-set' dataset.\n",
            "(203769, 167)\n",
            "(203769, 2)\n",
            "(234355, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Split\n",
        "splits = np.load('elliptic_splits.npz', allow_pickle=True)\n",
        "print(splits)\n",
        "# data = pd.read_csv('elliptic_txs_features.csv', header=None, low_memory=False)\n",
        "# edgelist = pd.read_csv('elliptic_txs_edgelist.csv', header=None, low_memory=False)\n",
        "\n",
        "dataset = []\n",
        "for item in splits.files:\n",
        "  print(item)\n",
        "  print(f\"No. of rows: {len(splits[item])}\")\n",
        "  split_data = data[data.index.isin(splits[item])]\n",
        "  split_classes = classes[classes.index.isin(splits[item])]\n",
        "  txnIds = split_data[split_data.columns[0]].to_numpy()\n",
        "  split_edgelist = edgelist[edgelist.iloc[:, 0].isin(txnIds)]\n",
        "  split_edgelist = split_edgelist[split_edgelist.iloc[:, 1].isin(txnIds)]\n",
        "  print(split_data.shape)\n",
        "  print(split_edgelist.shape)\n",
        "\n",
        "  # Filter nodes that are in edgelist only\n",
        "  # IMPORTANT: Nodes will be used later, so it has to tally with the nodes in the edgelist\n",
        "  split_data = split_data[split_data.iloc[:, 0].isin(split_edgelist.iloc[:,0]) | split_data.iloc[:, 0].isin(split_edgelist.iloc[:,1])]\n",
        "  labels_data = split_classes[split_classes.iloc[:, 0].isin(split_edgelist.iloc[:,0]) | split_data.iloc[:, 0].isin(split_edgelist.iloc[:,1])]\n",
        "  labels_data = labels_data.iloc[:, -1] # last column\n",
        "\n",
        "  # Separate nodes, features\n",
        "  original_nodes = split_data.iloc[:, 0]   # first column (transaction IDs)\n",
        "  features_data = split_data.iloc[:, 1:-1]   # all columns except last and first column (txn id)\n",
        "\n",
        "  # Remap Y labels to be either 0 or 1\n",
        "  labels_data = labels_data[~labels_data.str.contains('unknown')]\n",
        "  labels_data = labels_data.map({'2': 0, '1': 1})\n",
        "\n",
        "  # Create a mapping from original txn IDs to new sequential indices\n",
        "  # Necessary to make it easier to process IDs in graph\n",
        "  unique_original_nodes = original_nodes.unique()\n",
        "  node_mapping = {node_id:i for i, node_id in enumerate(unique_original_nodes)}\n",
        "\n",
        "  # Initialize features and labels\n",
        "  num_unique_nodes = len(unique_original_nodes)\n",
        "  features = torch.zeros(num_unique_nodes, features_data.shape[1], dtype=torch.float)\n",
        "  labels = torch.zeros(num_unique_nodes, dtype=torch.long)\n",
        "\n",
        "  # Align\n",
        "  nodes = torch.tensor([node_mapping[node_id] for node_id in original_nodes], dtype=torch.long)\n",
        "  features[nodes] = torch.tensor(features_data.values, dtype=torch.float)\n",
        "  labels[nodes] = torch.tensor(labels_data.values, dtype=torch.long)\n",
        "\n",
        "  print(f\"No. of nodes: {num_unique_nodes}\") # Should be equal to num of rows in dataset\n",
        "  print(f\"Original nodes shape: {original_nodes.shape}\")\n",
        "  print(f\"Original features shape: {features_data.shape}\")\n",
        "  print(f\"Original labels shape: {labels_data.shape}\")\n",
        "  print(f\"Features shape: {features.shape}\")\n",
        "  print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "  # Update the edgelist as well to use the new sequential indices\n",
        "  src_nodes = split_edgelist[split_edgelist.columns[0]].map(node_mapping).values\n",
        "  dest_nodes = split_edgelist[split_edgelist.columns[1]].map(node_mapping).values\n",
        "  dataset.append({\n",
        "      \"set\": item,\n",
        "      \"features\": features,\n",
        "      \"labels\": labels,\n",
        "      \"nodes\": nodes,\n",
        "      \"src_nodes\": src_nodes,\n",
        "      \"dest_nodes\": dest_nodes\n",
        "  })\n",
        "\n",
        "# print(dataset[0])"
      ],
      "metadata": {
        "id": "8pQsi8DN7g78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "696846ce-83e8-414c-cedd-c230ffed54f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NpzFile 'elliptic_splits.npz' with keys: train_idx, val_idx, test_idx\n",
            "train_idx\n",
            "No. of rows: 27938\n",
            "(27938, 167)\n",
            "(12882, 2)\n",
            "No. of nodes: 15671\n",
            "Original nodes shape: (15671,)\n",
            "Original features shape: (15671, 165)\n",
            "Original labels shape: (15671,)\n",
            "Features shape: torch.Size([15671, 165])\n",
            "Labels shape: torch.Size([15671])\n",
            "val_idx\n",
            "No. of rows: 9313\n",
            "(9313, 167)\n",
            "(1415, 2)\n",
            "No. of nodes: 2227\n",
            "Original nodes shape: (2227,)\n",
            "Original features shape: (2227, 165)\n",
            "Original labels shape: (2227,)\n",
            "Features shape: torch.Size([2227, 165])\n",
            "Labels shape: torch.Size([2227])\n",
            "test_idx\n",
            "No. of rows: 9313\n",
            "(9313, 167)\n",
            "(1550, 2)\n",
            "No. of nodes: 2355\n",
            "Original nodes shape: (2355,)\n",
            "Original features shape: (2355, 165)\n",
            "Original labels shape: (2355,)\n",
            "Features shape: torch.Size([2355, 165])\n",
            "Labels shape: torch.Size([2355])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN MODEL\n",
        "\n",
        "# Train Dataset\n",
        "trainDS = dataset[0]\n",
        "features = trainDS[\"features\"]\n",
        "labels = trainDS[\"labels\"]\n",
        "nodes = trainDS[\"nodes\"]\n",
        "src = torch.tensor(trainDS[\"src_nodes\"], dtype=torch.long)\n",
        "dst = torch.tensor(trainDS[\"dest_nodes\"], dtype=torch.long)\n",
        "print(features)\n",
        "print(labels)\n",
        "print(nodes)\n",
        "print(src)\n",
        "print(dst)\n",
        "\n",
        "print(\"Label distribution:\", torch.bincount(labels))\n",
        "\n",
        "graph = dgl.graph((src, dst))\n",
        "graph = dgl.add_self_loop(graph)\n",
        "\n",
        "print(f\"Number of nodes: {graph.number_of_nodes()}, Number of edges: {graph.number_of_edges()}\")\n",
        "\n",
        "# Validation Dataset\n",
        "validationDS = dataset[1]\n",
        "features_v = trainDS[\"features\"]\n",
        "labels_v = trainDS[\"labels\"]\n",
        "nodes_v = trainDS[\"nodes\"]\n",
        "src_v = torch.tensor(trainDS[\"src_nodes\"], dtype=torch.long)\n",
        "dst_v = torch.tensor(trainDS[\"dest_nodes\"], dtype=torch.long)\n",
        "graph_v = dgl.graph((src_v, dst_v))\n",
        "graph_v = dgl.add_self_loop(graph_v)\n",
        "\n",
        "print(\"\\nSTARTING TRAINING...\")\n",
        "epoch = 100\n",
        "model = train(graph, features, labels, nodes, graph_v, features_v, labels_v, nodes_v, epoch)\n",
        "print(\"\\nEND OF TRAINING\")"
      ],
      "metadata": {
        "id": "EKGkRAQvC7-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd9e39e5-271b-4853-b7a9-76f94a441ded"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.0000e+00,  1.6305e-01,  1.9638e+00,  ..., -1.3116e-01,\n",
            "          6.7780e-01, -1.2061e-01],\n",
            "        [ 1.0000e+00, -5.0271e-03,  5.7894e-01,  ..., -1.3116e-01,\n",
            "          3.3321e-01, -1.2061e-01],\n",
            "        [ 1.0000e+00, -1.5136e-01, -1.8467e-01,  ..., -1.3116e-01,\n",
            "         -9.7524e-02, -1.2061e-01],\n",
            "        ...,\n",
            "        [ 4.9000e+01, -1.7249e-01, -9.0143e-02,  ..., -1.3116e-01,\n",
            "         -9.7524e-02, -1.2061e-01],\n",
            "        [ 4.9000e+01, -1.8297e-02, -1.1549e-01,  ..., -1.3116e-01,\n",
            "         -9.7524e-02, -1.2061e-01],\n",
            "        [ 4.9000e+01, -1.7041e-01, -7.8164e-02,  ..., -1.3116e-01,\n",
            "         -9.7524e-02, -1.2061e-01]])\n",
            "tensor([0, 0, 0,  ..., 1, 0, 1])\n",
            "tensor([    0,     1,     2,  ..., 15668, 15669, 15670])\n",
            "tensor([    9,    11,    13,  ..., 15523, 15662, 15661])\n",
            "tensor([   10,    12,    11,  ..., 15662, 15478, 15489])\n",
            "Label distribution: tensor([14702,   969])\n",
            "Number of nodes: 15671, Number of edges: 28553\n",
            "\n",
            "STARTING TRAINING...\n",
            "[165, 165, 165, 165]\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 0, Training Loss: 0.7520, Validation Loss: 0.6888\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 1, Training Loss: 0.6894, Validation Loss: 0.6405\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 2, Training Loss: 0.6412, Validation Loss: 0.5914\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 3, Training Loss: 0.5924, Validation Loss: 0.5410\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 4, Training Loss: 0.5421, Validation Loss: 0.4872\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 5, Training Loss: 0.4885, Validation Loss: 0.4312\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 6, Training Loss: 0.4327, Validation Loss: 0.3757\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 7, Training Loss: 0.3772, Validation Loss: 0.3240\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 8, Training Loss: 0.3253, Validation Loss: 0.2807\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 9, Training Loss: 0.2816, Validation Loss: 0.2498\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 10, Training Loss: 0.2503, Validation Loss: 0.2341\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 11, Training Loss: 0.2343, Validation Loss: 0.2333\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 12, Training Loss: 0.2331, Validation Loss: 0.2429\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 13, Training Loss: 0.2426, Validation Loss: 0.2562\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 14, Training Loss: 0.2558, Validation Loss: 0.2674\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 15, Training Loss: 0.2670, Validation Loss: 0.2733\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 16, Training Loss: 0.2729, Validation Loss: 0.2730\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 17, Training Loss: 0.2727, Validation Loss: 0.2673\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 18, Training Loss: 0.2671, Validation Loss: 0.2579\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 19, Training Loss: 0.2577, Validation Loss: 0.2471\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 20, Training Loss: 0.2471, Validation Loss: 0.2381\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 21, Training Loss: 0.2382, Validation Loss: 0.2328\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 22, Training Loss: 0.2328, Validation Loss: 0.2319\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 23, Training Loss: 0.2320, Validation Loss: 0.2344\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 24, Training Loss: 0.2345, Validation Loss: 0.2379\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 25, Training Loss: 0.2380, Validation Loss: 0.2404\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 26, Training Loss: 0.2404, Validation Loss: 0.2409\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 27, Training Loss: 0.2409, Validation Loss: 0.2395\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 28, Training Loss: 0.2395, Validation Loss: 0.2371\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 29, Training Loss: 0.2371, Validation Loss: 0.2345\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 30, Training Loss: 0.2346, Validation Loss: 0.2325\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 31, Training Loss: 0.2326, Validation Loss: 0.2314\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 32, Training Loss: 0.2314, Validation Loss: 0.2311\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 33, Training Loss: 0.2311, Validation Loss: 0.2315\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 34, Training Loss: 0.2314, Validation Loss: 0.2321\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 35, Training Loss: 0.2321, Validation Loss: 0.2327\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 36, Training Loss: 0.2326, Validation Loss: 0.2330\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 37, Training Loss: 0.2329, Validation Loss: 0.2329\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 38, Training Loss: 0.2328, Validation Loss: 0.2324\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 39, Training Loss: 0.2323, Validation Loss: 0.2315\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 40, Training Loss: 0.2315, Validation Loss: 0.2305\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 41, Training Loss: 0.2305, Validation Loss: 0.2295\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 42, Training Loss: 0.2295, Validation Loss: 0.2285\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 43, Training Loss: 0.2286, Validation Loss: 0.2276\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 44, Training Loss: 0.2277, Validation Loss: 0.2267\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 45, Training Loss: 0.2269, Validation Loss: 0.2257\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 46, Training Loss: 0.2260, Validation Loss: 0.2245\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 47, Training Loss: 0.2248, Validation Loss: 0.2230\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 48, Training Loss: 0.2234, Validation Loss: 0.2208\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 49, Training Loss: 0.2215, Validation Loss: 0.2181\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 50, Training Loss: 0.2189, Validation Loss: 0.2147\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 51, Training Loss: 0.2160, Validation Loss: 0.2105\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 52, Training Loss: 0.2119, Validation Loss: 0.2057\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 53, Training Loss: 0.2078, Validation Loss: 0.2008\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 54, Training Loss: 0.2040, Validation Loss: 0.1960\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 55, Training Loss: 0.1993, Validation Loss: 0.1916\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 56, Training Loss: 0.1951, Validation Loss: 0.1877\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 57, Training Loss: 0.1893, Validation Loss: 0.1830\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 58, Training Loss: 0.1850, Validation Loss: 0.1785\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 59, Training Loss: 0.1819, Validation Loss: 0.1752\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 60, Training Loss: 0.1774, Validation Loss: 0.1713\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 61, Training Loss: 0.1732, Validation Loss: 0.1658\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 62, Training Loss: 0.1681, Validation Loss: 0.1633\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 63, Training Loss: 0.1667, Validation Loss: 0.1636\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 64, Training Loss: 0.1654, Validation Loss: 0.1661\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 65, Training Loss: 0.1645, Validation Loss: 0.1644\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 66, Training Loss: 0.1610, Validation Loss: 0.1595\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 67, Training Loss: 0.1575, Validation Loss: 0.1551\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 68, Training Loss: 0.1580, Validation Loss: 0.1593\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 69, Training Loss: 0.1548, Validation Loss: 0.1631\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 70, Training Loss: 0.1549, Validation Loss: 0.1591\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 71, Training Loss: 0.1512, Validation Loss: 0.1514\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 72, Training Loss: 0.1466, Validation Loss: 0.1473\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 73, Training Loss: 0.1469, Validation Loss: 0.1498\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 74, Training Loss: 0.1425, Validation Loss: 0.1502\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 75, Training Loss: 0.1403, Validation Loss: 0.1382\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 76, Training Loss: 0.1372, Validation Loss: 0.1348\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 77, Training Loss: 0.1363, Validation Loss: 0.1408\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 78, Training Loss: 0.1346, Validation Loss: 0.1366\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 79, Training Loss: 0.1318, Validation Loss: 0.1314\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 80, Training Loss: 0.1314, Validation Loss: 0.1304\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 81, Training Loss: 0.1295, Validation Loss: 0.1332\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 82, Training Loss: 0.1250, Validation Loss: 0.1249\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 83, Training Loss: 0.1248, Validation Loss: 0.1215\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 84, Training Loss: 0.1258, Validation Loss: 0.1276\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 85, Training Loss: 0.1234, Validation Loss: 0.1197\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 86, Training Loss: 0.1219, Validation Loss: 0.1127\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 87, Training Loss: 0.1181, Validation Loss: 0.1152\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 88, Training Loss: 0.1161, Validation Loss: 0.1209\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 89, Training Loss: 0.1148, Validation Loss: 0.1114\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 90, Training Loss: 0.1155, Validation Loss: 0.1075\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 91, Training Loss: 0.1111, Validation Loss: 0.1201\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 92, Training Loss: 0.1130, Validation Loss: 0.1079\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 93, Training Loss: 0.1109, Validation Loss: 0.1062\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 94, Training Loss: 0.1115, Validation Loss: 0.1142\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 95, Training Loss: 0.1080, Validation Loss: 0.1039\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 96, Training Loss: 0.1089, Validation Loss: 0.0968\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 97, Training Loss: 0.1068, Validation Loss: 0.1082\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 98, Training Loss: 0.1073, Validation Loss: 0.1077\n",
            "TRAINING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "VALIDATING\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Batch Size: 15671\n",
            "Epoch 99, Training Loss: 0.1043, Validation Loss: 0.0987\n",
            "Returning model...\n",
            "\n",
            "END OF TRAINING\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# TEST\n",
        "testDS = dataset[2]\n",
        "features = testDS[\"features\"]\n",
        "labels = testDS[\"labels\"]\n",
        "nodes = testDS[\"nodes\"]\n",
        "src = torch.tensor(testDS[\"src_nodes\"], dtype=torch.long)\n",
        "dst = torch.tensor(testDS[\"dest_nodes\"], dtype=torch.long)\n",
        "print(features)\n",
        "print(labels)\n",
        "print(nodes)\n",
        "print(src)\n",
        "print(dst)\n",
        "\n",
        "graph = dgl.graph((src, dst))\n",
        "graph = dgl.add_self_loop(graph)\n",
        "\n",
        "print(f\"Number of nodes: {graph.number_of_nodes()}, Number of edges: {graph.number_of_edges()}\")\n",
        "\n",
        "print(\"\\nTESTING\")\n",
        "\n",
        "model.eval()\n",
        "print(\"Label distribution:\", torch.bincount(labels))\n",
        "labels = labels.detach().numpy()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  features = features.cuda()\n",
        "  nodes = nodes.cuda()\n",
        "  graph = graph.to(\"cuda\")\n",
        "\n",
        "outputs = model(graph, features, nodes).cpu().argmax(dim=1).detach().numpy()\n",
        "print(\"Pred distribution:\", torch.bincount(model(graph, features, nodes).argmax(dim=1)))\n",
        "\n",
        "acc = accuracy_score(labels, outputs)\n",
        "prec = precision_score(labels, outputs)\n",
        "rec = recall_score(labels, outputs)\n",
        "f1  = f1_score(labels, outputs)\n",
        "print(\"Accuracy: \", acc)\n",
        "print(\"Precision: \", prec)\n",
        "print(\"Recall: \", rec)\n",
        "print(\"F1 Score: \", f1)\n"
      ],
      "metadata": {
        "id": "CiaEeVDcZtVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31735103-1682-469a-f077-032e10370182"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.0000, -0.1729, -0.1847,  ..., -0.0932, -0.0688, -0.1206],\n",
            "        [ 1.0000, -0.1729, -0.1847,  ..., -0.1312, -0.0975, -0.1206],\n",
            "        [ 1.0000,  0.0923,  1.2390,  ..., -0.1312,  0.0748, -0.1206],\n",
            "        ...,\n",
            "        [49.0000, -0.0942, -0.1162,  ..., -0.1312, -0.0975, -0.1206],\n",
            "        [49.0000,  0.7024, -0.1227,  ..., -0.1312, -0.0975, -0.1206],\n",
            "        [49.0000,  0.7033, -0.1202,  ..., -0.1312, -0.0975, -0.1206]])\n",
            "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
            "tensor([   0,    1,    2,  ..., 2352, 2353, 2354])\n",
            "tensor([   0,    3,   11,  ..., 2350, 2339, 2347])\n",
            "tensor([   1,    6,   12,  ..., 2342, 2342, 2344])\n",
            "Number of nodes: 2355, Number of edges: 3905\n",
            "\n",
            "TESTING\n",
            "Label distribution: tensor([2212,  143])\n",
            "Batch Size: 2355\n",
            "Batch Size: 2355\n",
            "Batch Size: 2355\n",
            "Batch Size: 2355\n",
            "Batch Size: 2355\n",
            "Batch Size: 2355\n",
            "Pred distribution: tensor([2209,  146], device='cuda:0')\n",
            "Accuracy:  0.9673036093418259\n",
            "Precision:  0.72\n",
            "Recall:  0.7552447552447552\n",
            "F1 Score:  0.7372013651877133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "LR: 0.001\n",
        "\n",
        "GraphSage Layers: 2\n",
        "\n",
        "Num Neighbours: 2\n",
        "\n",
        "----\n",
        "## Split with illicit/licit nodes\n",
        "| Type    | train loss | validation loss | Accuracy | Precision | Recall | F1 Score\n",
        "| -------- | ------- | ------- | ------- | ------- | ------- | ------- |\n",
        "| GraphSAGE-Mean   |   0.1036  |  0.0906   |   0.972  |  0.829   |  0.678   |  0.746   |\n",
        "| GraphSAGE-MaxPool    |  0.1043  |  0.0987   |  0.967   |   0.72  |   0.755  |  0.737   |\n",
        "| GraphSAGE-LSTM    |  0.0843  |  0.0733   |   0.975  |  0.85   |  0.713   |  0.776   |\n"
      ],
      "metadata": {
        "id": "UdPsWSU_q3E4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1BM4C0Kjz6lQ"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}