{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "collapsed_sections": [
        "dAVxDomSLuNh"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GraphSAGE"
      ],
      "metadata": {
        "id": "L1pXvMBymFpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "4D5S0Bzbmsce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us-TC5nwqGDm",
        "outputId": "0285cb66-56fa-4404-efda-8c3989913bca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 14 15:23:10 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   76C    P0             36W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl -f https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html # Install appropriate version see - https://www.dgl.ai/pages/start.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzpyMmh8nD-p",
        "outputId": "49e33d01-ec1d-4fae-a201-7a3d05921fe1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.12/dist-packages (2.4.0+cu124)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.12/dist-packages (from dgl) (3.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from dgl) (25.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from dgl) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from dgl) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from dgl) (4.67.1)\n",
            "Requirement already satisfied: torch<=2.4.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.4.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (2025.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (3.20.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (75.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<=2.4.0->dgl) (12.6.85)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->dgl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->dgl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->dgl) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.4.0->dgl) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch<=2.4.0->dgl) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gf66s9nemCqE"
      },
      "outputs": [],
      "source": [
        "import torch, traceback, gc\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Aggregators"
      ],
      "metadata": {
        "id": "IeOWI6Y4m4W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MeanAggregator(nn.Module):\n",
        "    def __init__(self, input_dim: int, output_dim: int):\n",
        "      super().__init__()\n",
        "      self.input_dim = input_dim\n",
        "      self.output_dim = output_dim\n",
        "      self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, neighbor_features: torch.Tensor) -> torch.Tensor:\n",
        "      # neighbor_features shape: [batch_size, num_neighbors, input_dim]\n",
        "      # Compute mean along the neighbor dimension\n",
        "      aggregated = torch.mean(neighbor_features, dim=1)  # [batch_size, input_dim]\n",
        "      return self.linear(aggregated)\n",
        "\n",
        "class MaxPoolAggregator(nn.Module): # No difference with min see paper - https://arxiv.org/pdf/1706.02216\n",
        "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = None):\n",
        "      super().__init__()\n",
        "      self.input_dim = input_dim\n",
        "      self.output_dim = output_dim\n",
        "      if hidden_dim is None:\n",
        "          hidden_dim = output_dim\n",
        "\n",
        "      self.mlp = nn.Sequential(\n",
        "          nn.Linear(input_dim, hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_dim, output_dim)\n",
        "      )\n",
        "\n",
        "    def forward(self, neighbor_features: torch.Tensor) -> torch.Tensor:\n",
        "        # neighbor_features shape: [batch_size, num_neighbors, input_dim]\n",
        "        batch_size, num_neighbors, input_dim = neighbor_features.shape\n",
        "\n",
        "        # Reshape to apply MLP to each neighbor\n",
        "        neighbor_features = neighbor_features.view(-1, input_dim)  # [batch_size * num_neighbors, input_dim]\n",
        "        neighbor_features = self.mlp(neighbor_features)  # [batch_size * num_neighbors, output_dim]\n",
        "\n",
        "        # Reshape back and apply max pooling\n",
        "        neighbor_features = neighbor_features.view(batch_size, num_neighbors, -1)\n",
        "        aggregated = torch.max(neighbor_features, dim=1)[0]  # [batch_size, output_dim]\n",
        "\n",
        "        return aggregated\n",
        "\n",
        "class LSTMAggregator(nn.Module):\n",
        "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = None):\n",
        "      super().__init__()\n",
        "      self.input_dim = input_dim\n",
        "      self.output_dim = output_dim\n",
        "      if hidden_dim is None:\n",
        "          hidden_dim = output_dim\n",
        "\n",
        "      self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "      self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, neighbor_features: torch.Tensor) -> torch.Tensor:\n",
        "        # neighbor_features shape: [batch_size, num_neighbors, input_dim]\n",
        "        # LSTM expects input in the same format\n",
        "        _, (h_n, _) = self.lstm(neighbor_features)\n",
        "\n",
        "        # Use the last hidden state\n",
        "        h_n = h_n.squeeze(0)  # [batch_size, hidden_dim]\n",
        "        return self.linear(h_n)\n"
      ],
      "metadata": {
        "id": "LkOHLXsOmrz1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphSAGELayer(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    input_dim: int,\n",
        "    output_dim: int,\n",
        "    aggregator_type: str = 'mean',\n",
        "    num_samples: int = 10,\n",
        "    dropout: float = 0.5,\n",
        "    normalize: bool = True):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.num_samples = num_samples\n",
        "    self.normalize = normalize\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Initialize aggregator\n",
        "    if aggregator_type == 'mean':\n",
        "        self.aggregator = MeanAggregator(input_dim, output_dim)\n",
        "    elif aggregator_type == 'maxpool':\n",
        "        self.aggregator = MaxPoolAggregator(input_dim, output_dim)\n",
        "    elif aggregator_type == 'lstm':\n",
        "        self.aggregator = LSTMAggregator(input_dim, output_dim)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown aggregator type: {aggregator_type}\")\n",
        "\n",
        "    # Self transformation\n",
        "    self.self_linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    # Final transformation (concatenation of self + neighbor)\n",
        "    self.final_linear = nn.Linear(output_dim * 2, output_dim)\n",
        "\n",
        "  def sample_neighbors(self, graph: dgl.DGLGraph, nodes: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "      # Use DGL's sampling functionality\n",
        "      sampled_graph = dgl.sampling.sample_neighbors(\n",
        "          graph, nodes, self.num_samples, replace=True\n",
        "      )\n",
        "\n",
        "      # Get source and destination nodes\n",
        "      src, dst = sampled_graph.edges()\n",
        "      return src, dst\n",
        "\n",
        "  def forward(self, graph: dgl.DGLGraph, features: torch.Tensor, nodes: torch.Tensor) -> torch.Tensor:\n",
        "      # if nodes is None:\n",
        "      #     nodes = torch.arange(graph.number_of_nodes(), device=features.device)\n",
        "\n",
        "      batch_size = len(nodes)\n",
        "      print(f\"Batch Size: {batch_size}\")\n",
        "\n",
        "      # Sample neighbors for each target node\n",
        "      src_nodes, dst_nodes = self.sample_neighbors(graph, nodes)\n",
        "\n",
        "      # Group neighbors by destination node\n",
        "      neighbor_features_list = []\n",
        "\n",
        "      for i, node in enumerate(nodes):\n",
        "          # Find neighbors of current node\n",
        "          neighbor_mask = (dst_nodes == node)\n",
        "          node_neighbors = src_nodes[neighbor_mask]\n",
        "\n",
        "          if len(node_neighbors) == 0:\n",
        "              # If no neighbors, use zero features\n",
        "              neighbor_feats = torch.zeros(self.num_samples, self.input_dim, device=features.device)\n",
        "          else:\n",
        "              # Get neighbor features\n",
        "              neighbor_feats = features[node_neighbors]\n",
        "\n",
        "              # Pad or truncate to max neighbours\n",
        "              # If neighbours < num_samples, pad with features from random existing neighbours\n",
        "              # Else if neighbours > num_samples, we randomly pick X(num_samples) number of neighbour features\n",
        "              if len(neighbor_feats) < self.num_samples:\n",
        "                  # Pad with repetition of existing neighbors\n",
        "                  indices = torch.randint(0, len(neighbor_feats), (self.num_samples - len(neighbor_feats),))\n",
        "                  padding = neighbor_feats[indices]\n",
        "                  neighbor_feats = torch.cat([neighbor_feats, padding], dim=0)\n",
        "              elif len(neighbor_feats) > self.num_samples:\n",
        "                  # Random sampling\n",
        "                  indices = torch.randperm(len(neighbor_feats))[:self.num_samples]\n",
        "                  neighbor_feats = neighbor_feats[indices]\n",
        "\n",
        "          neighbor_features_list.append(neighbor_feats)\n",
        "\n",
        "      # Stack neighbor features\n",
        "      neighbor_features = torch.stack(neighbor_features_list, dim=0)  # [batch_size, num_samples, input_dim]\n",
        "\n",
        "      # Apply dropout\n",
        "      neighbor_features = self.dropout(neighbor_features)\n",
        "\n",
        "      # Aggregate neighbor features - h^kN = aggregate(h^k-1N)\n",
        "      aggregated_neighbors = self.aggregator(neighbor_features)  # [batch_size, output_dim]\n",
        "\n",
        "      # Transform self features\n",
        "      self_features = features[nodes]  # [batch_size, input_dim]\n",
        "      self_features = self.dropout(self_features)\n",
        "      transformed_self = self.self_linear(self_features)  # [batch_size, output_dim]\n",
        "\n",
        "      # Concatenate self and neighbor features - h^k = concat(h^k-1, h^kN)\n",
        "      combined = torch.cat([transformed_self, aggregated_neighbors], dim=1)  # [batch_size, 2*output_dim]\n",
        "\n",
        "      # Final transformation - h^k = W^k * h^k\n",
        "      output = self.final_linear(combined)  # [batch_size, output_dim]\n",
        "\n",
        "      # Apply activation - sigma(h^k)\n",
        "      output = F.relu(output)\n",
        "\n",
        "      # L2 normalization\n",
        "      if self.normalize:\n",
        "          output = F.normalize(output, p=2, dim=1)\n",
        "\n",
        "      return output\n",
        "\n",
        "class GraphSAGE(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_dim: int,\n",
        "      hidden_dims: List[int],\n",
        "      output_dim: int,\n",
        "      aggregator_type: str = 'mean',\n",
        "      # num_samples: List[int] = None,\n",
        "      num_samples: int = 2,\n",
        "      dropout: float = 0.5,\n",
        "      normalize: bool = True):\n",
        "    super().__init__()\n",
        "\n",
        "    # if num_samples is None:\n",
        "    #     num_samples = [10] * len(hidden_dims)\n",
        "\n",
        "    # assert len(hidden_dims) == len(num_samples), \"Length mismatch between hidden_dims and num_samples\"\n",
        "\n",
        "    self.layers = nn.ModuleList()\n",
        "\n",
        "    # Concatenate for stacking multiple layers\n",
        "    dims = [input_dim] + hidden_dims + [output_dim]\n",
        "    print(dims)\n",
        "\n",
        "    # Create GraphSAGE layers\n",
        "    for i in range(len(dims) - 1):\n",
        "      layer = GraphSAGELayer(\n",
        "          input_dim=dims[i],\n",
        "          output_dim=dims[i + 1],\n",
        "          aggregator_type=aggregator_type,\n",
        "          # num_samples=num_samples[min(i, len(num_samples) - 1)],\n",
        "          num_samples=num_samples,\n",
        "          dropout=dropout if i < len(dims) - 2 else 0,  # No dropout in last layer\n",
        "          normalize=normalize if i < len(dims) - 2 else False  # No normalization in last layer\n",
        "      )\n",
        "      self.layers.append(layer)\n",
        "\n",
        "  def forward(self, graph: dgl.DGLGraph, features: torch.Tensor, nodes: torch.Tensor) -> torch.Tensor:\n",
        "    h = features\n",
        "    for layer in self.layers:\n",
        "        h_new = layer(graph, h, nodes)\n",
        "\n",
        "        # For next layer, we need to update the full feature matrix\n",
        "        if nodes is not None:\n",
        "            h_full = h.clone()\n",
        "            h_full[nodes] = h_new\n",
        "            h = h_full\n",
        "        else:\n",
        "            h = h_new\n",
        "\n",
        "    return h if nodes is None else h[nodes]\n",
        "\n",
        "class GraphSAGENodeClassifier(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dims: List[int], num_classes: int, **kwargs):\n",
        "      super().__init__()\n",
        "      self.graphsage = GraphSAGE(input_dim, hidden_dims, hidden_dims[-1], **kwargs)\n",
        "      self.classifier = nn.Linear(hidden_dims[-1], num_classes)\n",
        "\n",
        "    def forward(self, graph: dgl.DGLGraph, features: torch.Tensor, nodes: torch.Tensor):\n",
        "      embeddings = self.graphsage(graph, features, nodes)\n",
        "      return self.classifier(embeddings)\n",
        "\n",
        "def train(graph, features, labels, nodes, graph_v, features_v, labels_v, nodes_v, epoch=5):\n",
        "  input_dim = features.shape[1]\n",
        "  num_classes = 2 # Illicit or Licit only\n",
        "\n",
        "  # Create model\n",
        "  model = GraphSAGENodeClassifier(\n",
        "      input_dim=input_dim,\n",
        "      hidden_dims=[input_dim, input_dim],\n",
        "      num_classes=num_classes,\n",
        "      aggregator_type='maxpool', # Define type of aggregator here - mean, maxpool, lstm\n",
        "      num_samples=2 # Define the number of neighbours to sample\n",
        "  )\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    features = features.cuda()\n",
        "    labels = labels.cuda()\n",
        "    nodes = nodes.cuda()\n",
        "    graph = graph.to(\"cuda\")\n",
        "\n",
        "    if graph_v is not None:\n",
        "      features_v = features_v.cuda()\n",
        "      labels_v = labels_v.cuda()\n",
        "      nodes_v = nodes_v.cuda()\n",
        "      graph_v = graph_v.to(\"cuda\")\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Track loss and metrics history for plotting\n",
        "  history = {\n",
        "      'epoch': [],\n",
        "      'train_loss': [],\n",
        "  }\n",
        "\n",
        "  for epoch in range(epoch):\n",
        "      # Training\n",
        "      model.train()\n",
        "      print(\"TRAINING\")\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      logits = model(graph, features, nodes)\n",
        "      loss = criterion(logits, labels)\n",
        "\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Add to history\n",
        "      history['epoch'].append(epoch)\n",
        "      history['train_loss'].append(loss.item())\n",
        "\n",
        "      # Validation\n",
        "      val_loss_disp = None\n",
        "      if graph_v is not None:\n",
        "        model.eval()\n",
        "        print(\"VALIDATING\")\n",
        "        logits = model(graph_v, features_v, nodes_v)\n",
        "        val_loss = criterion(logits, labels_v)\n",
        "        val_loss_disp = str(round(val_loss.item(), 4))\n",
        "\n",
        "      print(f\"Epoch {epoch}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss_disp}\")\n",
        "  print(\"Returning model...\")\n",
        "  return model, history\n"
      ],
      "metadata": {
        "id": "L99ppMfzm83y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation"
      ],
      "metadata": {
        "id": "P6xICMWADdnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading from Labelled Split (with timestep split)"
      ],
      "metadata": {
        "id": "CkbX97_ZMuXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_elliptic_splits(include_unknowns=True, combine_train_val=True):\n",
        "  \"\"\"Load pre-computed splits from splits folder\"\"\"\n",
        "  splits_dir = 'splits/full_dataset' if include_unknowns else 'splits/labeled_only'\n",
        "  print(f\"Loading splits from {splits_dir} with include_unknowns={include_unknowns}\")\n",
        "\n",
        "  # Load features and classes for each split\n",
        "  def load_split_data(split_name):\n",
        "      features_df = pd.read_csv(f'{splits_dir}/{split_name}_features.csv')  # Has header\n",
        "      classes_df = pd.read_csv(f'{splits_dir}/{split_name}_classes.csv')\n",
        "      edges_df = pd.read_csv(f'{splits_dir}/{split_name}_edges.csv')\n",
        "\n",
        "      # Process features - txId is first column, timestep is second, features start from 3rd\n",
        "      node_ids = features_df['txId'].values\n",
        "      timesteps = features_df['timestep'].values  # Extract timestep information\n",
        "      features = features_df.iloc[:, 1:].values.astype(np.float32)  # Skip only txId, include timestep and all features\n",
        "\n",
        "      # Process labels - handle both string and integer class values\n",
        "      labels = []\n",
        "      for _, row in classes_df.iterrows():\n",
        "          class_val = row['class']\n",
        "\n",
        "          # Handle both string and integer class values\n",
        "          if class_val == '1' or class_val == 1:  # illicit\n",
        "              labels.append(0)\n",
        "          elif class_val == '2' or class_val == 2:  # licit\n",
        "              labels.append(1)\n",
        "          else:  # unknown (string 'unknown' or any other value)\n",
        "              labels.append(-1 if include_unknowns else None)\n",
        "\n",
        "      # Filter out None labels if not including unknowns\n",
        "      if not include_unknowns:\n",
        "          valid_mask = [l is not None for l in labels]\n",
        "          valid_indices = [i for i, valid in enumerate(valid_mask) if valid]\n",
        "          node_ids = node_ids[valid_indices]\n",
        "          timesteps = timesteps[valid_indices]\n",
        "          features = features[valid_indices]\n",
        "          labels = [labels[i] for i in valid_indices]\n",
        "\n",
        "      # Create node mapping for edges\n",
        "      node_to_idx = {node_id: idx for idx, node_id in enumerate(node_ids)}\n",
        "      node_index = [node_to_idx[node_id] for node_id in node_ids]\n",
        "\n",
        "      # Process edges\n",
        "      edge_list = []\n",
        "      for _, row in edges_df.iterrows():\n",
        "          if row['txId1'] in node_to_idx and row['txId2'] in node_to_idx:\n",
        "              edge_list.append([node_to_idx[row['txId1']], node_to_idx[row['txId2']]])\n",
        "\n",
        "      edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous() if edge_list else torch.empty((2, 0), dtype=torch.long)\n",
        "\n",
        "      return {\n",
        "          'features': torch.tensor(features, dtype=torch.float),\n",
        "          'edge_index': edge_index,\n",
        "          'labels': torch.tensor(labels, dtype=torch.long),\n",
        "          'timesteps': torch.tensor(timesteps, dtype=torch.long),\n",
        "          'node_ids': node_ids,\n",
        "          'node_index': torch.tensor(node_index, dtype=torch.long)\n",
        "      }\n",
        "\n",
        "  # Load train, val, test splits\n",
        "  train_data = load_split_data('train')\n",
        "  val_data = load_split_data('val')\n",
        "  test_data = load_split_data('test')\n",
        "\n",
        "  print(f\"Split sizes:\")\n",
        "  print(f\"  Train: {train_data['features'].shape[0]} nodes, {train_data['edge_index'].shape[1]} edges\")\n",
        "  print(f\"  Val: {val_data['features'].shape[0]} nodes, {val_data['edge_index'].shape[1]} edges\")\n",
        "  print(f\"  Test: {test_data['features'].shape[0]} nodes, {test_data['edge_index'].shape[1]} edges\")\n",
        "\n",
        "  # Combine train and val if requested (paper setup)\n",
        "  if combine_train_val:\n",
        "      print(\"Combining train and val into single training set (paper setup)\")\n",
        "\n",
        "      # Concatenate features and labels\n",
        "      combined_features = torch.cat([train_data['features'], val_data['features']], dim=0)\n",
        "      combined_labels = torch.cat([train_data['labels'], val_data['labels']], dim=0)\n",
        "      combined_timesteps = torch.cat([train_data['timesteps'], val_data['timesteps']], dim=0)\n",
        "      combined_node_ids = np.concatenate([train_data['node_ids'], val_data['node_ids']])\n",
        "\n",
        "      # Adjust edge indices for val data\n",
        "      val_edges_adjusted = val_data['edge_index'] + train_data['features'].shape[0]\n",
        "      combined_edge_index = torch.cat([train_data['edge_index'], val_edges_adjusted], dim=1)\n",
        "\n",
        "      # Adjust node index for val data\n",
        "      val_nodes_adjusted = val_data['node_index'] + train_data['features'].shape[0]\n",
        "      combined_node_index = torch.cat([train_data['node_index'], val_nodes_adjusted], dim=0)\n",
        "\n",
        "      train_data = {\n",
        "          'features': combined_features,\n",
        "          'edge_index': combined_edge_index,\n",
        "          'labels': combined_labels,\n",
        "          'timesteps': combined_timesteps,\n",
        "          'node_ids': combined_node_ids,\n",
        "          'node_index': combined_node_index\n",
        "      }\n",
        "\n",
        "      print(f\"Combined train: {train_data['features'].shape[0]} nodes, {train_data['edge_index'].shape[1]} edges\")\n",
        "\n",
        "  # Count labels for each split\n",
        "  for split_name, split_data in [('Train', train_data), ('Test', test_data)]:\n",
        "      labels = split_data['labels']\n",
        "      if include_unknowns:\n",
        "          illicit_count = (labels == 0).sum().item()\n",
        "          licit_count = (labels == 1).sum().item()\n",
        "          unknown_count = (labels == -1).sum().item()\n",
        "          print(f\"  {split_name}: illicit={illicit_count}, licit={licit_count}, unknown={unknown_count}\")\n",
        "      else:\n",
        "          illicit_count = (labels == 0).sum().item()\n",
        "          licit_count = (labels == 1).sum().item()\n",
        "          print(f\"  {split_name}: illicit={illicit_count}, licit={licit_count}\")\n",
        "\n",
        "  return train_data, test_data\n",
        "\n",
        "def adapt_to_dataset(name, original_nodes, features_data, split_edgelist, labels_data):\n",
        "  # Remove nodes that are not part of edgelist\n",
        "  # THIS IS IMPORTANT ELSE, WILL CRASH IF GRAPH AND NODES DONT ALIGN DURING SAMPLING\n",
        "  nodes = original_nodes.clone().detach()\n",
        "  node_mask = torch.isin(nodes, torch.cat((split_edgelist[0], split_edgelist[1]), dim=0))\n",
        "  nodes = nodes[node_mask]\n",
        "  features_out = features_data[node_mask]\n",
        "  labels_out = labels_data[node_mask]\n",
        "\n",
        "  # Update nodes and edges indices\n",
        "  # else, features and labels indices would not match\n",
        "  node_mapping = {node_id:i for i, node_id in enumerate(nodes.numpy().tolist())}\n",
        "  remapped_nodes = torch.empty_like(nodes, dtype=torch.long)\n",
        "  for i, val in enumerate(nodes):\n",
        "      remapped_nodes[i] = node_mapping[val.item()]\n",
        "  nodes = remapped_nodes\n",
        "  remapped_edgelist0 = torch.empty_like(split_edgelist[0], dtype=torch.long)\n",
        "  for i, val in enumerate(split_edgelist[0]):\n",
        "      remapped_edgelist0[i] = node_mapping[val.item()]\n",
        "  split_edgelist[0] = remapped_edgelist0\n",
        "  remapped_edgelist1 = torch.empty_like(split_edgelist[1], dtype=torch.long)\n",
        "  for i, val in enumerate(split_edgelist[1]):\n",
        "      remapped_edgelist1[i] = node_mapping[val.item()]\n",
        "  split_edgelist[1] = remapped_edgelist1\n",
        "\n",
        "  num_unique_nodes = len(nodes)\n",
        "\n",
        "  print(f\"No. of nodes: {num_unique_nodes}\")\n",
        "  print(f\"Original nodes shape: {len(original_nodes)}\")\n",
        "  print(f\"Original features shape: {features_data.shape}\")\n",
        "  print(f\"Original labels shape: {labels_data.shape}\")\n",
        "\n",
        "  print(f\"Nodes shape: {nodes.shape}\")\n",
        "  print(f\"Features shape: {features_out.shape}\")\n",
        "  print(f\"Labels shape: {labels_out.shape}\")\n",
        "\n",
        "  src_nodes = split_edgelist[0] # The first row contains source node indices\n",
        "  dest_nodes = split_edgelist[1] # The second row contains destination node indices\n",
        "\n",
        "  return {\n",
        "      \"set\": name,\n",
        "      \"features\": features_out,\n",
        "      \"labels\": labels_out,\n",
        "      \"nodes\": nodes,\n",
        "      \"src_nodes\": src_nodes.numpy(), # Will be converted to tensor later\n",
        "      \"dest_nodes\": dest_nodes.numpy() # Will be converted to tensor later\n",
        "  }\n",
        "\n",
        "train_data, test_data = load_elliptic_splits(include_unknowns=False, combine_train_val=True)\n",
        "\n",
        "dataset = []\n",
        "train_data_ds = adapt_to_dataset(\"train\", train_data[\"node_index\"], train_data[\"features\"], train_data[\"edge_index\"], train_data[\"labels\"])\n",
        "dataset.append(train_data_ds)\n",
        "test_data_ds = adapt_to_dataset(\"test\", test_data[\"node_index\"], test_data[\"features\"], test_data[\"edge_index\"], test_data[\"labels\"])\n",
        "dataset.append(test_data_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAfzXAIkL0jb",
        "outputId": "799d9415-9612-4a63-8bb9-073d975f92e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading splits from splits/labeled_only with include_unknowns=False\n",
            "Split sizes:\n",
            "  Train: 26381 nodes, 20151 edges\n",
            "  Val: 2989 nodes, 2375 edges\n",
            "  Test: 16670 nodes, 13726 edges\n",
            "Combining train and val into single training set (paper setup)\n",
            "Combined train: 29370 nodes, 22526 edges\n",
            "  Train: illicit=3379, licit=25991\n",
            "  Test: illicit=1083, licit=15587\n",
            "No. of nodes: 23072\n",
            "Original nodes shape: 29370\n",
            "Original features shape: torch.Size([29370, 166])\n",
            "Original labels shape: torch.Size([29370])\n",
            "Nodes shape: torch.Size([23072])\n",
            "Features shape: torch.Size([23072, 166])\n",
            "Labels shape: torch.Size([23072])\n",
            "No. of nodes: 12395\n",
            "Original nodes shape: 16670\n",
            "Original features shape: torch.Size([16670, 166])\n",
            "Original labels shape: torch.Size([16670])\n",
            "Nodes shape: torch.Size([12395])\n",
            "Features shape: torch.Size([12395, 166])\n",
            "Labels shape: torch.Size([12395])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading from Kaggle + Labelled split (without timestep split)"
      ],
      "metadata": {
        "id": "dAVxDomSLuNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub[pandas-datasets]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btTIq8HkDYI7",
        "outputId": "f8fd188a-6fa6-4db3-82ec-a106a61f525f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "# LOAD FEATURES\n",
        "file_path = \"elliptic_bitcoin_dataset/elliptic_txs_features.csv\"\n",
        "data = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"ellipticco/elliptic-data-set\",\n",
        "  file_path,\n",
        "  pandas_kwargs={\"header\": None},\n",
        ")\n",
        "# LOAD CLASSES\n",
        "file_path = \"elliptic_bitcoin_dataset/elliptic_txs_classes.csv\"\n",
        "classes = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"ellipticco/elliptic-data-set\",\n",
        "  file_path,\n",
        ")\n",
        "# LOAD EDGELIST\n",
        "file_path = \"elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv\"\n",
        "edgelist = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"ellipticco/elliptic-data-set\",\n",
        "  file_path,\n",
        ")\n",
        "print(data.shape)\n",
        "print(classes.shape)\n",
        "print(edgelist.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRaR-b02DGsL",
        "outputId": "298d6ac4-0722-4845-cc72-81e64b44611a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3839568209.py:7: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  data = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'elliptic-data-set' dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3839568209.py:15: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  classes = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'elliptic-data-set' dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3839568209.py:22: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  edgelist = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'elliptic-data-set' dataset.\n",
            "(203769, 167)\n",
            "(203769, 2)\n",
            "(234355, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Split\n",
        "splits = np.load('elliptic_splits.npz', allow_pickle=True)\n",
        "print(splits)\n",
        "# data = pd.read_csv('elliptic_txs_features.csv', header=None, low_memory=False)\n",
        "# edgelist = pd.read_csv('elliptic_txs_edgelist.csv', header=None, low_memory=False)\n",
        "\n",
        "dataset = []\n",
        "for item in splits.files:\n",
        "  print(item)\n",
        "  print(f\"No. of rows: {len(splits[item])}\")\n",
        "  split_data = data[data.index.isin(splits[item])]\n",
        "  split_classes = classes[classes.index.isin(splits[item])]\n",
        "  txnIds = split_data[split_data.columns[0]].to_numpy()\n",
        "  split_edgelist = edgelist[edgelist.iloc[:, 0].isin(txnIds)]\n",
        "  split_edgelist = split_edgelist[split_edgelist.iloc[:, 1].isin(txnIds)]\n",
        "  print(split_data.shape)\n",
        "  print(split_edgelist.shape)\n",
        "\n",
        "  # Filter nodes that are in edgelist only\n",
        "  # IMPORTANT: Nodes will be used later, so it has to tally with the nodes in the edgelist\n",
        "  split_data = split_data[split_data.iloc[:, 0].isin(split_edgelist.iloc[:,0]) | split_data.iloc[:, 0].isin(split_edgelist.iloc[:,1])]\n",
        "  labels_data = split_classes[split_classes.iloc[:, 0].isin(split_edgelist.iloc[:,0]) | split_data.iloc[:, 0].isin(split_edgelist.iloc[:,1])]\n",
        "  labels_data = labels_data.iloc[:, -1] # last column\n",
        "\n",
        "  # Separate nodes, features\n",
        "  original_nodes = split_data.iloc[:, 0]   # first column (transaction IDs)\n",
        "  features_data = split_data.iloc[:, 1:-1]   # all columns except last and first column (txn id)\n",
        "\n",
        "  # Remap Y labels to be either 0 or 1\n",
        "  labels_data = labels_data[~labels_data.str.contains('unknown')]\n",
        "  labels_data = labels_data.map({'2': 0, '1': 1})\n",
        "\n",
        "  # Create a mapping from original txn IDs to new sequential indices\n",
        "  # Necessary to make it easier to process IDs in graph\n",
        "  unique_original_nodes = original_nodes.unique()\n",
        "  node_mapping = {node_id:i for i, node_id in enumerate(unique_original_nodes)}\n",
        "\n",
        "  # Initialize features and labels\n",
        "  num_unique_nodes = len(unique_original_nodes)\n",
        "  features = torch.zeros(num_unique_nodes, features_data.shape[1], dtype=torch.float)\n",
        "  labels = torch.zeros(num_unique_nodes, dtype=torch.long)\n",
        "\n",
        "  # Align\n",
        "  nodes = torch.tensor([node_mapping[node_id] for node_id in original_nodes], dtype=torch.long)\n",
        "  features[nodes] = torch.tensor(features_data.values, dtype=torch.float)\n",
        "  labels[nodes] = torch.tensor(labels_data.values, dtype=torch.long)\n",
        "\n",
        "  print(f\"No. of nodes: {num_unique_nodes}\") # Should be equal to num of rows in dataset\n",
        "  print(f\"Original nodes shape: {original_nodes.shape}\")\n",
        "  print(f\"Original features shape: {features_data.shape}\")\n",
        "  print(f\"Original labels shape: {labels_data.shape}\")\n",
        "  print(f\"Features shape: {features.shape}\")\n",
        "  print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "  # Update the edgelist as well to use the new sequential indices\n",
        "  src_nodes = split_edgelist[split_edgelist.columns[0]].map(node_mapping).values\n",
        "  dest_nodes = split_edgelist[split_edgelist.columns[1]].map(node_mapping).values\n",
        "  dataset.append({\n",
        "      \"set\": item,\n",
        "      \"features\": features,\n",
        "      \"labels\": labels,\n",
        "      \"nodes\": nodes,\n",
        "      \"src_nodes\": src_nodes,\n",
        "      \"dest_nodes\": dest_nodes\n",
        "  })\n",
        "\n",
        "# print(dataset[0])"
      ],
      "metadata": {
        "id": "8pQsi8DN7g78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "074ed7d2-2e16-4f79-a539-7cdc5ce2e2ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NpzFile 'elliptic_splits.npz' with keys: train_idx, val_idx, test_idx\n",
            "train_idx\n",
            "No. of rows: 27938\n",
            "(27938, 167)\n",
            "(12882, 2)\n",
            "No. of nodes: 15671\n",
            "Original nodes shape: (15671,)\n",
            "Original features shape: (15671, 165)\n",
            "Original labels shape: (15671,)\n",
            "Features shape: torch.Size([15671, 165])\n",
            "Labels shape: torch.Size([15671])\n",
            "val_idx\n",
            "No. of rows: 9313\n",
            "(9313, 167)\n",
            "(1415, 2)\n",
            "No. of nodes: 2227\n",
            "Original nodes shape: (2227,)\n",
            "Original features shape: (2227, 165)\n",
            "Original labels shape: (2227,)\n",
            "Features shape: torch.Size([2227, 165])\n",
            "Labels shape: torch.Size([2227])\n",
            "test_idx\n",
            "No. of rows: 9313\n",
            "(9313, 167)\n",
            "(1550, 2)\n",
            "No. of nodes: 2355\n",
            "Original nodes shape: (2355,)\n",
            "Original features shape: (2355, 165)\n",
            "Original labels shape: (2355,)\n",
            "Features shape: torch.Size([2355, 165])\n",
            "Labels shape: torch.Size([2355])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "4ZyYSBl8e3rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN MODEL\n",
        "\n",
        "# Train Dataset\n",
        "trainDS = dataset[0]\n",
        "features = trainDS[\"features\"]\n",
        "labels = trainDS[\"labels\"]\n",
        "nodes = trainDS[\"nodes\"]\n",
        "src = torch.tensor(trainDS[\"src_nodes\"], dtype=torch.long)\n",
        "dst = torch.tensor(trainDS[\"dest_nodes\"], dtype=torch.long)\n",
        "print(features)\n",
        "print(labels)\n",
        "print(nodes)\n",
        "print(src)\n",
        "print(dst)\n",
        "\n",
        "print(\"Label distribution:\", torch.bincount(labels))\n",
        "\n",
        "graph = dgl.graph((src, dst))\n",
        "graph = dgl.add_self_loop(graph)\n",
        "\n",
        "print(f\"Number of nodes: {graph.number_of_nodes()}, Number of edges: {graph.number_of_edges()}\")\n",
        "\n",
        "# Validation Dataset\n",
        "validationDS = dataset[1]\n",
        "features_v = None\n",
        "labels_v = None\n",
        "nodes_v = None\n",
        "graph_v = None\n",
        "if(validationDS[\"set\"] == 'val_idx'):\n",
        "  features_v = validationDS[\"features\"]\n",
        "  labels_v = validationDS[\"labels\"]\n",
        "  nodes_v = validationDS[\"nodes\"]\n",
        "  src_v = torch.tensor(validationDS[\"src_nodes\"], dtype=torch.long)\n",
        "  dst_v = torch.tensor(validationDS[\"dest_nodes\"], dtype=torch.long)\n",
        "  graph_v = dgl.graph((src_v, dst_v))\n",
        "  graph_v = dgl.add_self_loop(graph_v)\n",
        "\n",
        "print(\"\\nSTARTING TRAINING...\")\n",
        "epoch = 100\n",
        "model, history = train(graph, features, labels, nodes, graph_v, features_v, labels_v, nodes_v, epoch)\n",
        "print(\"\\nEND OF TRAINING\")"
      ],
      "metadata": {
        "id": "EKGkRAQvC7-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25a51a7-9ed1-4c39-8a93-183d6caa8635"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.0000e+00,  1.6305e-01,  1.9638e+00,  ...,  6.7780e-01,\n",
            "         -1.2061e-01, -1.1979e-01],\n",
            "        [ 1.0000e+00, -5.0271e-03,  5.7894e-01,  ...,  3.3321e-01,\n",
            "         -1.2061e-01, -1.1979e-01],\n",
            "        [ 1.0000e+00, -1.4785e-01, -1.8467e-01,  ..., -9.7524e-02,\n",
            "         -1.2061e-01, -1.1979e-01],\n",
            "        ...,\n",
            "        [ 3.4000e+01, -1.7211e-01, -1.3181e-01,  ..., -9.7524e-02,\n",
            "         -1.2061e-01, -1.1979e-01],\n",
            "        [ 3.4000e+01, -1.7298e-01, -8.1852e-02,  ..., -1.4060e-01,\n",
            "         -1.7609e+00, -1.7610e+00],\n",
            "        [ 3.4000e+01, -1.7290e-01, -7.0152e-02,  ..., -9.7524e-02,\n",
            "         -1.2061e-01, -1.1979e-01]])\n",
            "tensor([1, 1, 1,  ..., 0, 1, 1])\n",
            "tensor([    0,     1,     2,  ..., 23069, 23070, 23071])\n",
            "tensor([    2,     4,     8,  ..., 22762, 22974, 22882])\n",
            "tensor([    3,     5,     9,  ..., 22687, 22730, 22949])\n",
            "Label distribution: tensor([ 1871, 21201])\n",
            "Number of nodes: 23072, Number of edges: 45598\n",
            "\n",
            "STARTING TRAINING...\n",
            "[166, 166, 166, 166]\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 0, Training Loss: 0.6809, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 1, Training Loss: 0.6145, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 2, Training Loss: 0.5638, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 3, Training Loss: 0.5156, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 4, Training Loss: 0.4679, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 5, Training Loss: 0.4214, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 6, Training Loss: 0.3771, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 7, Training Loss: 0.3375, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 8, Training Loss: 0.3062, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 9, Training Loss: 0.2870, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 10, Training Loss: 0.2815, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 11, Training Loss: 0.2877, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 12, Training Loss: 0.2996, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 13, Training Loss: 0.3102, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 14, Training Loss: 0.3153, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 15, Training Loss: 0.3138, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 16, Training Loss: 0.3070, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 17, Training Loss: 0.2975, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 18, Training Loss: 0.2884, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 19, Training Loss: 0.2823, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 20, Training Loss: 0.2810, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 21, Training Loss: 0.2834, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 22, Training Loss: 0.2869, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 23, Training Loss: 0.2888, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 24, Training Loss: 0.2886, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 25, Training Loss: 0.2864, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 26, Training Loss: 0.2832, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 27, Training Loss: 0.2802, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 28, Training Loss: 0.2782, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 29, Training Loss: 0.2770, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 30, Training Loss: 0.2765, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 31, Training Loss: 0.2761, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 32, Training Loss: 0.2748, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 33, Training Loss: 0.2728, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 34, Training Loss: 0.2692, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 35, Training Loss: 0.2645, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 36, Training Loss: 0.2589, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 37, Training Loss: 0.2527, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 38, Training Loss: 0.2456, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 39, Training Loss: 0.2376, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 40, Training Loss: 0.2278, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 41, Training Loss: 0.2187, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 42, Training Loss: 0.2113, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 43, Training Loss: 0.2026, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 44, Training Loss: 0.1938, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 45, Training Loss: 0.1887, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 46, Training Loss: 0.1835, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 47, Training Loss: 0.1787, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 48, Training Loss: 0.1773, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 49, Training Loss: 0.1747, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 50, Training Loss: 0.1756, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 51, Training Loss: 0.1705, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 52, Training Loss: 0.1681, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 53, Training Loss: 0.1636, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 54, Training Loss: 0.1640, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 55, Training Loss: 0.1604, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 56, Training Loss: 0.1587, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 57, Training Loss: 0.1597, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 58, Training Loss: 0.1528, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 59, Training Loss: 0.1547, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 60, Training Loss: 0.1477, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 61, Training Loss: 0.1499, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 62, Training Loss: 0.1437, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 63, Training Loss: 0.1458, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 64, Training Loss: 0.1406, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 65, Training Loss: 0.1412, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 66, Training Loss: 0.1350, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 67, Training Loss: 0.1372, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 68, Training Loss: 0.1337, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 69, Training Loss: 0.1325, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 70, Training Loss: 0.1290, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 71, Training Loss: 0.1284, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 72, Training Loss: 0.1249, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 73, Training Loss: 0.1221, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 74, Training Loss: 0.1215, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 75, Training Loss: 0.1177, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 76, Training Loss: 0.1174, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 77, Training Loss: 0.1150, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 78, Training Loss: 0.1115, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 79, Training Loss: 0.1105, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 80, Training Loss: 0.1115, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 81, Training Loss: 0.1092, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 82, Training Loss: 0.1096, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 83, Training Loss: 0.1052, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 84, Training Loss: 0.1032, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 85, Training Loss: 0.1035, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 86, Training Loss: 0.1048, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 87, Training Loss: 0.1054, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 88, Training Loss: 0.1003, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 89, Training Loss: 0.0980, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 90, Training Loss: 0.1010, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 91, Training Loss: 0.1004, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 92, Training Loss: 0.0996, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 93, Training Loss: 0.0936, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 94, Training Loss: 0.0972, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 95, Training Loss: 0.0941, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 96, Training Loss: 0.0950, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 97, Training Loss: 0.0949, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 98, Training Loss: 0.0949, Validation Loss: None\n",
            "TRAINING\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Batch Size: 23072\n",
            "Epoch 99, Training Loss: 0.0943, Validation Loss: None\n",
            "Returning model...\n",
            "\n",
            "END OF TRAINING\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Training Loss"
      ],
      "metadata": {
        "id": "WFxZSAeDFFT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set matplotlib to use non-interactive backend\n",
        "plt.ioff()\n",
        "\n",
        "# Training Loss Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history['epoch'], history['train_loss'], 'b-', linewidth=2, alpha=0.8)\n",
        "plt.title(f'Training Loss - {'labeled_only'.replace(\"_\", \" \").title()}', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Training Loss', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "AJqhD0hIEtmI",
        "outputId": "fbea19b9-27fc-48a4-b395-69f91a5a9ece"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2436336504.py:12: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
            "  plt.legend()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "-wE3r9YIGZwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# TEST\n",
        "if(len(dataset) == 3):\n",
        "  testDS = dataset[2]\n",
        "else:\n",
        "  testDS = dataset[1]\n",
        "features = testDS[\"features\"]\n",
        "labels = testDS[\"labels\"]\n",
        "nodes = testDS[\"nodes\"]\n",
        "src = torch.tensor(testDS[\"src_nodes\"], dtype=torch.long)\n",
        "dst = torch.tensor(testDS[\"dest_nodes\"], dtype=torch.long)\n",
        "print(features)\n",
        "print(labels)\n",
        "print(nodes)\n",
        "print(src)\n",
        "print(dst)\n",
        "\n",
        "graph = dgl.graph((src, dst))\n",
        "graph = dgl.add_self_loop(graph)\n",
        "\n",
        "print(f\"Number of nodes: {graph.number_of_nodes()}, Number of edges: {graph.number_of_edges()}\")\n",
        "\n",
        "print(\"\\nTESTING\")\n",
        "\n",
        "model.eval()\n",
        "print(\"Label distribution:\", torch.bincount(labels))\n",
        "labels = labels.detach().numpy()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  features = features.cuda()\n",
        "  nodes = nodes.cuda()\n",
        "  graph = graph.to(\"cuda\")\n",
        "\n",
        "outputs = model(graph, features, nodes).cpu().argmax(dim=1).detach().numpy()\n",
        "print(\"Pred distribution:\", torch.bincount(model(graph, features, nodes).argmax(dim=1)))\n",
        "\n",
        "acc = accuracy_score(labels, outputs)\n",
        "prec = precision_score(labels, outputs)\n",
        "rec = recall_score(labels, outputs)\n",
        "f1  = f1_score(labels, outputs)\n",
        "print(\"Accuracy: \", acc)\n",
        "print(\"Precision: \", prec)\n",
        "print(\"Recall: \", rec)\n",
        "print(\"F1 Score: \", f1)\n"
      ],
      "metadata": {
        "id": "CiaEeVDcZtVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bcc9bb6-215d-45cd-e4dd-1a85657a6a64"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 3.5000e+01, -1.6683e-01, -1.1551e-01,  ..., -1.1377e-02,\n",
            "         -1.7609e+00, -1.7610e+00],\n",
            "        [ 3.5000e+01, -1.6723e-01, -1.1509e-01,  ..., -1.4060e-01,\n",
            "          1.5197e+00,  1.5214e+00],\n",
            "        [ 3.5000e+01, -1.7251e-01, -1.2047e-01,  ..., -1.4060e-01,\n",
            "         -1.7609e+00, -1.7610e+00],\n",
            "        ...,\n",
            "        [ 4.9000e+01, -1.7041e-01, -7.8164e-02,  ..., -9.7524e-02,\n",
            "         -1.2061e-01, -1.1979e-01],\n",
            "        [ 4.9000e+01, -9.3732e-02, -1.1616e-01,  ..., -9.7524e-02,\n",
            "         -1.2061e-01, -1.1979e-01],\n",
            "        [ 4.9000e+01, -1.7201e-01, -7.8182e-02,  ..., -9.7524e-02,\n",
            "         -1.2061e-01, -1.1979e-01]])\n",
            "tensor([1, 1, 0,  ..., 0, 1, 0])\n",
            "tensor([    0,     1,     2,  ..., 12392, 12393, 12394])\n",
            "tensor([    0,     2,     5,  ..., 12085, 12368, 12367])\n",
            "tensor([    1,     3,     6,  ..., 12368, 11993, 12014])\n",
            "Number of nodes: 12395, Number of edges: 26121\n",
            "\n",
            "TESTING\n",
            "Label distribution: tensor([  576, 11819])\n",
            "Batch Size: 12395\n",
            "Batch Size: 12395\n",
            "Batch Size: 12395\n",
            "Batch Size: 12395\n",
            "Batch Size: 12395\n",
            "Batch Size: 12395\n",
            "Pred distribution: tensor([    0, 12395], device='cuda:0')\n",
            "Accuracy:  0.9535296490520371\n",
            "Precision:  0.9535296490520371\n",
            "Recall:  1.0\n",
            "F1 Score:  0.9762121086974478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "## Setup\n",
        "LR: 0.001\n",
        "\n",
        "GraphSage Layers: 2\n",
        "\n",
        "Num Neighbours: 2\n",
        "\n",
        "----\n",
        "## Labelled Split with illicit/licit nodes\n",
        "| Type    | train loss | validation loss | Accuracy | Precision | Recall | F1 Score\n",
        "| -------- | ------- | ------- | ------- | ------- | ------- | ------- |\n",
        "| GraphSAGE-Mean   |   0.1036  |  0.0906   |   0.972  |  0.829   |  0.678   |  0.746   |\n",
        "| GraphSAGE-MaxPool    |  0.1043  |  0.0987   |  0.967   |   0.72  |   0.755  |  0.737   |\n",
        "| GraphSAGE-LSTM    |  0.0843  |  0.0733   |   0.975  |  0.85   |  0.713   |  0.776   |\n"
      ],
      "metadata": {
        "id": "UdPsWSU_q3E4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1BM4C0Kjz6lQ"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}